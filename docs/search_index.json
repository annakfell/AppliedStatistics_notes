[
["index.html", "Applied Statistics Preface", " Applied Statistics Robin A. Donatello and Edward A. Roualdes Last Updated 2018-08-04 Preface This document is a set of course notes for several Applied Statistics courses at California State University, Chico. This is not a textbook replacement, and topics covered will vary depending on the instructor. To make this clear we use the term notebook to refer to this document so as not to be confused with a traditional textbook. Some data and examples in this notebook are drawn from Practical Multivariate Analysis, 5th ed, Afifi, May, Clark and used with permission by the authors. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],
["data-prep.html", "Chapter 1 Preparing Data for Analysis", " Chapter 1 Preparing Data for Analysis Write an introduction here. 1.0.1 Reproducible Workflows PrepareData Why do we need a codebook? You are your own collaborator 6 months from now. Make sure you will be able to understand what you were doing. Investing the time to do things clearly and in a reproducible manner will make your future self happy. Comment your code with explanations and instructions. How did you get from point A to B? Why did you recode this variable in this manner? We need to record those steps (not just for posterity). This means your code must be saved in a script file. Include sufficient notes to yourself describing what you are doing and why. For R, this can be in a .R or .RMD file. I always prefer the latter. For SPSS you can specify to paste the syntax and copy into a .sps script file. For SAS you’ll use a .sas file For STATA this will be a .do file Repro Figure Credits: Roger Peng What stages of this pipeline can we conduct using R Markdown? 1.0.2 Data Management Questions to ask yourself (and the data) while preparing a data management file. Do you need to code out missing data? Do you need to code out skip patterns? Do you need to make response codes more logical? Do you need to recode categorical variables to quantitative? Do you need to create secondary variables? Many of these answers will come only after you look at your data. This can be looking at the raw data itself but also looking at tables and charts generated from the data. "],
["import-data.html", "1.1 Import data", " 1.1 Import data This section uses the raw depression data set from the Afifi et.al. textbook. library(ggplot2) depress &lt;- read.table(&quot;https://norcalbiostat.netlify.com/data/Depress.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) SPSS syntax will look similar like this: GET DATA /TYPE=XLSX /FILE=&#39;C:\\path\\to\\file.xlsx&#39; /SHEET=name &#39;Name-of-Sheet&#39; /CELLRANGE=full /READNAMES=on /ASSUMEDSTRWIDTH=32767. EXECUTE. DATASET NAME DataSetExcel WINDOW=FRONT. Reference on importing data into SPSS: https://libguides.library.kent.edu/SPSS/ImportData. The absolute first thing you should do is to look at your raw data table. Are the column headers variable names? Did all the rows get read in? Are there any extra columns or rows included? 1.1.1 Renaming varible names for sanity sake Turn all variable names to lower case. This is especially frustrating for R and STATA users where syntax is case sensitive. This is completely optional but helpful names(depress) &lt;- tolower(names(depress)) You should also rename any variable that has spaces or special characters in the name. "],
["missing-data.html", "1.2 Missing data", " 1.2 Missing data In Excel, missing data can show up as a blank cell. In SPSS it is represented as a . period. R displays missing data as NA values. Missing Data in SPSS: https://stats.idre.ucla.edu/spss/modules/missing-data/ Why would data be missing? Other than the obvious data entry errors, tech glitches or just non-cooperative plants or people, sometimes values are out of range and you would rather delete them than change their value (data edit). Lets look at the religion variable in the depression data set. table(depress$relig, useNA=&quot;always&quot;) ## ## 1 2 3 4 6 &lt;NA&gt; ## 155 51 30 56 2 0 Looking at the codebook, there is no category 6 for religion. Let’s change all values to NA. depress$relig[depress$relig==6] &lt;- NA This code says take all rows where relig is equal to 6, and change them to NA. Confirm recode. table(depress$relig, useNA=&quot;always&quot;) ## ## 1 2 3 4 &lt;NA&gt; ## 155 51 30 56 2 Notice the use of the useNA=&quot;always&quot; argument. If we just looked at the base table without this argument, we would have never known there was missing data! table(depress$relig) ## ## 1 2 3 4 ## 155 51 30 56 What about continuous variables? Well there happens to be no other missing data in this data set, so let’s make up a set of 7 data points stored in a variable named y. y &lt;- c(1, 2, 3, NA, 4, NA, 6) y ## [1] 1 2 3 NA 4 NA 6 The #1 way to identify missing data in a continuous variable is by looking at the summary() values. mean(y) ## [1] NA summary(y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.0 2.0 3.0 3.2 4.0 6.0 2 mean(y, na.rm=TRUE) ## [1] 3.2 In R, any arithmetic function (like addition, multipliation) on missing data results in a missing value. The na.rm=TRUE toggle tells R to calculate the complete case mean. This is a biased measure of the mean, but missing data is a topic worthy of it’s own course. "],
["identifying-variable-types.html", "1.3 Identifying Variable Types", " 1.3 Identifying Variable Types The str function is short for structure. This shows you the variable names, what data types R thinks each variable are, and some of the raw data. You can also use the view() function to open the data as a simliar spreadsheet format, or head() to see the top 6 rows of the data. The latter is sometimes less than helpful for a very large data set. str(depress) ## &#39;data.frame&#39;: 294 obs. of 37 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ sex : int 2 1 2 2 2 1 2 1 2 1 ... ## $ age : int 68 58 45 50 33 24 58 22 47 30 ... ## $ marital : int 5 3 2 3 4 2 2 1 2 2 ... ## $ educat : int 2 4 3 3 3 3 2 3 3 2 ... ## $ employ : int 4 1 1 3 1 1 5 1 4 1 ... ## $ income : int 4 15 28 9 35 11 11 9 23 35 ... ## $ relig : int 1 1 1 1 1 1 1 1 2 4 ... ## $ c1 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ c2 : int 0 0 0 0 0 0 1 1 1 0 ... ## $ c3 : int 0 1 0 0 0 0 1 2 1 0 ... ## $ c4 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ c5 : int 0 0 1 1 0 0 1 2 0 0 ... ## $ c6 : int 0 0 0 1 0 0 0 1 3 0 ... ## $ c7 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ c8 : int 0 0 0 3 3 0 2 0 0 0 ... ## $ c9 : int 0 0 0 0 3 1 2 0 0 0 ... ## $ c10 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ c11 : int 0 0 0 0 0 0 0 0 0 0 ... ## $ c12 : int 0 1 0 0 0 1 0 0 3 0 ... ## $ c13 : int 0 0 0 0 0 2 0 0 0 0 ... ## $ c14 : int 0 0 1 0 0 0 0 0 3 0 ... ## $ c15 : int 0 1 1 0 0 0 3 0 2 0 ... ## $ c16 : int 0 0 1 0 0 2 0 1 3 0 ... ## $ c17 : int 0 1 0 0 0 1 0 1 0 0 ... ## $ c18 : int 0 0 0 0 0 0 0 1 0 0 ... ## $ c19 : int 0 0 0 0 0 0 0 1 0 0 ... ## $ c20 : int 0 0 0 0 0 0 1 0 0 0 ... ## $ cesd : int 0 4 4 5 6 7 15 10 16 0 ... ## $ cases : int 0 0 0 0 0 0 0 0 1 0 ... ## $ drink : int 2 1 1 2 1 1 2 2 1 1 ... ## $ health : int 2 1 2 1 1 1 3 1 4 1 ... ## $ regdoc : int 1 1 1 1 1 1 1 2 1 1 ... ## $ treat : int 1 1 1 2 1 1 1 2 1 2 ... ## $ beddays : int 0 0 0 0 1 0 0 0 1 0 ... ## $ acuteill: int 0 0 0 0 1 1 1 1 0 0 ... ## $ chronill: int 1 1 0 1 0 1 1 0 1 0 ... Right away this tells me that R thinks all variables are numeric integers, not categorical variables. This will have to be changed. We’ll get to that in a moment. In SPSS you’ll the following set of icons to tell you what data types the program thinks each column is: Consider the variable that measures marital status. table(depress$marital) ## ## 1 2 3 4 5 ## 73 127 43 13 38 str(depress$marital) ## int [1:294] 5 3 2 3 4 2 2 1 2 2 ... class(depress$marital) ## [1] &quot;integer&quot; What data type does R see this variable as? When variables have numerical levels it is necessary to ensure that the program knows it is a factor variable. The following code uses the factor() function to take the marital status variable and convert it into a factor variable with specified labels that match the codebook. depress$marital &lt;- factor(depress$marital, labels = c(&quot;Never Married&quot;, &quot;Married&quot;, &quot;Divorced&quot;, &quot;Separated&quot;, &quot;Widowed&quot;)) It is important to confirm the recode worked. If it did not you will have to re-read in the raw data set again since the variable sex was replaced. table(depress$marital) ## ## Never Married Married Divorced Separated Widowed ## 73 127 43 13 38 class(depress$marital) ## [1] &quot;factor&quot; Create a boxplot of income across marital status category. ggplot(depress, aes(y=income, x=marital)) + geom_boxplot() Boxplots are nice because they clearly show the range where 50% of the data lie and any potential outliers. Boxplots can also indicate skewness, but sometimes it is helpful to visualize the location of the mean as well as the median. ggplot2 has a nice stat_summary layer that will calculate and add the means to the current plot. ggplot(depress, aes(y=income, x=marital)) + geom_boxplot() + stat_summary(fun.y=mean, colour=&quot;blue&quot;, size=3, geom=&quot;point&quot;) "],
["outliers.html", "1.4 Outliers", " 1.4 Outliers Let’s look at the age variable in the depression data set. par(mfrow=c(1,2)) boxplot(depress$age) hist(depress$age) Just looking at the data graphically raises no red flags. The boxplot shows no outlying values and the histogram does not look wildly skewed. This is where knowledge about the data set is essential. The codebook does not provide a valid range for the data, but the description of the data starting on page 3 in the textbook clarifies that this data set is on adults. In the research world, this specifies 18 years or older. Now look back at the graphics. See anything odd? It appears as if the data go pretty far below 20, possibly below 18. Let’s check the numerical summary to get more details. summary(depress$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9.00 28.00 42.50 44.38 59.00 89.00 The minimum value is a 9, which is outside the range of valid values for this variable. This is where you, as a statistician, data analyst or researcher goes back to the PI and asks for advice. Should this data be set to missing, or edited in a way that changes this data point into a valid piece of data. As an example of a common data entry error, and for demonstration purposes, I went in and changed a 19 to a 9. So the correct thing to do here is to change that 9, back to a 19. This is a very good use of the ifelse() function. depress$age &lt;- ifelse(depress$age==9, 19, depress$age) The logical statement is depress$age9. Wherever this is true, replace the value of depress$age with 19, wherever this is false then keep the value of depress$age unchanged (by “replacing” the new value with the same old value). Alternatively, you can change that one value using bracket notation. Here you are specifying that you only want the rows where age==9, and directly assign a value of 19 to those rows. depress$age[depress$age==9] &lt;- 19 Confirm the recode. summary(depress$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18.00 28.00 42.50 44.41 59.00 89.00 Looks like it worked. "],
["creating-secondary-variables.html", "1.5 Creating secondary variables", " 1.5 Creating secondary variables 1.5.1 Collapsing variables into fewer categories For unbiased and accurate results of a statistical analysis, sufficient data has to be present. Often times once you start slicing and dicing the data to only look at certain groups, or if you are interested in the behavior of certain variables across levels of another variable, sometimes you start to run into small sample size problems. For example, consider marital status again. There are only 13 people who report being separated. This could potentially be too small of a group size for valid statistical analysis. One way to deal with insufficient data within a certain category is to collapse categories. The following code uses the recode() function from the car package to create a new variable that I am calling marital2 that combines the Divorced and Separated levels. library(car) marital2 &lt;- recode(depress$marital, &quot;&#39;Divorced&#39; = &#39;Sep/Div&#39;; &#39;Separated&#39; = &#39;Sep/Div&#39;&quot;) Always confirm your recodes. table(depress$marital, marital2, useNA=&quot;always&quot;) ## marital2 ## Married Never Married Sep/Div Widowed &lt;NA&gt; ## Never Married 0 73 0 0 0 ## Married 127 0 0 0 0 ## Divorced 0 0 43 0 0 ## Separated 0 0 13 0 0 ## Widowed 0 0 0 38 0 ## &lt;NA&gt; 0 0 0 0 0 This confirms that records where marital (rows) is Divorced or Separated have the value of Sep/Div for marital2 (columns). And that no missing data crept up in the process. Now I can drop the temporary marital2 variable and actually fix marital. (keeping it clean) depress$marital &lt;- recode(depress$marital, &quot;&#39;Divorced&#39; = &#39;Sep/Div&#39;; &#39;Separated&#39; = &#39;Sep/Div&#39;&quot;) rm(marital2) 1.5.2 Binning a continuous variable into categorical ranges. Let’s create a new variable that categorizes income into the following ranges: &lt;30, [30, 40), [40,50), [50, 60), 60+. The easiest way is to use the cut2 function in the package Hmisc. Note you don’t have to load the package fully to use a function from within that package. Useful for times when you only need to use a function once or twice. depress$inc_cut &lt;- Hmisc::cut2(depress$income, cuts=c(30,40,50,60)) table(depress$inc_cut) ## ## [ 2,30) [30,40) [40,50) [50,60) [60,65] ## 231 28 16 9 10 1.5.3 Dichotomizing Dichotomous variables tend to be binary indicator variables where a code of 1 is the level you’re interested in. For example, gender is coded as 2=Female and 1=Male. This is in the right direction but it needs to be 0/1. depress$sex &lt;- depress$sex -1 table(depress$sex) ## ## 0 1 ## 111 183 0/1 binary coding is mandatory for many analyses. One simple reason is that now you can calculate the mean and interpret it as a proportion. mean(depress$sex) ## [1] 0.622449 62% of individuals in this data set are female. Sometimes the data is recorded as 1/2 (Yes/No), so just subtracting from 1 doesn’t create a positive indicator of the variable. For example, drink=1 if they are a regular drinker, and drink=2 if they are not. We want not drinking to be coded as 0, not 2. table(depress$drink) ## ## 1 2 ## 234 60 The ifelse() function says that if depress$DRINK has a value equal to 2 ==2, then change the value to 0. Otherwise leave it alone. depress$drink &lt;- ifelse(depress$drink==2, 0, depress$drink) table(depress$drink) ## ## 0 1 ## 60 234 1.5.4 Sum or Average values across multiple variables The Center for Epidemiologic Studies Depression Scale (CESD) is series of questions asked to a person to measure their level of depression. CESD is calculated as the sum of all 20 component variables, and is already on this data set. Let’s create a new variable named sleep as subscale for sleep quality by adding up question numbers 5, 11, and 19. Reference: http://cesd-r.com/cesdr/ depress$sleep &lt;- depress$c5 + depress$c11 + depress$c19 ## # depress &lt;- depress %&gt;% mutate(sleep = c5+c11+c19) # Not run. dplyr example summary(depress$sleep) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 1.000 1.167 2.000 7.000 "],
["transformations-for-normality.html", "1.6 Transformations for Normality", " 1.6 Transformations for Normality Let’s look at assessing normal distributions using the cleaned depression data set. rm(depress) # remove the current version that was used in the previous part of this markdown file depress &lt;- read.table(&quot;https://norcalbiostat.netlify.com/data/depress_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) hist(depress$income, prob=TRUE, xlab=&quot;Annual income (in thousands)&quot;, main=&quot;Histogram and Density curve of Income&quot;, ylab=&quot;&quot;) lines(density(depress$income), col=&quot;blue&quot;) summary(depress$income) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 9.00 15.00 20.57 28.00 65.00 The distribution of annual income is slightly skewed right with a mean of $20.5k per year and a median of $15k per year income. The range of values goes from $2k to $65k. Reported income above $40k appear to have been rounded to the nearest $10k, because there are noticeable peaks at $40k, $50k, and $60k. In general, transformations are more effective when the the standard deviation is large relative to the mean. One rule of thumb is if the sd/mean ratio is less than 1/4, a transformation may not be necessary. sd(depress$income) / mean(depress$income) ## [1] 0.743147 Alternatively Hoaglin, Mosteller and Tukey (1985) showed that if the largest observation divided by the smallest observation is over 2, then the data may not be sufficiently variable for the transformation to be decisive. max(depress$income) / (min(depress$income)+.1) ## [1] 30.95238 Note these rules are not meaningful for data without a natural zero. Another common method of assessing normality is to create a normal probability (or normal quantile) plot. qqnorm(depress$income);qqline(depress$income, col=&quot;red&quot;) The points on the normal probability plot do not follow the red reference line very well. The dots show a more curved, or U shaped form rather than following a linear line. This is another indication that the data is skewed and a transformation for normality should be created. Create three new variables: log10inc as the log base 10 of Income, loginc as the natural log of Income, and xincome which is equal to the negative of one divided by the cubic root of income. log10inc &lt;- log10(depress$income) loginc &lt;- log(depress$income) xincome &lt;- -1/(depress$income)^(-1/3) Create a single plot that display normal probability plots for the original, and each of the three transformations of income. Use the base graphics grid organizer par(mfrow=c(r,c)) where r is the number of rows and c is the number of columns. Which transformation does a better job of normalizing the distribution of Income? par(mfrow=c(2,2)) # Try (4,1) and (1,4) to see how this works. qqnorm(depress$income, main=&quot;Income&quot;); qqline(depress$income,col=&quot;blue&quot;) qqnorm(log10inc, main=&quot;Log 10&quot;); qqline(log10inc, col=&quot;blue&quot;) qqnorm(loginc, main = &quot;Natural Log&quot;); qqline(loginc, col=&quot;blue&quot;) qqnorm(xincome, main=&quot;-1/cuberoot(income)&quot;); qqline(xincome, col=&quot;blue&quot;) "],
["saving-your-changes.html", "1.7 Saving your changes", " 1.7 Saving your changes You’ve just made a ton of changes! Save or export the new data set to your computer. Edit the codebook to reflect the changes that you made. Save this codebook with todays date as well. Keep the data, codebook and data management file in the same folder. The Sys.Date() function takes the current date from your computer. The value is then formatted nicely for human consumption and added (pasted) to the file name before written to the working directory as a new text file. date &lt;- format(Sys.Date(), &quot;%m%d%y&quot;) filename &lt;- paste(&quot;depress_&quot;, date, &quot;.txt&quot;, sep=&quot;&quot;) write.table(depress, filename, sep=&quot;\\t&quot;, row.names=FALSE) "],
["wide-vs-long-data.html", "1.8 Wide vs. Long data", " 1.8 Wide vs. Long data The data on Lung function originally was recorded in wide format, with separate variables for mother’s and father’s FEV1 score (MFEV1 and FFEV). In this format, the data is one record per family. fev &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/Lung_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) head(fev) ## ID AREA FSEX FAGE FHEIGHT FWEIGHT FFVC FFEV1 MSEX MAGE MHEIGHT MWEIGHT ## 1 1 1 1 53 61 161 391 3.23 2 43 62 136 ## 2 2 1 1 40 72 198 441 3.95 2 38 66 160 ## 3 3 1 1 26 69 210 445 3.47 2 27 59 114 ## 4 4 1 1 34 68 187 433 3.74 2 36 58 123 ## 5 5 1 1 46 61 121 354 2.90 2 39 62 128 ## 6 6 1 1 44 72 153 610 4.91 2 36 66 125 ## MFVC MFEV1 OCSEX OCAGE OCHEIGHT OCWEIGHT OCFVC OCFEV1 MCSEX MCAGE ## 1 370 3.31 2 12 59 115 296 2.79 NA NA ## 2 411 3.47 1 10 56 66 323 2.39 NA NA ## 3 309 2.65 1 8 50 59 114 1.11 NA NA ## 4 265 2.06 2 11 57 106 256 1.85 1 9 ## 5 245 2.33 1 16 61 88 260 2.47 2 12 ## 6 349 3.06 1 15 67 100 389 3.55 1 13 ## MCHEIGHT MCWEIGHT MCFVC MCFEV1 YCSEX YCAGE YCHEIGHT YCWEIGHT YCFVC ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 49 56 159 1.30 NA NA NA NA NA ## 5 60 85 268 2.34 2 10 50 53 154 ## 6 57 87 276 2.37 2 10 55 72 195 ## YCFEV1 ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 1.43 ## 6 1.69 To analyze the effect of gender on FEV, the data need to be in long format, with a single variable for fev and a separate variable for gender. The following code chunk demonstrates one method of combining data on height, gender, age and FEV1 for both males and females. fev2 &lt;- data.frame(gender = c(fev$FSEX, fev$MSEX), rev = c(fev$FFEV1, fev$MFEV1), ht = c(fev$FHEIGHT, fev$MHEIGHT), age = c(fev$FAGE, fev$MAGE)) fev2$gender &lt;- factor(fev2$gender, labels=c(&quot;M&quot;, &quot;F&quot;)) head(fev2) ## gender rev ht age ## 1 M 3.23 61 53 ## 2 M 3.95 72 40 ## 3 M 3.47 69 26 ## 4 M 3.74 68 34 ## 5 M 2.90 61 46 ## 6 M 4.91 72 44 Nearly all analysis procedures and most graphing procedures require the data to be in long format. There are several R packages that can help with this including reshape2 and tidyr. "],
["dealing-with-missing-data-post-analysis.html", "1.9 Dealing with missing data post-analysis", " 1.9 Dealing with missing data post-analysis Case when: you want to add model predictions to the data set, but you have missing data that was automatically dropped prior to analysis. If your original data had missing values, here is one way to get the factor scores for available data back onto the data set. Alternatively you can look into methods to conduct factor analysis with missing data (FactomineR) If no ID column exists, create one: id = 1:NROW(data) Use select() to extract ID and all variables used in the factor analysis Do na.omit() Conduct factor analysis on this subsetted data set Use bind_cols() to add columns containing factor scores to this subsetted data set as described above Use select() to only keep the ID and the factor score variables Then left_join() the factor scores back to the original data, using the ID variable as the joining key. "],
["data-viz.html", "Chapter 2 Visualizing Data", " Chapter 2 Visualizing Data In Progress "],
["select-analysis.html", "Chapter 3 Selecting Appropriate Analyses", " Chapter 3 Selecting Appropriate Analyses In Progress Considerations: Purpose of analysis. Types of variables in data set. Data used in analysis. Assumptions needed; satisfied? Choice of analyses is often arbitrary: consider several Example: 5 independent variables: 3 interval, 1 ordinal, 1 nominal 1 dependent variable: interval Analysis options Multiple regression: pretend independent ordinal variable is an interval variable use dummy (0 /1) variables for nominal variables Analysis of variance: categorize all independent variables Analysis of covariance: leave variables as is, check assumptions Logistic regression: Categorize dependent variable: high, low Survival analysis: IF dependent variable is time to an event Unsure? Do several and compare results. "],
["reg-intro.html", "Chapter 4 Introduction", " Chapter 4 Introduction The general purpose of regression is to learn more about the relationship between several independent or predictor variables and a quantitative dependent variable. Multiple regression procedures are very widely used in research. In general, this inferential tool allows us to ask (and hopefully answer) the general question “what is the best predictor of…”, and does “additional variable A” or “additional variable B” confound the relationship between my explanatory and response variable?” Educational researchers might want to learn about the best predictors of success in high-school. Sociologists may want to find out which of the multiple social indicators best predict whether or not a new immigrant group will adapt to their new country of residence. Biologists may want to find out which factors (i.e. temperature, barometric pressure, humidity, etc.) best predict caterpillar reproduction. This chapter starts by recapping notation and topics for simple linear regression, when there is only one predictor. Then we move into generalization of these concepts to many predictors, and model building topics such as stratification, interactions, and categorical predictors. "],
["simple-linear-regression.html", "Chapter 5 Simple Linear Regression", " Chapter 5 Simple Linear Regression The goal of linear regression is to Describe the relationship between an independent variable X and a continuous dependent variable \\(Y\\) as a straight line. The textbook discusses two cases: Fixed-\\(X\\): values of \\(X\\) are preselected by investigator Variable-\\(X\\): have random sample of \\((X,Y)\\) values Calculations are the same, Draw inferences regarding this relationship Predict value of \\(Y\\) for a given value of \\(X\\) "],
["mathmatical-model.html", "5.1 Mathmatical Model", " 5.1 Mathmatical Model The mean of \\(Y\\) values at any given \\(X\\) is \\(\\beta_{0} + \\beta_{1} X\\) The variance of \\(Y\\) values at any \\(X\\) is \\(\\sigma^2\\) (same for all X) \\(Y\\) values are normally distributed at any given \\(X\\) (need for inference) Figure 6.2 "],
["parameter-estimates.html", "5.2 Parameter Estimates", " 5.2 Parameter Estimates Estimate the slope \\(\\beta_{1}\\) and intercept \\(\\beta_{0}\\) using least-squares methods. The residual mean squared error (RMSE) is an estimate of the variance \\(s^{2}\\) Typically interested in inference on \\(\\beta_{1}\\) Assume no relationship between \\(X\\) and \\(Y\\) \\((H_{0}: \\beta_{1}=0)\\) until there is reason to believe there is one \\((H_{0}: \\beta_{1} \\neq 0)\\) "],
["interval-estimation.html", "5.3 Interval estimation", " 5.3 Interval estimation Everything is estimated with some degree of error Confidence intervals for the mean of \\(Y\\) Prediction intervals for an individual \\(Y\\) Which one is wider? Why? "],
["corelation-coefficient.html", "5.4 Corelation Coefficient", " 5.4 Corelation Coefficient The correlation coefficient \\(\\rho\\) measures the strength of association between \\(X\\) and \\(Y\\) in the population. \\(\\sigma^{2} = VAR(Y|X)\\) is the variance of \\(Y\\) for a specific \\(X\\). \\(\\sigma_{y}^{2} = VAR(Y)\\) is the variance of \\(Y\\) for all \\(X\\)’s. \\[ \\sigma^{2} = \\sigma_{y}^{2}(1-\\rho^{2})\\] \\[ \\rho^{2} = \\frac{\\sigma_{y}^{2} - \\sigma^{2}}{\\sigma_{y}^{2}}\\] \\(\\rho^{2}\\) = reduction in variance of Y associated with knowledge of X/original variance of Y Coefficient of Determiniation: \\(100\\rho^{2}\\) = % of variance of Y associated with X or explained by X Caution: association vs. causation. "],
["assumptions.html", "5.5 Assumptions", " 5.5 Assumptions Homogeneity of variance (same \\(\\sigma^{2}\\)) Not extremely serious Can use transformations to achieve it Graphical assessment: Plot the residuals against the x variable, add a lowess line. This assumption is upheld if there is no relationship/trend between the residuals and the predictor. Normal residuals Slight departures OK Can use transformations to achieve it Graphical assessment: normal qqplot of the model residuals. Randomness / Independence Very serious Can use hierarchical models for clustered samples No real good way to “test” for independence. Need to know how the sample was obtained. Linear relationship Slight departures OK Can use transformations to achieve it Graphical assessment: Simple scatterplot of \\(y\\) vs \\(x\\). Looking for linearity in the relationship. Should be done prior to any analysis. "],
["example.html", "5.6 Example", " 5.6 Example Using a cleaned version of the Lung function data set, lets explore the relationship between height and FEV for fathers in this data set. fev &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/Lung_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) qplot(y=FFEV1, x=FHEIGHT, geom=&quot;point&quot;, data=fev, xlab=&quot;Height&quot;, ylab=&quot;FEV1&quot;, main=&quot;Scatter Diagram with Regression (blue) and Lowess (red) Lines of FEV1 Versus Height for Fathers.&quot;) + geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;blue&quot;) + geom_smooth(se=FALSE, col=&quot;red&quot;) There does appear to be a tendency for taller men to have higher FEV1. Let’s fit a linear model and report the regression parameter estimates. model &lt;- lm(FFEV1 ~ FHEIGHT, data=fev) summary(model) ## ## Call: ## lm(formula = FFEV1 ~ FHEIGHT, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56688 -0.35290 0.04365 0.34149 1.42555 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.08670 1.15198 -3.548 0.000521 *** ## FHEIGHT 0.11811 0.01662 7.106 4.68e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5638 on 148 degrees of freedom ## Multiple R-squared: 0.2544, Adjusted R-squared: 0.2494 ## F-statistic: 50.5 on 1 and 148 DF, p-value: 4.677e-11 The least squares equation is \\(Y = -4.087 + 0.118X\\). confint(model) ## 2.5 % 97.5 % ## (Intercept) -6.36315502 -1.8102499 ## FHEIGHT 0.08526328 0.1509472 For ever inch taller a father is, his FEV1 measurement significantly increases by .12 (95%CI: .09, .15, p&lt;.0001). The correlation between FEV1 and height is \\(\\sqrt{.2544}\\) = 0.5. Lastly, check assumptions on the residuals to see if the model results are valid. Homogeneity of variance plot(model$residuals ~ fev$FHEIGHT) lines(lowess(model$residuals ~ fev$FHEIGHT), col=&quot;red&quot;) Normal residuals qqnorm(model$residuals) qqline(model$residuals, col=&quot;red&quot;) No major deviations away from what is expected. "],
["multiple-linear-regression.html", "Chapter 6 Multiple Linear Regression", " Chapter 6 Multiple Linear Regression Extends simple linear regression. Describes a linear relationship between a single continuous \\(Y\\) variable, and several \\(X\\) variables. Predicts \\(Y\\) from \\(X_{1}, X_{2}, \\ldots , X_{P}\\). Now it’s no longer a 2D regression line, but a \\(p\\) dimensional regression plane. "],
["types-of-x-variables.html", "6.1 Types of X variables", " 6.1 Types of X variables Fixed: The levels of \\(X\\) are selected in advance with the intent to measure the affect on an outcome \\(Y\\). Variable: Random sample of individuals from the population is taken and \\(X\\) and \\(Y\\) are measured on each individual. X’s can be continuous or discrete (categorical) X’s can be transformations of other X’s, e.g., \\(log(x), x^{2}\\). "],
["mathematical-model.html", "6.2 Mathematical Model", " 6.2 Mathematical Model \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\ldots + \\beta_{p}x_{pi} + \\epsilon_{i}\\] The assumptions on the residuals \\(\\epsilon_{i}\\) still hold: They have mean zero They are homoscedastic, that is all have the same finite variance: \\(Var(\\epsilon_{i})=\\sigma^{2}&lt;\\infty\\) Distinct error terms are uncorrelated: (Independent) \\(\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0,\\forall i\\neq j.\\) The regression model relates \\(y\\) to a function of \\(\\textbf{X}\\) and \\(\\mathbf{\\beta}\\), where \\(\\textbf{X}\\) is a \\(nxp\\) matrix of \\(p\\) covariates on \\(n\\) observations and \\(\\mathbf{\\beta}\\) is a length \\(p\\) vector of regression coefficients. In matrix notation this looks like: \\[ \\textbf{y} = \\textbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\] "],
["parameter-estimation.html", "6.3 Parameter Estimation", " 6.3 Parameter Estimation The goal of regression analysis is to minimize the residual error. That is, to minimize the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable. \\[ \\epsilon_{i} = \\hat{y_{i}} - y_{i}\\] The method of Least Squares accomplishes this by finding parameter estimates \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimized the sum of the squared residuals: \\[ \\sum_{i=1}^{n} \\epsilon_{i} \\] For simple linear regression the regression coefficient estimates that minimize the sum of squared errors can be calculated as: \\[ \\hat{\\beta_{0}} = \\bar{y} - \\hat{\\beta_{1}}\\bar{x} \\quad \\mbox{ and } \\quad \\hat{\\beta_{1}} = r\\frac{s_{y}}{s_{x}} \\] For multiple linear regression, the fitted values \\(\\hat{y_{i}}\\) are calculated as the linear combination of x’s and \\(\\beta\\)’s, \\(\\sum_{i=1}^{p}X_{ij}\\beta_{j}\\). The sum of the squared residual errors (the distance between the observed point \\(y_{i}\\) and the fitted value) now has the following form: \\[ \\sum_{i=1}^{n} |y_{i} - \\sum_{i=1}^{p}X_{ij}\\beta_{j}|^{2}\\] Or in matrix notation \\[ || \\mathbf{y} - \\mathbf{X}\\mathbf{\\beta} ||^{2} \\] The details of methods to calculate the Least Squares estimate of \\(\\beta\\)’s is left to a course in mathematical statistics. "],
["example-1.html", "6.4 Example", " 6.4 Example The analysis in example (???) concluded that FEV1 in fathers significantly increases by 0.12 (95% CI:0.09, 0.15) liters per additional inch in height (p&lt;.0001). Looking at the multiple \\(R^{2}\\) (correlation of determination), this simple model explains 25% of the variance seen in the outcome \\(y\\). However, FEV tends to decrease with age for adults, so we should be able to predict it better if we use both height and age as independent variables in a multiple regression equation. What direction do you expect the slope coefficient for age to be? For height? Fitting a regression model in R with more than 1 predictor is done by adding each variable to the right hand side of the model notation connected with a +. mv_model &lt;- lm(FFEV1 ~ FAGE + FHEIGHT, data=fev) summary(mv_model) ## ## Call: ## lm(formula = FFEV1 ~ FAGE + FHEIGHT, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.34708 -0.34142 0.00917 0.37174 1.41853 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.760747 1.137746 -2.427 0.0165 * ## FAGE -0.026639 0.006369 -4.183 4.93e-05 *** ## FHEIGHT 0.114397 0.015789 7.245 2.25e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5348 on 147 degrees of freedom ## Multiple R-squared: 0.3337, Adjusted R-squared: 0.3247 ## F-statistic: 36.81 on 2 and 147 DF, p-value: 1.094e-13 confint(mv_model) ## 2.5 % 97.5 % ## (Intercept) -5.00919751 -0.51229620 ## FAGE -0.03922545 -0.01405323 ## FHEIGHT 0.08319434 0.14559974 Holding height constant, a father who is one year older is expected to have a FEV value 0.03 (0.01, 0.04) liters less than another man (p&lt;.0001). Holding height constant, a father who is 1cm taller than another man is expected to have a FEV value of 0.11 (.08, 0.15) liter greater than the other man (p&lt;.0001). For the model that includes age, the coefficient for height is now 0.11, which is interpreted as the rate of change of FEV1 as a function of height after adjusting for age. This is also called the partial regression coefficient of FEV1 on height after adjusting for age. Both height and age are significantly associated with FEV in fathers (p&lt;.0001 each). "],
["model-diagnostics.html", "6.5 Model Diagnostics", " 6.5 Model Diagnostics The same set of regression diagnostics can be examined to identify any potential influential points, outliers or other problems with the linear model. par(mfrow=c(2,2)) plot(mv_model) "],
["multicollinearity.html", "6.6 Multicollinearity", " 6.6 Multicollinearity Occurs when some of the X variables are highly intercorrelated. Affects estimates and their SE’s (p. 143) Look at tolerance, and its inverse, the Variance Inflation Factor (VIF) Need tolerance &lt; 0.01, or VIF &gt; 100. car::vif(mv_model) ## FAGE FHEIGHT ## 1.003163 1.003163 tolerance = 1/car::vif(mv_model) tolerance ## FAGE FHEIGHT ## 0.9968473 0.9968473 Solution: use variable selection to delete some X variables. Alternatively, use dimension reduction techniques such as Principal Components "],
["what-to-watch-out-for.html", "6.7 What to watch out for", " 6.7 What to watch out for Representative sample Range of prediction should match observed range of X in sample Use of nominal or ordinal, rather than interval or ratio data Errors-in-variables Correlation does not imply causation Violation of assumptions Influential points Appropriate model Multicollinearity "],
["model-building.html", "Chapter 7 Model Building", " Chapter 7 Model Building Model building methods are used mainly in exploratory situations where many independent variables have been measured, but a final model explaining the dependent variable has not been reached. You want to build a model that contains enough covariates to explain the model well, but still be parsimonious such that the model is still interpretable. This chapter introduces different types of covariates that can be used, stratified models, confounding and moderation. We then conclude with measures of model fit and methods to compare between competing models. "],
["categorical-predictors.html", "7.1 Categorical Predictors", " 7.1 Categorical Predictors Let’s continue to model the length of the iris petal based on the length of the sepal, controlling for species. But here we’ll keep species as a categorical variable. What happens if we just put the variable in the model? summary(lm(Petal.Length ~ Sepal.Length + Species, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76390 -0.17875 0.00716 0.17461 0.79954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.70234 0.23013 -7.397 1.01e-11 *** ## Sepal.Length 0.63211 0.04527 13.962 &lt; 2e-16 *** ## Speciesversicolor 2.21014 0.07047 31.362 &lt; 2e-16 *** ## Speciesvirginica 3.09000 0.09123 33.870 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2826 on 146 degrees of freedom ## Multiple R-squared: 0.9749, Adjusted R-squared: 0.9744 ## F-statistic: 1890 on 3 and 146 DF, p-value: &lt; 2.2e-16 Examine the coefficient names, Speciesversicolor and Speciesvirginica. R (and most software packages) automatically take a categorical variable and turn it into a series of binary indicator variables. Let’s look at what the software program does in the background. Below is a sample of the iris data. The first column shows the row number, specifically I am only showing 2 sample rows from each species. The second column is the value of the sepal length, the third is the binary indicator for if the iris is from species versicolor, next the binary indicator for if the iris is from species virginica, and lastly the species as a 3 level categorical variable (which is what we’re used to seeing at this point.) Sepal.Length Speciesversicolor Speciesvirginica Species 1 5.1 0 0 setosa 2 4.9 0 0 setosa 51 7 1 0 versicolor 52 6.4 1 0 versicolor 101 6.3 0 1 virginica 102 5.8 0 1 virginica 7.1.1 Factor variable coding Most commonly known as “Dummy coding”. Not an informative term to use. Better used term: Indicator variable Math notation: I(gender == “Female”). A.k.a reference coding For a nominal X with K categories, define K indicator variables. Choose a reference (referent) category: Leave it out Use remaining K-1 in the regression. Often, the largest category is chosen as the reference category. For the iris example, 2 indicator variables are created for versicolor and virginica. Interpreting the regression coefficients are going to be compared to the reference group. In this case, it is species setosa. The mathematical model is now written as follows, where \\(x_{1}\\) is Sepal Length, \\(x_{2}\\) is the indicator for versicolor, and \\(x_{3}\\) the indicator for virginica \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{3i}+ \\epsilon_{i}\\] Let’s look at the regression coefficients and their 95% confidence intervals from the main effects model again. main.eff.model &lt;- lm(Petal.Length ~ Sepal.Length + Species, data=iris) pander(main.eff.model) Fitting linear model: Petal.Length ~ Sepal.Length + Species Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.702 0.2301 -7.397 1.005e-11 Sepal.Length 0.6321 0.04527 13.96 1.121e-28 Speciesversicolor 2.21 0.07047 31.36 9.646e-67 Speciesvirginica 3.09 0.09123 33.87 4.918e-71 pander(confint(main.eff.model)) 2.5 % 97.5 % (Intercept) -2.157 -1.248 Sepal.Length 0.5426 0.7216 Speciesversicolor 2.071 2.349 Speciesvirginica 2.91 3.27 In this main effects model, Species only changes the intercept. The effect of species is not multiplied by Sepal length. The interpretations are the following: \\(b_{1}\\): After controlling for species, Petal length significantly increases with the length of the sepal (0.63, 95% CI 0.54-0.72, p&lt;.0001). \\(b_{2}\\): Versicolor has on average 2.2cm longer petal lengths compared to setosa (95% CI 2.1-2.3, p&lt;.0001). \\(b_{3}\\): Virginica has on average 3.1cm longer petal lengths compared to setosa (95% CI 2.9-3.3, p&lt;.0001). 7.1.2 Wald test The Wald test is used for simultaneous tests of \\(Q\\) variables in a model Consider a model with \\(P\\) variables and you want to test if \\(Q\\) additional variables are useful. \\(H_{0}: Q\\) additional variables are useless, i.e., their \\(\\beta\\)’s all = 0 \\(H_{A}: Q\\) additional variables are useful The traditional test statistic that we’ve seen since Intro stats is \\(\\frac{\\hat{\\theta}-\\theta}{\\sqrt{Var(\\hat{\\theta})}}\\) The Wald test generalizes this test any linear combination of predictors. \\[ (R\\hat{\\theta}_{n}-r)^{&#39;}[R({\\hat{V}}_{n}/n)R^{&#39;}]^{-1} (R\\hat{\\theta}_{n}-r) \\quad \\xrightarrow{\\mathcal{D}} \\quad F(Q,n-P) \\] Where \\(\\mathbf{R}\\) is the vector of coefficients for the \\(\\beta\\), and \\(\\hat{V}_{n}\\) is a consistent estimator of the covariance matrix. Instead of a normal distribution, this test statistic has an \\(F\\) distribution with \\(Q\\) and \\(n-P\\) degrees of freedom. In the case where we’re testing \\(\\beta_{p}=\\beta_{q}=...=0\\), \\(\\mathbf{R}\\) is all 1’s. This can be done in R by using the regTermTest() function in the survey package. library(survey) regTermTest(main.eff.model, &quot;Species&quot;) ## Wald test for Species ## in lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) ## F = 624.9854 on 2 and 146 df: p= &lt; 2.22e-16 7.1.2.0.1 Example 1: Employment status on depression score Consider a model to predict depression using age, employment status and whether or not the person was chronically ill in the past year as covariates. This example uses the cleaned depression data set. depress &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/depress_081217.txt&quot;, header=TRUE,sep=&quot;\\t&quot;) full_model &lt;- lm(cesd ~ age + chronill + employ, data=depress) pander(summary(full_model)) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.48 1.502 7.646 3.191e-13 age -0.133 0.03514 -3.785 0.0001873 chronill 2.688 1.024 2.625 0.009121 employHouseperson 6.75 1.797 3.757 0.0002083 employIn School 1.967 5.995 0.328 0.7431 employOther 4.897 4.278 1.145 0.2533 employPT 3.259 1.472 2.214 0.02765 employRetired 3.233 1.886 1.714 0.08756 employUnemp 7.632 2.339 3.263 0.001238 Fitting linear model: cesd ~ age + chronill + employ Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 294 8.385 0.1217 0.09704 The results of this model show that age and chronic illness are statistically associated with CESD (each p&lt;.006). However employment status shows mixed results. Some employment statuses are significantly different from the reference group, some are not. So overall, is employment status associated with depression? Recall that employment is a categorical variable, and all the coefficient estimates shown are the effect of being in that income category has on depression compared to being employed full time. For example, the coefficient for PT employment is greater than zero, so they have a higher CESD score compared to someone who is fully employed. But what about employment status overall? Not all employment categories are significantly different from FT status. To test that employment status affects CESD we need to do a global test that all \\(\\beta\\)’s are 0. \\(H_{0}: \\beta_{3} = \\beta_{4} = \\beta_{5} = \\beta_{6} = \\beta_{7} = \\beta_{8} = 0\\) \\(H_{A}\\): At least one \\(\\beta_{j}\\) is not 0. regTermTest(full_model, &quot;employ&quot;) ## Wald test for employ ## in lm(formula = cesd ~ age + chronill + employ, data = depress) ## F = 4.153971 on 6 and 285 df: p= 0.0005092 Confirm that the degrees of freedom are correct. It should equal the # of categories in the variable you are testing, minus 1. Employment has 7 levels, so \\(df=6\\). Or equivalently, the degrees of freedom are the number of \\(beta\\)’s you are testing to be 0. The p-value of this Wald test is significant, thus employment significantly predicts CESD score. 7.1.2.1 Example 2: Blood Pressure load(url(&quot;https://norcalbiostat.netlify.com/data/addhealth_clean.Rdata&quot;)) addhealth$smoke &lt;- ifelse(addhealth$eversmoke_c==&quot;Smoker&quot;, 1, 0) Consider a logistic model on smoking status (0= never smoked, 1=has smoked) using gender, income, and blood pressure class (bp_class) as predictors. \\[ logit(Y) = \\beta_{0} + \\beta_{1}\\mbox{(female)} + \\beta_{2}\\mbox{(income)} + \\beta_{3}\\mbox{(Pre-HTN)} + \\beta_{4}\\mbox{(HTN-I)} + \\beta_{5}\\mbox{(HTN-II)} \\] bp.mod &lt;- glm(smoke ~ female_c + income + bp_class, data=addhealth, family=&#39;binomial&#39;) pander(summary(bp.mod)) Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.046 0.1064 9.836 7.881e-23 female_cFemale -0.6182 0.07617 -8.117 4.798e-16 income -3.929e-06 1.411e-06 -2.785 0.005346 bp_classPre-HTN 0.07289 0.08206 0.8882 0.3745 bp_classHTN-I -0.02072 0.1093 -0.1895 0.8497 bp_classHTN-II 0.02736 0.1888 0.1449 0.8848 (Dispersion parameter for binomial family taken to be 1 ) Null deviance: 4853 on 3728 degrees of freedom Residual deviance: 4769 on 3723 degrees of freedom It is unlikely that blood pressure is associated with smoking status, all groups are not statistically significantly different from the reference group (all p-values are large). Let’s test that hypothesis formally using a Wald Test. regTermTest(bp.mod, &quot;bp_class&quot;) ## Wald test for bp_class ## in glm(formula = smoke ~ female_c + income + bp_class, family = &quot;binomial&quot;, ## data = addhealth) ## F = 0.428004 on 3 and 3723 df: p= 0.73294 The Wald Test has a large p-value of 0.73, thus blood pressure classification is not associated with smoking status. This means blood pressure classification should not be included in a model to explain smoking status. "],
["stratification.html", "7.2 Stratification", " 7.2 Stratification Stratified models examine the regression equations for each subgroup of the population and seeing if the relationship between the response and explanatory variables changed for at least one subgroup. Consider the relationship between the length of an iris petal, and the length of it’s sepal. Earlier we found that the iris species modified this relationship. Lets consider a binary indicator variable for species that groups veriscolor and virginica together. iris$setosa &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0) table(iris$setosa, iris$Species) ## ## setosa versicolor virginica ## 0 0 50 50 ## 1 50 0 0 Within the setosa species, there is little to no relationship between sepal and petal length. For the other two species, the relationship looks still significantly positive, but in the combined sample there appears to be a strong positive relationship (blue). ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, col=as.factor(setosa))) + geom_point() + theme_bw() + theme(legend.position=&quot;top&quot;) + scale_color_manual(name=&quot;Species setosa&quot;, values=c(&quot;red&quot;, &quot;darkgreen&quot;)) + geom_smooth(se=FALSE, method=&quot;lm&quot;) + geom_smooth(aes(x=Sepal.Length, y=Petal.Length), col=&quot;blue&quot;, se=FALSE, method=&#39;lm&#39;) The mathematical model describing the relationship between Petal length (\\(Y\\)), and Sepal length (\\(X\\)), for species setosa (\\(s\\)) versus not-setosa (\\(n\\)), is written as follows: \\[ Y_{is} \\sim \\beta_{0s} + \\beta_{1s}*x_{i} + \\epsilon_{is} \\qquad \\epsilon_{is} \\sim \\mathcal{N}(0,\\sigma^{2}_{s})\\] \\[ Y_{in} \\sim \\beta_{0n} + \\beta_{1n}*x_{i} + \\epsilon_{in} \\qquad \\epsilon_{in} \\sim \\mathcal{N}(0,\\sigma^{2}_{n}) \\] In each model, the intercept, slope, and variance of the residuals can all be different. This is the unique and powerful feature of stratified models. The downside is that each model is only fit on the amount of data in that particular subset. Furthermore, each model has 3 parameters that need to be estimated: \\(\\beta_{0}, \\beta_{1}\\), and \\(\\sigma^{2}\\), for a total of 6 for the two models. The more parameters that need to be estimated, the more data we need. "],
["moderation.html", "7.3 Moderation", " 7.3 Moderation Moderation occurs when the relationship between two variables depends on a third variable. The third variable is referred to as the moderating variable or simply the moderator. The moderator affects the direction and/or strength of the relationship between the explanatory (\\(x\\)) and response (\\(y\\)) variable. This tends to be an important When testing a potential moderator, we are asking the question whether there is an association between two constructs, but separately for different subgroups within the sample. This is also called a stratified model, or a subgroup analysis. Here are 3 scenarios demonstrating how a third variable can modify the relationship between the original two variables. Scenario 1 - Significant relationship at bivariate level (saying expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the strength (i.e., p-value is Non-Significant) of the relationship changes. Could just change strength for one level of third variable, not necessarily all levels of the third variable. Scenario 2 - Non-significant relationship at bivariate level (saying do not expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the relationship becomes significant (saying expect to see it in at least one of the sub-groups or levels of third variable, but not in entire population because was not significant before tested for moderation). Could just become significant in one level of the third variable, not necessarily all levels of the third variable. Scenario 3 - Significant relationship at bivariate level (saying expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the direction (i.e., means change order/direction) of the relationship changes. Could just change direction for one level of third variable, not necessarily all levels of the third variable. Recall that common analysis methods for analyzing bivariate relationships come in very few flavors: Correlation (Q~Q) Linear Regression (Q~Q) \\(\\chi^{2}\\) (C~C) ANOVA (Q~C) 7.3.1 Example 1: Sepal vs Petal Length We just got done looking at the relationship between the length of an iris’s Sepal, and the length (cm) of it’s petal. overall &lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + geom_smooth(se=FALSE) + theme_bw() by_spec &lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, col=Species)) + geom_point() + geom_smooth(se=FALSE) + theme_bw() + theme(legend.position=&quot;top&quot;) library(gridExtra) grid.arrange(overall, by_spec , ncol=2) Is the relationship between sepal length and petal length the same within each species? Let’s look at the correlation between these two continuous variables overall cor(iris$Sepal.Length, iris$Petal.Length) ## [1] 0.8717538 stratified by species by(iris, iris$Species, function(x) cor(x$Sepal.Length, x$Petal.Length)) ## iris$Species: setosa ## [1] 0.2671758 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 0.754049 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 0.8642247 There is a strong, positive, linear relationship between the sepal length of the flower and the petal length when ignoring the species. The correlation coefficient \\(r\\) for virginica and veriscolor are similar to the overall \\(r\\) value, 0.86 and 0.75 respectively compared to 0.87. However the correlation between sepal and petal length for species setosa is only 0.26. The points are clearly clustered by species, the slope of the lowess line between virginica and versicolor appear similar in strength, whereas the slope of the line for setosa is closer to zero. This would imply that petal length for Setosa may not be affected by the length of the sepal. 7.3.2 Example 2: Simpson’s Paradox Sometimes moderating variables can result in what’s known as Simpson’s Paradox https://en.wikipedia.org/wiki/Simpson%27s_paradox "],
["interactions.html", "7.4 Interactions", " 7.4 Interactions If we care about how species changes the relationship between petal and sepal length, we can fit a model with an interaction between sepal length (\\(x_{1}\\)) and species. For this first example let \\(x_{2}\\) be an indicator for when species == setosa . Note that both main effects of sepal length, and setosa species are also included in the model. Interactions are mathematically represented as a multiplication between the two variables that are interacting. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}x_{2i}\\] Ifwe evaluate this model for both levels of \\(x_{2}\\), the resulting models are the same as the stratified models. When \\(x_{2} = 0\\), the record is on an iris not from the setosa species. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(0) + \\beta_{3}x_{1i}(0)\\] which simplifies to \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i}\\] When \\(x_{2} = 1\\), the record is on an iris of the setosa species. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(1) + \\beta_{3}x_{1i}(1)\\] which simplifies to \\[ Y_{i} \\sim (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3})x_{i}\\] Each subgroup model has a different intercept and slope, but we had to estimate 4 parameters in the interaction model, and 6 for the fully stratified model. Interactions are fit in R by simply multiplying * the two variables together in the model statement. summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96754 -0.19948 -0.01386 0.22597 1.05479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55571 0.37509 -4.148 5.68e-05 *** ## Sepal.Length 1.03189 0.05957 17.322 &lt; 2e-16 *** ## setosa 2.35877 0.88266 2.672 0.00839 ** ## Sepal.Length:setosa -0.90026 0.17000 -5.296 4.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3929 on 146 degrees of freedom ## Multiple R-squared: 0.9515, Adjusted R-squared: 0.9505 ## F-statistic: 954.1 on 3 and 146 DF, p-value: &lt; 2.2e-16 The coefficient \\(b_{3}\\) for the interaction term is significant, confirming that species changes the relationship between sepal length and petal length. 7.4.1 Example summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96754 -0.19948 -0.01386 0.22597 1.05479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55571 0.37509 -4.148 5.68e-05 *** ## Sepal.Length 1.03189 0.05957 17.322 &lt; 2e-16 *** ## setosa 2.35877 0.88266 2.672 0.00839 ** ## Sepal.Length:setosa -0.90026 0.17000 -5.296 4.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3929 on 146 degrees of freedom ## Multiple R-squared: 0.9515, Adjusted R-squared: 0.9505 ## F-statistic: 954.1 on 3 and 146 DF, p-value: &lt; 2.2e-16 If \\(x_{2}=0\\), then the effect of \\(x_{1}\\) on \\(Y\\) simplifies to: \\(\\beta_{1}\\) \\(b_{1}\\) The effect of sepal length on petal length for non-setosa species of iris (setosa=0) For non-setosa species, the petal length increases 1.03cm for every additional cm of sepal length. If \\(x_{2}=1\\), then the effect of \\(x_{1}\\) on \\(Y\\) model simplifies to: \\(\\beta_{1} + \\beta_{3}\\) For setosa species, the petal length increases by 1.03-0.9=0.13 cm for every additional cm of sepal length. The main effects (\\(b_{1}\\), \\(b_{2}\\)) cannot be interpreted by themselves when there is an interaction in the model. Let’s up the game now and look at the full interaction model with a categorical version of species. Recall \\(x_{1}\\) is Sepal Length, \\(x_{2}\\) is the indicator for versicolor, and \\(x_{3}\\) the indicator for virginica . \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{3i} + \\beta_{4}x_{1i}x_{2i} + \\beta_{5}x_{1i}x_{3i}+\\epsilon_{i}\\] summary(lm(Petal.Length ~ Sepal.Length + Species + Sepal.Length*Species, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Species + Sepal.Length * ## Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68611 -0.13442 -0.00856 0.15966 0.79607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8031 0.5310 1.512 0.133 ## Sepal.Length 0.1316 0.1058 1.244 0.216 ## Speciesversicolor -0.6179 0.6837 -0.904 0.368 ## Speciesvirginica -0.1926 0.6578 -0.293 0.770 ## Sepal.Length:Speciesversicolor 0.5548 0.1281 4.330 2.78e-05 *** ## Sepal.Length:Speciesvirginica 0.6184 0.1210 5.111 1.00e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2611 on 144 degrees of freedom ## Multiple R-squared: 0.9789, Adjusted R-squared: 0.9781 ## F-statistic: 1333 on 5 and 144 DF, p-value: &lt; 2.2e-16 The slope of the relationship between sepal length and petal length is calculated as follows, for each species: setosa \\((x_{2}=0, x_{3}=0): b_{1}=0.13\\) versicolor \\((x_{2}=1, x_{3}=0): b_{1} + b_{2} + b_{4} = 0.13+0.55 = 0.68\\) virginica \\((x_{2}=0, x_{3}=1): b_{1} + b_{3} + b_{5} = 0.13+0.62 = 0.75\\) Compare this to the estimates gained from the stratified model: coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;setosa&quot;))) ## (Intercept) Sepal.Length ## 0.8030518 0.1316317 coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;versicolor&quot;))) ## (Intercept) Sepal.Length ## 0.1851155 0.6864698 coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;virginica&quot;))) ## (Intercept) Sepal.Length ## 0.6104680 0.7500808 They’re the same! Proof that an interaction is equivalent to stratification. 7.4.2 Example What if we now wanted to include other predictors in the model? How does sepal length relate to petal length after controlling for petal width? We add the variable for petal width into the model summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa + Petal.Width, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa + Petal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.83519 -0.18278 -0.01812 0.17004 1.06968 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.86850 0.27028 -3.213 0.00162 ** ## Sepal.Length 0.66181 0.05179 12.779 &lt; 2e-16 *** ## setosa 1.83713 0.62355 2.946 0.00375 ** ## Petal.Width 0.97269 0.07970 12.204 &lt; 2e-16 *** ## Sepal.Length:setosa -0.61106 0.12213 -5.003 1.61e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2769 on 145 degrees of freedom ## Multiple R-squared: 0.9761, Adjusted R-squared: 0.9754 ## F-statistic: 1478 on 4 and 145 DF, p-value: &lt; 2.2e-16 So far, petal width, and the combination of species and sepal length are both significantly associated with petal length. Note of caution: Stratification implies that the stratifying variable interacts with all other variables. So if we were to go back to the stratified model where we fit the model of petal length on sepal length AND petal width, stratified by species, we would be implying that species interacts with both sepal length and petal width. E.g. the following stratified model \\(Y = A + B + C + D + C*D\\), when D=1 \\(Y = A + B + C + D + C*D\\), when D=0 is the same as the following interaction model: \\(Y = A + B + C + D + A*D + B*D + C*D\\) "],
["confounding.html", "7.5 Confounding", " 7.5 Confounding One primary purpose of a multivariable model is to assess the relationship between a particular explanatory variable \\(x\\) and your response variable \\(y\\), after controlling for other factors. As we just discussed, those other factors (characteristics/variables) could also be explaining part of the variability seen in \\(y\\). If the relationship between \\(x_{1}\\) and \\(y\\) is bivariately significant, but then no longer significant once \\(x_{2}\\) has been added to the model, then \\(x_{2}\\) is said to explain, or confound, the relationship between \\(x_{1}\\) and \\(y\\). All the ways covariates can affect response variables Credit: A blog about statistical musings Easy to read short article from a Gastroenterology journal on how to control confounding effects by statistical analysis. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4017459/ "],
["variable-selection-process.html", "7.6 Variable Selection Process", " 7.6 Variable Selection Process We want to choose a set of independent variables that both will yield a good prediction using as few variables as possible. We also need to consider controlling for moderators and confounders. In many situations where regression is used, the investigator has strong justification for including certain variables in the model. previous studies accepted theory The investigator may have prior justification for using certain variables but may be open to suggestions for the remaining variables. The set of independent variables can be broken down into logical subsets The usual demographics are entered first (age, gender, ethnicity) A set of variables that other studies have shown to affect the dependent variable A third set of variables that could be associated but the relationship has not yet been examined. Partially model-driven regression analysis and partially an exploratory analysis. Automated versions of variable selection processes should not be used blindly. 7.6.1 Automated selection procedures Forward selection: X variables added one at a time until optimal model reached Backward elimination: X variables removed one at a time until optimal model reached Stepwise selection: Combination of forward and backward. “… perhaps the most serious source of error lies in letting statistical procedures make decisions for you.” “Don’t be too quick to turn on the computer. Bypassing the brain to compute by reflex is a sure recipe for disaster.” Good and Hardin, Common Errors in Statistics (and How to Avoid Them), p. 3, p. 152 Take home message: Don’t use these blindly. Stopping criteria and algorithm can be different for different software programs. Can reject perfectly plausible models from later consideration Hides relationships between variables (X3 is added and now X1 is no longer significant. X1 vs X3 should be looked at) 7.6.2 The lesser of three evils: Best Subsets (PMA5 Section 8.7) Select one X with highest simple \\(r\\) with Y Select two X’s with highest multiple \\(r\\) with Y Select three X’s with highest multiple \\(r\\) with Y etc. Compute adjusted R2, AIC or BIC each time. Compare and choose among the “best subsets” of various sizes. Ways to conduct best subsets regression in R: https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html "],
["comparing-between-models.html", "7.7 Comparing between models", " 7.7 Comparing between models When working with multiple models, how do you choose the optimal model? How do we measure “optimal”? RMSE: Root Mean Squared Error How biased are the results? How ``far away&quot; are the estimates \\(\\hat{\\theta}\\) from the truth \\(\\theta\\)? \\[ \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} = \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}. \\] Maximize the Likelihood function What is the likelihood that we observed the data \\(x\\), given parameter values \\(\\theta\\). \\[ \\mathcal{L}(\\theta \\mid x)=p_{\\theta }(x)=P_{\\theta }(X=x) \\] For strictly convenient mathematical matters, we tend to work with the log-likelihood (LL). Great because \\(log\\) is a monotonic increasing function, maximizing the LL = maximizing the likelihood function. We can compare between models using functions based off the LL. There are several measures we can use to compare between competing models. Multiple \\(R^{2}\\) If the model explains a large amount of variation in the outcome that’s good right? So we could consider using \\(R^{2}\\) as a selection criteria and trying to find the model that maximizes this value. The residual sum of squares (RSS in the book or SSE) can be written as \\(\\sum(Y-\\hat{Y})^{2}(1-R^{2})\\). Therefore minimizing the RSS is equivalent to maximizing the multiple correlation coefficient. Problem: The multiple \\(R^{2}\\) always increases as predictors are added to the model. - Ex. 1: N = 100, P = 1, E(\\(R^{2}\\)) = 0.01 - Ex. 2: N = 21, P = 10, E(\\(R^{2}\\)) = 0.5 Problem: \\(R^{2} = 1-\\frac{Model SS}{Total SS}\\) is biased: If population \\(R^{2}\\) is really zero, then E(\\(R^{2}\\)) = P/(N-1). Adjusted \\(R^{2}\\) To alleviate bias use Mean squares instead of SS. \\(R^{2} = 1-\\frac{Model MS}{Total MS}\\) equivalently, \\(R^{2}_{adj} = R^{2} - \\frac{p(1-R^{2})}{n-p-1}\\) Now Adjusted \\(R^{2}\\) is approximately unbiased and won’t inflate as \\(p\\) increases. Mallows \\(C_{p}\\) \\[ C_{p} = (N-P-1)\\left(\\frac{RMSE}{\\hat{\\sigma}^{2}} -1 \\right) + (P+1) \\] Smaller is better When all variables are chosen, \\(P+1\\) is at it’s maximum but the other part of \\(C_{p}\\) is zero since \\(RMSE\\)==\\(\\hat{\\sigma}^{2}\\) Akaike Information Criterion (AIC) A penalty is applied to the deviance that increases as the number of parameters \\(p\\) increase. Tries to find a parsimonious model that is closer to the “truth”. Uses an information function, e.g., the likelihood function \\((LL)\\). \\[ AIC = -2LL + 2p\\] Smaller is better Can also be written as a function of the residual sum of squares (RSS) (in book) Estimates the information in one model relative to other models So if all models suck, your AIC will just tell you which one sucks less. Bayesian Information Criterion (BIC) Similar to AIC. Tries to find a parsimonious model that is more likely to be the “truth”. The smaller BIC, the better. \\[ BIC = -2LL + ln(N)*(P+1)\\] AIC vs BIC Both are “penalized likelihood” functions Each = -2log likelihood + penalty AIC: penalty = 2, BIC: penalty = ln(N) For any N &gt; 7, ln(N) &gt; 2 Thus, BIC penalizes larger models more heavily. They often agree. When they disagree, AIC chooses a larger model than BIC. "],
["what-to-watch-out-for-1.html", "7.8 What to watch out for", " 7.8 What to watch out for Multicollinearity Missing Data Use previous research as a guide Variables not included can bias the results Significance levels are only a guide Perform model diagnostics after selection to check model fit. Use common sense: A sub-optimal subset may make more sense than optimal one "],
["glm.html", "Chapter 8 Generalized Linear Models", " Chapter 8 Generalized Linear Models One of the primary assumptions with linear regression, is that the error terms have a specific distribution. Namely: \\[ \\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}) \\qquad i=1, \\ldots, n, \\quad \\mbox{and } \\epsilon_{i} \\perp \\epsilon_{j}, i \\neq j \\] When your outcome variable \\(y\\) is non-continuous/non-normal, the above assumption fails dramatically. Generalized Linear Models (GLM) allows for different data type outcomes by allowing the linear portion of the model (\\(\\mathbf{X}\\beta\\)) to be related to the outcome variable \\(y\\) using a link function, that allows the magnitude of the variance of the errors (\\(\\sigma\\)) to be related to the predicted values themselves. There are three overarching types of non-continuous outcomes that can be modeled with GLM’s. Binary data: Logistic or Probit regression Multinomial/categorical data: Multinomial or Ordinal Logistic regression. Count data: Poisson regression At this time these notes goes into depth for Logistic regression only, due to its commonplace in data analysis tools. "],
["fitting-glms-in-r.html", "8.1 Fitting GLMs in R", " 8.1 Fitting GLMs in R Generalized linear regression models can be fit in R using the glm() function. This function can fit an entire family of distributions and can be thought of as \\(E(Y|X) = C(X)\\) where \\(C\\) is the link function that relates \\(Y\\) to \\(X\\). Linear regression: C = Identity function (no change) Logistic regression: C = logit function Poisson regression: C = log function The general syntax is similar to lm(), with the additional required family= argument. See ?family for a list of options. glm(y ~ x1 + x2 + x3, data=DATA, family=&quot;binomial&quot;) "],
["binary-data.html", "8.2 Binary Data", " 8.2 Binary Data Goals: Assess the impact selected covariates have on the probability of an outcome occurring. Predict the likelihood / chance / probability of an event occurring given a certain covariate pattern. Binary data can be fit using a Logistic Model or a Probit Model. Consider an outcome variable \\(Y\\) with two levels: Y = 1 if event, = 0 if no event. Let \\(p_{i} = P(y_{i}=1)\\). The logistic model relates the probability of an event based on a linear combination of X’s. \\[ log\\left( \\frac{p_{i}}{1-p_{i}} \\right) = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\ldots + \\beta_{p}x_{pi} \\] Since the odds are defined as the probability an event occurs divided by the probability it does not occur: \\((p/(1-p))\\), the function \\(log\\left(\\frac{p_{i}}{1-p_{i}}\\right)\\) is also known as the log odds, or more commonly called the logit. This is the link function for the logistic regression model. This in essence takes a binary outcome 0/1 variable, turns it into a continuous probability (which only has a range from 0 to 1) Then the logit(p) has a continuous distribution ranging from \\(-\\infty\\) to \\(\\infty\\), which is the same form as a Multiple Linear Regression (continuous outcome modeled on a set of covariates) Back solving the logistic model for \\(p_{i} = e^{\\beta X} / (1+e^{\\beta X})\\) gives us the probability of an event. \\[ p_{i} = \\frac{e^{\\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\ldots + \\beta_{p}x_{pi}}} {1 + e^{\\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\ldots + \\beta_{p}x_{pi}}} \\] The probit function uses the inverse CDF for the normal distribution as the link function. 8.2.1 Example: The effect of gender on Depression Is gender associated with depression? Read in the depression data and recode sex to be an indicator of being male. depress &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/depress_081217.txt&quot;) names(depress) &lt;- tolower(names(depress)) # make all variable names lower case. Binary outcome variable: Symptoms of Depression (cases) Binary predictor variable: Gender (sex) as an indicator of being female The outcome \\(y\\) is a 0/1 Bernoulli random variable. The sum of a vector of Bernoulli’s (\\(\\sum_{i=1}^{n}y_{i}\\)) has a Binomial distribution. When we specify that family = &quot;binomial&quot; the glm() function auto-assigns “logit” link function. dep_sex_model &lt;- glm(cases ~ sex, data=depress, family=&quot;binomial&quot;) summary(dep_sex_model) ## ## Call: ## glm(formula = cases ~ sex, family = &quot;binomial&quot;, data = depress) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.7023 -0.7023 -0.4345 -0.4345 2.1941 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.3125 0.3315 -6.976 3.04e-12 *** ## sex 1.0386 0.3767 2.757 0.00583 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 268.12 on 293 degrees of freedom ## Residual deviance: 259.40 on 292 degrees of freedom ## AIC: 263.4 ## ## Number of Fisher Scoring iterations: 5 We exponentiate the coefficients to back transform the \\(\\beta\\) estimates into Odds Ratios exp(coef(dep_sex_model)) ## (Intercept) sex ## 0.0990099 2.8251748 Females have 2.8 times the odds of showing signs of depression compared to males. Confidence Intervals The OR is not a linear function of the \\(x&#39;s\\), but \\(\\beta\\) is. This means that a CI for the OR is created by calculating a CI for \\(\\beta\\), and then exponentiating the endpoints. A 95% CI for the OR can be calculated as: \\[e^{\\hat{\\beta} \\pm 1.96 SE_{\\beta}} \\] exp(confint(dep_sex_model)) ## 2.5 % 97.5 % ## (Intercept) 0.04843014 0.1801265 ## sex 1.39911056 6.2142384 8.2.2 Multiple Logistic Regression Just like multiple linear regression, additional predictors are simply included in the model using a + symbol. mvmodel &lt;- glm(cases ~ age + income + sex, data=depress, family=&quot;binomial&quot;) summary(mvmodel) ## ## Call: ## glm(formula = cases ~ age + income + sex, family = &quot;binomial&quot;, ## data = depress) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0249 -0.6524 -0.5050 -0.3179 2.5305 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.67646 0.57881 -1.169 0.24253 ## age -0.02096 0.00904 -2.318 0.02043 * ## income -0.03656 0.01409 -2.595 0.00946 ** ## sex 0.92945 0.38582 2.409 0.01600 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 268.12 on 293 degrees of freedom ## Residual deviance: 247.54 on 290 degrees of freedom ## AIC: 255.54 ## ## Number of Fisher Scoring iterations: 5 The sign of the \\(\\beta\\) coefficients can be interpreted in the same manner as with linear regression. The odds of being depressed are less if the respondent has a higher income and is older, and higher if the respondent is female. 8.2.3 Interpretation The OR provides a directly understandable statistic for the relationship between \\(y\\) and a specific \\(x\\) given all other \\(x\\)’s in the model are fixed. For a continuous variable X with slope coefficient \\(\\beta\\), the quantity \\(e^{b}\\) is interpreted as the ratio of the odds for a person with value (X+1) relative to the odds for a person with value X. \\(exp(kb)\\) is the incremental odds ratio corresponding to an increase of \\(k\\) units in the variable X, assuming that the values of all other X variables remain unchanged. Where does \\(OR = e^{\\beta}\\) come from? The full model is: \\[log(odds) = -0.676 - 0.02096*age - .03656*income + 0.92945*gender\\] We want to calculate the Odds Ratio of depression for women compared to men. \\[ OR = \\frac{Odds (Y=1|F)}{Odds (Y=1|M)} \\] Write out the equations for men and women separately. \\[ = \\frac{e^{-0.676 - 0.02096*age - .03656*income + 0.92945(1)}} {e^{-0.676 - 0.02096*age - .03656*income + 0.92945(0)}}\\] Applying rules of exponents to simplify. \\[ = \\frac{e^{-0.676}e^{- 0.02096*age}e^{- .03656*income}e^{0.92945(1)}} {e^{-0.676}e^{- 0.02096*age}e^{- .03656*income}e^{0.92945(0)}}\\] \\[ = \\frac{e^{0.92945(1)}} {e^{0.92945(0)}}\\] \\[ = e^{0.92945} \\] exp(.92945) ## [1] 2.533116 exp(coef(mvmodel)[4]) ## sex ## 2.533112 The odds of a female being depressed are 2.53 times greater than the odds for Males after adjusting for the linear effects of age and income (p=.016). 8.2.3.1 Effect of a k unit change Sometimes a 1 unit change in a continuous variable is not meaningful. exp(coef(mvmodel)) ## (Intercept) age income sex ## 0.5084157 0.9792605 0.9640969 2.5331122 exp(confint(mvmodel)) ## 2.5 % 97.5 % ## (Intercept) 0.1585110 1.5491849 ## age 0.9615593 0.9964037 ## income 0.9357319 0.9891872 ## sex 1.2293435 5.6586150 The Adjusted odds ratio (AOR) for increase of 1 year of age is 0.98 (95%CI .96, 1.0) How about a 10 year increase in age? \\(e^{10*\\beta_{age}} = e^{-.21} = .81\\) exp(10*coef(mvmodel)[2]) ## age ## 0.8109285 with a confidence interval of round(exp(10*confint(mvmodel)[2,]),3) ## 2.5 % 97.5 % ## 0.676 0.965 Controlling for gender and income, an individual has 0.81 (95% CI 0.68, 0.97) times the odds of being depressed compared to someone who is 10 years younger than them. 8.2.3.1.1 Example: The relationship between income, employment status and depression. This example follows PMA5 Ch 12.7 Here I create the binary indicators of lowincome (annual income &lt;$10k/year) and underemployed (part time or unemployed). depress$lowincome &lt;- ifelse(depress$income &lt; 10, 1, 0) table(depress$lowincome, depress$income, useNA=&quot;always&quot;) ## ## 2 4 5 6 7 8 9 11 12 13 15 16 18 19 20 23 24 25 26 27 28 31 ## 0 0 0 0 0 0 0 0 17 2 18 24 1 1 25 3 25 2 1 1 1 19 1 ## 1 7 8 10 12 18 14 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## &lt;NA&gt; 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## ## 32 35 36 37 42 45 55 65 &lt;NA&gt; ## 0 1 24 1 1 1 15 9 10 0 ## 1 0 0 0 0 0 0 0 0 0 ## &lt;NA&gt; 0 0 0 0 0 0 0 0 0 depress$underemployed &lt;- ifelse(depress$employ %in% c(&quot;PT&quot;, &quot;Unemp&quot;), 1, 0 ) table(depress$underemployed, depress$employ, useNA=&quot;always&quot;) ## ## FT Houseperson In School Other PT Retired Unemp &lt;NA&gt; ## 0 167 27 2 4 0 38 0 0 ## 1 0 0 0 0 42 0 14 0 ## &lt;NA&gt; 0 0 0 0 0 0 0 0 The Main Effects model assumes that the effect of income on depression is independent of employment status, and the effect of employment status on depression is independent of income. me_model &lt;- glm(cases ~ lowincome + underemployed, data=depress, family=&quot;binomial&quot;) summary(me_model) ## ## Call: ## glm(formula = cases ~ lowincome + underemployed, family = &quot;binomial&quot;, ## data = depress) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9085 -0.5843 -0.5279 -0.5279 2.0197 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.9003 0.2221 -8.556 &lt; 2e-16 *** ## lowincome 0.2192 0.3353 0.654 0.51322 ## underemployed 1.0094 0.3470 2.909 0.00363 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 268.12 on 293 degrees of freedom ## Residual deviance: 259.93 on 291 degrees of freedom ## AIC: 265.93 ## ## Number of Fisher Scoring iterations: 4 To formally test whether an interaction term is necessary, we add the interaction term into the model and assess whether the coefficient for the interaction term is significantly different from zero. me_intx_model &lt;- glm(cases ~ lowincome + underemployed + lowincome*underemployed, data=depress, family=&quot;binomial&quot;) summary(me_intx_model) ## ## Call: ## glm(formula = cases ~ lowincome + underemployed + lowincome * ## underemployed, family = &quot;binomial&quot;, data = depress) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3537 -0.5790 -0.5790 -0.4717 2.1219 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.7011 0.2175 -7.822 5.21e-15 *** ## lowincome -0.4390 0.4324 -1.015 0.31005 ## underemployed 0.2840 0.4501 0.631 0.52802 ## lowincome:underemployed 2.2615 0.7874 2.872 0.00408 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 268.12 on 293 degrees of freedom ## Residual deviance: 251.17 on 290 degrees of freedom ## AIC: 259.17 ## ## Number of Fisher Scoring iterations: 4 8.2.4 Goodness of Fit Tests to see if there is sufficient reason to believe that the data does not fit a logistic model \\(H_{0}\\) The data do come from a logistic model. \\(H_{a}\\) The data do not come from a logistic model. This means that a small p-value indicates that the model does not fit the data. We’ll look specifically at the Hosmer-Lemeshow (HL) Goodness of fit (GoF) test 8.2.4.1 HL GoF Compute the probability (\\(p_{i}\\)) of event (risk) for each observation. Sort data by this \\(p\\). Divide into \\(G\\) equal sized groups in ascending order (G=10 is common, i.e. split into deciles) Then for each group we calculate \\(O_{1g}\\): the observed number of events \\(E_{1g}\\): the expected number of events as the \\(\\sum_{i} p_{ig}\\) \\(O_{0g}\\): the observed number of non-events \\(E_{0g}\\): the expected number of events as the \\(1-\\sum_{i} p_{ig}\\) Then the HL test statistic (\\(H\\)) has a \\(\\chi^{2}\\) distribution and is is calculated as: \\[ H = \\sum_{g=1}^{G}\\left({\\frac {(O_{1g}-E_{1g})^{2}}{E_{1g}}}+{\\frac {(O_{0g}-E_{0g})^{2}}{E_{0g}}}\\right) \\sim \\chi^{2}_{G-2} \\] 8.2.4.2 HL GoF in R MKmisc::HLgof.test(fit = fitted(me_intx_model), obs = me_intx_model$y) ## $C ## ## Hosmer-Lemeshow C statistic ## ## data: fitted(me_intx_model) and me_intx_model$y ## X-squared = 5.614e-17, df = 1, p-value = 1 ## ## ## $H ## ## Hosmer-Lemeshow H statistic ## ## data: fitted(me_intx_model) and me_intx_model$y ## X-squared = 5.614e-17, df = 8, p-value = 1 A very low test statistic and a very high p-value indicate that this model fits the data well. "],
["classification-of-binary-outcomes.html", "8.3 Classification of Binary outcomes", " 8.3 Classification of Binary outcomes Sometimes Odds Ratios can be difficult to interpret or understand. Sometimes you just want to report the probability of the event occurring. Or sometimes you want to predict whether or not a new individual is going to have the event. For all of these, we need to calculate \\(p_{i} = P(y_{i}=1)\\), the probability of the event. For the main effects model of depression on age, income and gender the predicted probability of depression is: \\[ P(depressed) = \\frac{e^{-0.676 - 0.02096*age - .03656*income + 0.92945*gender}} {1 + e^{-0.676 - 0.02096*age - .03656*income + 0.92945*gender}} \\] Let’s compare the probability of being depressed for males and females separately, while holding age and income constant at their average value. depress %&gt;% summarize(age=mean(age), income=mean(income)) ## age income ## 1 44.41497 20.57483 Plug the coefficient estimates and the values of the variables into the equation and calculate. \\[ P(depressed|Female) = \\frac{e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(1)}} {1 + e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(1)}} \\] XB.f &lt;- -0.676 - 0.02096*(44.4) - .03656*(20.6) + 0.92945 exp(XB.f) / (1+exp(XB.f)) ## [1] 0.1930504 \\[ P(depressed|Male) = \\frac{e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(0)}} {1 + e^{-0.676 - 0.02096(44.4) - .03656(20.6) + 0.92945(0)}} \\] XB.m &lt;- -0.676 - 0.02096*(44.4) - .03656*(20.6) exp(XB.m) / (1+exp(XB.m)) ## [1] 0.08629312 The probability for a 44.4 year old female who makes $20.6k annual income has a 0.19 probability of being depressed. The probability of depression for a male of equal age and income is 0.86. 8.3.1 Calculating predictions So what if you want to get the model predicted probability of the event for all individuals in the data set? There’s no way I’m doing that calculation for every person in the data set. Using the main effects model from above, stored in the object mvmodel, we can call the predict() command to generate a vector of predictions for each row used in the model. Any row with missing data on any variable used in the model will NOT get a predicted value. The predict() function can calculate predictions for any GLM. The model object mvmodel stores the information that it was a logistic regression. model.pred.prob &lt;- predict(mvmodel, type=&#39;response&#39;) head(model.pred.prob) ## 1 2 3 4 5 6 ## 0.21108906 0.08014012 0.15266203 0.24527840 0.15208679 0.17056409 8.3.1.1 Distribution of Predictions How well does our model do to predict depression? plot.mpp &lt;- data.frame(prediction = model.pred.prob, truth = factor(mvmodel$y, labels=c(&quot;Not Depressed&quot;, &quot;Depressed&quot;))) ggplot(plot.mpp, aes(x=truth, y=prediction, fill=truth)) + geom_jitter(width=.2) + geom_violin(alpha=.4) + theme_bw() What things can you infer from this plot? Where should we put the cutoff value? At what probability should we classify a record as “depressed”? 8.3.1.2 Optimal Cutoff Value Often we adjust the cutoff value to improve accuracy. This is where we have to put our gut feeling of what probability constitutes “high risk”. For some models, this could be as low as 30%. It’s whatever the probability is that optimally separates the classes. Let’s look at two ways to visualize model performance as a function of cutoff. 8.3.2 ROC Curves We can create a Receiver operating characteristic (ROC) curve to help find that sweet spot. ROC curves show the balance between sensitivity and specificity. We’ll use the [ROCR] package. It only takes 3 commands: calculate prediction() using the model calculate the model performance() on both true positive rate and true negative rate for a whole range of cutoff values. plot the curve. The colorize option colors the curve according to the probability cutoff point. library(ROCR) pr &lt;- prediction(model.pred.prob, mvmodel$y) perf &lt;- performance(pr, measure=&quot;tpr&quot;, x.measure=&quot;fpr&quot;) plot(perf, colorize=TRUE, lwd=3, print.cutoffs.at=c(seq(0,1,by=0.1))) abline(a=0, b=1, lty=2) We can also use the performance() function and say we want to evaluate the \\(f1\\) measure perf.f1 &lt;- performance(pr,measure=&quot;f&quot;) plot(perf.f1) ROC curves: Can also be used for model comparison: http://yaojenkuo.io/diamondsROC.html The Area under the Curve (auc) also gives you a measure of overall model accuracy. auc &lt;- performance(pr, measure=&#39;auc&#39;) auc@y.values ## [[1]] ## [1] 0.695041 8.3.3 Model Performance Say we decide that a value of 0.15 is our optimal cutoff value. We can use this probability to classify each row into groups. The assigned class values must match the data type and levels of the true value. It also has to be in the same order, so the 0 group needs to come first. Then we calculate a [Confusion Matrix] using the similarly named function from the caret package. At it’s core, this is a 2x2 table containing counts of each combination of predicted value and true value. library(caret) plot.mpp$pred.class &lt;- ifelse(plot.mpp$prediction &lt;0.15, 0,1) plot.mpp$pred.class &lt;- factor(plot.mpp$pred.class, labels=c(&quot;Not Depressed&quot;, &quot;Depressed&quot;)) confusionMatrix(plot.mpp$pred.class, plot.mpp$truth, positive=&quot;Depressed&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Not Depressed Depressed ## Not Depressed 123 10 ## Depressed 121 40 ## ## Accuracy : 0.5544 ## 95% CI : (0.4956, 0.6121) ## No Information Rate : 0.8299 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.1615 ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.8000 ## Specificity : 0.5041 ## Pos Pred Value : 0.2484 ## Neg Pred Value : 0.9248 ## Prevalence : 0.1701 ## Detection Rate : 0.1361 ## Detection Prevalence : 0.5476 ## Balanced Accuracy : 0.6520 ## ## &#39;Positive&#39; Class : Depressed ## 123 people were correctly predicted to not be depressed (True Negative, \\(n_{11}\\)) 121 people were incorrectly predicted to be depressed (False Positive, \\(n_{21}\\)) 10 people were incorrectly predicted to not be depressed (False Negative, \\(n_{12}\\)) 40 people were correctly predicted to be depressed (True Positive, \\(n_{22}\\)) Other terminology: Sensitivity/Recall/True positive rate: P(predicted positive | total positive) = 40/(10+40) = .8 Specificity/true negative rate: P(predicted negative | total negative) = 123/(123+121) = .504 Precision/positive predicted value: P(true positive | predicted positive) = 40/(121+40) = .2484 Accuracy: (TP + TN)/ Total: (40 + 123)/(40+123+121+10) = .5544 Balanced Accuracy: \\([(n_{11}/n_{.1}) + (n_{22}/n_{.2})]/2\\) - This is to adjust for class size imbalances (like in this example) F1 score: the harmonic mean of precision and recall. This ranges from 0 (bad) to 1 (good): \\(2*\\frac{precision*recall}{precision + recall}\\) = 2*(.2484*.8)/(.2484+.8) = .38 "],
["categorical-data.html", "8.4 Categorical Data", " 8.4 Categorical Data Multinomial Regression Ordinal Logistic Regression "],
["count-data.html", "8.5 Count Data", " 8.5 Count Data Lets consider modeling the distribution of the number of of occurrences of a rare event in a specified period of time - e.g. Number of thunderstorms in a year If we assume: Rate (\\(\\mu\\)) is fixed over time Successive occurrences independent of each other Then we can use the Poisson distribution. \\[ P(Y=y) = e^{-\\mu}\\frac{\\mu^{y}}{y!} \\] The Poisson distribution has a distinct feature where the mean of the distribution \\(\\mu\\), is also the variance. Plot of Histogram of a Poisson Distribution with a Mean of 5 and a Normal Curve 8.5.0.1 Poisson Regression Just another GLM - we use a \\(ln\\) as the link function. This lets us model the log rates using a linear combination of covariates. \\[ ln(\\mu) = \\mathbf{X}\\beta \\] Then the expected rate of events per unit of time is: \\[ \\mu = e^{\\mathbf{X}\\beta} \\] This model assumes that the time of “exposure” for each record is identical. Number of cigarettes per month Number of epileptic seizures per week Number of people with lung cancer in four cities If this is not the case (often), then this model needs to include an offset. e.g. observing each patient for epileptic seizures for a different number of days accounting for different sizes or structures of populations of interest (e.g. different cities with lung cancer) What actually gets fit in glm is the model of expected counts, rather than rates, with an offset for the time period \\(T\\). If all time periods are the same, then T is constant, and a linear combination of the intercept, thus dropped from the model. \\[ ln(\\lambda) = \\mathbf{X}\\beta + ln(T) \\] While this offset will be added to the regression model as if it were another variable, it’s not quite the same because the regression coefficient for the \\(ln(T)\\) term is fixed at 1. The generic formula for fitting a poisson model using glm is: glm(y ~ x1 + x2 + offset(log(T)), family=&#39;poisson&#39;) or alternatively as an argument glm(y ~ x1 + x2, offset = log(T), family=&#39;poisson&#39;) The interpretation of the \\(\\beta\\) regression coefficients are differences in the log rate (or the log rate-ratio). So, just like with a logistic regression often we back-transform the coefficients by exponentiating before interpreting. So \\(e^{\\beta}\\) is now the rate-ratio. The intercept term is not a ratio, but a baseline rate when all covariates are 0 For other covariates, the coefficient is the relative change per unit change in the covariate. one year older males vs females Also, similar to logistic regression, since the outcome was transformed, the standard errors are not useful or interpretable as is. To calculate confidence intervals for the rate ratios, calculate the CI for \\(\\beta\\) exponentiate each end point. 8.5.0.2 Example: Modeling counts from the Add Health data Wave IVset. better example forthcoming Let’s model the number of siblings someone has, based off their age at Wave 1 (2008). Visualize hist(addhealth$nsib, xlab=&quot;Number of siblings&quot;, ylab=&quot;Count&quot;, main=&quot;&quot;,axes=FALSE, ylim=c(0,3000)) axis(1);axis(2, las=2);box() nsib.model &lt;- glm(nsib ~ agew1 + female, data=addhealth, family=&quot;poisson&quot;) pander(summary(nsib.model)) Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.2647 0.1014 2.611 0.009019 agew1 0.0443 0.005989 7.397 1.39e-13 female 0.0969 0.01909 5.076 3.851e-07 (Dispersion parameter for poisson family taken to be 1 ) Null deviance: 6411 on 3917 degrees of freedom Residual deviance: 6335 on 3915 degrees of freedom betas &lt;- cbind(coef(nsib.model), confint(nsib.model)) kable(exp(betas), digits=3) 2.5 % 97.5 % (Intercept) 1.303 1.068 1.589 agew1 1.045 1.033 1.058 female 1.102 1.061 1.144 "],
["mv-intro.html", "Chapter 9 Introduction", " Chapter 9 Introduction There is an important distinction between multivariable and multivariate models. multivariable : Multiple predictor variables (\\(\\mathbf{x}\\)). multivariate: Multiple response variables (\\(\\mathbf{Y}\\)). Often analysts will misuse multivaraite when they really mean multivariable. Multivariate techniques are Primarily used as an exploratory technique Restructure interrelated variables Simplify description Reduce dimensionality Avoid multicollinearity problems in regression We will discuss two different, but related techniques: Principal Component Analysis and Factor Analysis. "],
["pca.html", "Chapter 10 Principal Component Analysis", " Chapter 10 Principal Component Analysis More nomenclature tidbits: It’s “Principal” Components (adjective), not “Principle” Components (noun) From Grammerist: As a noun, principal refers to (1) one who holds a presiding position or rank, and (2) capital or property before interest, and it’s also an adjective meaning (3) first or most important in rank Principle is only a noun. In its primary sense, it refers to a basic truth, law, assumption, or rule. This third definition (3) is the context in which we will be using this term. Not variable selection Principal Components Analysis (PCA) differs from variable selection in two ways: No dependent variable exists Variables are not eliminated but rather summary variables, i.e., principal components, are computed from all of the original variables. We are trying to understand a phenomenon by collecting a series of component measurements, but the underlying mechanics is complex and not easily understood by simply looking at each component individually. The data could be redundant and high levels of multicolinearity may be present. "],
["basic-idea.html", "10.1 Basic Idea", " 10.1 Basic Idea Consider a hypothetical data set that consists of 100 random pairs of observations \\(X_{1}\\) and \\(X_{2}\\) that are correlated. Let \\(X_{1} \\sim \\mathcal{N}(100, 100)\\), \\(X_{2} \\sim \\mathcal{N}(50, 50)\\), with \\(\\rho_{12} = \\frac{1}{\\sqrt{2}}\\). In matrix notation this is written as: \\(\\mathbf{X} \\sim \\mathcal{N}\\left(\\mathbf{\\mu}, \\mathbf{\\Sigma}\\right)\\) where \\[\\mathbf{\\mu} = \\left(\\begin{array} {r} \\mu_{1} \\\\ \\mu_{2} \\end{array}\\right), \\mathbf{\\Sigma} = \\left(\\begin{array} {cc} \\sigma_{1}^{2} &amp; \\rho_{12}\\sigma_{x}\\sigma_{y} \\\\ \\rho_{12}\\sigma_{x}\\sigma_{y} &amp; \\sigma_{2}^{2} \\end{array}\\right) \\]. set.seed(456) m &lt;- c(100, 50) s &lt;- matrix(c(100, sqrt(.5*100*50), sqrt(.5*100*50), 50), nrow=2) data &lt;- data.frame(MASS::mvrnorm(n=100, mu=m, Sigma=s)) colnames(data) &lt;- c(&quot;X1&quot;, &quot;X2&quot;) plot(X2 ~ X1, data=data, pch=16) Goal: Create two new variables \\(C_{1}\\) and \\(C_{2}\\) as linear combinations of \\(\\mathbf{x_{1}}\\) and \\(\\mathbf{x_{2}}\\) \\[ \\mathbf{C_{1}} = a_{11}\\mathbf{x_{1}} + a_{12}\\mathbf{x_{2}} \\] \\[ \\mathbf{C_{2}} = a_{21}\\mathbf{x_{1}} + a_{22}\\mathbf{x_{2}} \\] or more simply \\(\\mathbf{C = aX}\\), where The \\(\\mathbf{x}\\)’s have been centered by subtracting their mean (\\(\\mathbf{x_{1}} = x_{1}-\\bar{x_{1}}\\)) \\(Var(C_{1})\\) is as large as possible Graphically we’re creating two new axes, where now \\(C_{1}\\) and \\(C_{2}\\) are uncorrelated. PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. Wikipedia "],
["more-generally.html", "10.2 More Generally", " 10.2 More Generally We want From \\(P\\) original variables \\(X_{1}, \\ldots , X_{P}\\) get \\(P\\) principal components \\(C_{1}, \\ldots , C_{P}\\) Where each \\(C_{j}\\) is a linear combination of the \\(X_{i}\\)’s: \\(C_{j} = a_{j1}X_{1} + a_{j2}X_{2} + \\ldots + a_{jP}X_{P}\\) The coefficients are chosen such that \\(Var(C_{1}) \\geq Var(C_{2}) \\geq \\ldots \\geq Var(C_{P})\\) Variance is a measure of information. Consider modeling prostate cancer. Gender has 0 variance. No information. Size of tumor: the variance is &gt; 0, it provides useful information. Any two PC’s are uncorrelated: \\(Cov(C_{i}, C_{j})=0, \\quad \\forall i \\neq j\\) We have \\[ \\left[ \\begin{array}{r} C_{1} \\\\ C_{2} \\\\ \\vdots \\\\ C_{P} \\end{array} \\right] = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1P} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2P} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{P1} &amp; a_{P2} &amp; \\ldots &amp; a_{PP} \\end{array} \\right] \\left[ \\begin{array}{r} X_{1} \\\\ X_{2} \\\\ \\vdots \\\\ X_{P} \\end{array} \\right] \\] Hotelling (1933) showed that the \\(a_{ij}\\)’s are solutions to \\((\\mathbf{\\Sigma} -\\lambda\\mathbf{I})\\mathbf{a}=\\mathbf{0}\\). \\(\\mathbf{\\Sigma}\\) is the variance-covariance matrix of the \\(\\mathbf{X}\\) variables. This means \\(\\lambda\\) is an eigenvalue and \\(\\mathbf{a}\\) an eigenvector of the covariance matrix \\(\\mathbf{\\Sigma}\\). Problem: There are infinite number of possible \\(\\mathbf{a}\\)’s Solution: Choose \\(a_{ij}\\)’s such that the sum of the squares of the coefficients for any one eigenvector is = 1. \\(P\\) unique eigenvalues and \\(P\\) corresponding eigenvectors. Which gives us Variances of the \\(C_{j}\\)’s add up to the sum of the variances of the original variables (total variance). Can be thought of as variance decomposition into orthogonal (independet) vectors (variables). With \\(Var(C_{1}) \\geq Var(C_{2}) \\geq \\ldots \\geq Var(C_{P})\\). "],
["generating-pcs-using-r.html", "10.3 Generating PC’s using R", " 10.3 Generating PC’s using R Calculating the principal components in R can be done using a call to the function prcomp(). STHDA has a good overview of the difference between prcomp() and princomp(). pr &lt;- princomp(data) summary(pr) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 11.4019265 4.2236767 ## Proportion of Variance 0.8793355 0.1206645 ## Cumulative Proportion 0.8793355 1.0000000 The summary output above shows the first PC (Comp.1) explains the highest proportion of variance. The values for the matrix \\(\\mathbf{A}\\) is contained in pr$loadings. pr$loadings ## ## Loadings: ## Comp.1 Comp.2 ## X1 -0.854 0.519 ## X2 -0.519 -0.854 ## ## Comp.1 Comp.2 ## SS loadings 1.0 1.0 ## Proportion Var 0.5 0.5 ## Cumulative Var 0.5 1.0 To visualize these new axes, we plot the centered data. a &lt;- pr$loadings x1 &lt;- with(data, X1 - mean(X1)) x2 &lt;- with(data, X2 - mean(X2)) plot(c(-40, 40), c(-20, 20), type=&quot;n&quot;,xlab=&quot;x1&quot;, ylab=&quot;x2&quot;) points(x=x1, y=x2, pch=16) abline(0, a[2,1]/a[1,1]); text(30, 10, expression(C[1])) abline(0, a[2,2]/a[1,2]); text(-10, 20, expression(C[2])) Plot the original data on the new axes we see that PC1 and PC2 are uncorrelated. The red vectors show you where the original coordinates were at. biplot(pr) "],
["using-the-correlation-matrix.html", "10.4 Using the correlation matrix", " 10.4 Using the correlation matrix Standardizing: Take \\(X\\) and divide each element by \\(\\sigma_{x}\\). \\(Z = X/\\sigma_{X}\\) Side note: Standardizing and centering == normalizing \\(Z = (X-\\bar{X})/\\sigma_{X}\\) Equivalent to analyzing the correlation matrix (\\(\\mathbf{R}\\)) instead of covariance matrix (\\(\\mathbf{\\Sigma}\\)). Using correlation matrix vs covariance matrix will generate different PC’s This makes sense given the difference in matricies: cov(data) #Covariance Matrix ## X1 X2 ## X1 100.74146 50.29187 ## X2 50.29187 48.59528 cor(data) #Correlation Matrix ## X1 X2 ## X1 1.0000000 0.7187811 ## X2 0.7187811 1.0000000 Standardizing your data prior to analysis aids the interpretation of the PC’s in a few ways The total variance is the number of variables \\(P\\) The proportion explained by each PC is the corresponding eigenvalue / \\(P\\) The correlation between \\(C_{i}\\) and standardized variable \\(x_{j}\\) can be written as \\(r_{ij} = a_{ij}SD(C_{i})\\) This last point means that for any given \\(C_{i}\\) we can quantify the relative degree of dependence of the PC on each of the standardized variables. This is a.k.a. the factor loading (we will return to this key term later). To calculate the principal components using the correlation matrix, you just need to specify that you want cor=TRUE. pr_corr &lt;- princomp(data, cor=TRUE) summary(pr_corr) ## Importance of components: ## Comp.1 Comp.2 ## Standard deviation 1.3110229 0.5303008 ## Proportion of Variance 0.8593906 0.1406094 ## Cumulative Proportion 0.8593906 1.0000000 If we use the covariance matrix and change the scale of a variable (i.e. in to cm) that will change the results of the PC’s Many researchers prefer to use the correlation matrix It compensates for the units of measurements for the different variables. Interpretations are made in terms of the standardized variables. "],
["data-reduction.html", "10.5 Data Reduction", " 10.5 Data Reduction Keep first \\(m\\) principal components as representatives of original P variables Keep enough to explain a large percentage of original total variance. Ideally you want a small number of PC’s that explain a large percentage of the total variance. Choosing \\(m\\) Rely on existing theory Explain a given % of variance (cumulative percentage plot) All eigenvalues &gt; 1 (Scree plot) Elbow rule (Scree Plot) These last two will be best explained using an example. "],
["example-analysis-of-depression.html", "10.6 Example Analysis of depression", " 10.6 Example Analysis of depression This example follows Analysis of depression data set section in PMA5 Section 14.5. This survey asks 20 questions on emotional states that relate to depression. Here I use PCA to reduce these 20 correlated variables down to a few uncorrelated variables that explain the most variance. 1. Read in the data and run princomp on the C1:C20 variables. depress &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/depress_081217.txt&quot;, header=TRUE) pc_dep &lt;- princomp(depress[,9:28], cor=TRUE) summary(pc_dep) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 2.6562036 1.21883931 1.10973409 1.03232021 ## Proportion of Variance 0.3527709 0.07427846 0.06157549 0.05328425 ## Cumulative Proportion 0.3527709 0.42704935 0.48862483 0.54190909 ## Comp.5 Comp.6 Comp.7 Comp.8 ## Standard deviation 1.00629648 0.98359581 0.97304489 0.87706188 ## Proportion of Variance 0.05063163 0.04837304 0.04734082 0.03846188 ## Cumulative Proportion 0.59254072 0.64091375 0.68825457 0.72671645 ## Comp.9 Comp.10 Comp.11 Comp.12 ## Standard deviation 0.83344885 0.81248191 0.77950975 0.74117295 ## Proportion of Variance 0.03473185 0.03300634 0.03038177 0.02746687 ## Cumulative Proportion 0.76144830 0.79445464 0.82483641 0.85230328 ## Comp.13 Comp.14 Comp.15 Comp.16 ## Standard deviation 0.73255278 0.71324438 0.67149280 0.61252016 ## Proportion of Variance 0.02683168 0.02543588 0.02254513 0.01875905 ## Cumulative Proportion 0.87913496 0.90457083 0.92711596 0.94587501 ## Comp.17 Comp.18 Comp.19 Comp.20 ## Standard deviation 0.56673129 0.54273638 0.51804873 0.445396635 ## Proportion of Variance 0.01605922 0.01472814 0.01341872 0.009918908 ## Cumulative Proportion 0.96193423 0.97666237 0.99008109 1.000000000 2. Pick a subset of PC’s to work with In the cumulative percentage plot below, I drew a horizontal line at 80%. So the first 9 PC’s explain around 75% of the total variance, and the first 10 can explain around 80%. library(ggplot2) var_pc &lt;- (pc_dep$sdev)^2 qplot(x=1:20, y=cumsum(var_pc)/sum(var_pc)*100, geom=&quot;point&quot;) + xlab(&quot;PC number&quot;) + ylab(&quot;Cumulative %&quot;) + ylim(c(0,100)) + geom_hline(aes(yintercept=80)) 3. Create a Scree plot by plotting the eigenvalue against the PC number. qplot(x=1:20, y=var_pc, geom=c(&quot;point&quot;, &quot;line&quot;)) + xlab(&quot;PC number&quot;) + ylab(&quot;Eigenvalue&quot;) + ylim(c(0,8)) Option 1: Take all eigenvalues &gt; 1 (\\(m=5\\)) Option 2: Use a cutoff point where the lines joining consecutive points are steep to the left of the cutoff point and flat right of the cutoff point. Point where the two slopes meet is the elbow. (\\(m=2\\)). 4. Examine the loadings pc_dep$loadings[1:3,1:5] ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## c1 0.2774384 -0.14497938 0.05770239 -0.002723687 -0.08826773 ## c2 0.3131829 0.02713557 0.03162990 0.247811083 -0.02439748 ## c3 0.2677985 -0.15471968 0.03459037 0.247246879 0.21830547 Here \\(X_{1}\\) = “I felt that I could not shake…” \\(X_{2}\\) = “I felt depressed…” So the PC’s are calculated as \\[ C_{1} = 0.277x_{1} + 0.313x_{2} + \\ldots \\\\ C_{2} = -0.1449x_{1} + 0.0271x_{2} + \\ldots \\] etc… The full question text for the depression data used here can be found on Table 15.7 in the PMA5 textbook. 5. Interpret the PC’s Visualize the loadings using heatmap.2() in the gplots package. I reversed the colors so that red was high positive correlation and yellow/white is low. half the options I use below come from this SO post. I had no idea what they did, so I took what the solution showed, and played with it (added/changed some to see what they did), and reviewed ?heatmap.2 to see what options were available. library(gplots) heatmap.2(pc_dep$loadings[,1:5], scale=&quot;none&quot;, Rowv=NA, Colv=NA, density.info=&quot;none&quot;, dendrogram=&quot;none&quot;, trace=&quot;none&quot;, col=rev(heat.colors(256))) Loadings over 0.5 (red) help us interpret what these components could “mean” Must know exact wording of component questions \\(C_{1}\\): a weighted average of most items. High value indicates the respondent had many symptoms of depression. Note sign of loadings are all positive and all roughly the same color. Recall \\(C_{2}\\): lethargy (high energetic). High loading on c14, 16, 17, low on 4, 8, 20 \\(C_{3}\\): friendliness of others. Large negative loading on c19, c9 etc. "],
["use-in-multiple-regression.html", "10.7 Use in Multiple Regression", " 10.7 Use in Multiple Regression Choose a handful of few principal components to use as predictors in a regression model Leads to more stable regression estimates. Alternative to variable selection Ex: several measures of behavior. Use PC\\(_{1}\\) or PC\\(_{1}\\) and PC\\(_{2}\\) as summary measures of all. "],
["things-to-watch-out-for.html", "10.8 Things to watch out for", " 10.8 Things to watch out for Eigenvalues are estimated variances of the PC’s and so are subject to large sample variations. The size of variance of last few principal components can be useful as indicator of multicollinearity among original variables Principal components derived from standardized variables differ from those derived from original variables Important that measurements are accurate, especially for detection of collinearity Arbitrary cutoff points should not be taken too seriously "],
["additional-references.html", "10.9 Additional References", " 10.9 Additional References A collection of other tools and websites that do a good job of explaining PCA. Principal Component Analysis Essentials in R tutorial by [STHDA]](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/) Stack Overflow This has animations, and walks through the explanation using wine and “how you would explain it to your grandma”. "],
["fa.html", "Chapter 11 Factor Analysis", " Chapter 11 Factor Analysis Example 1 An investigator has asked each respondent in a survey whether he or she strongly agrees, agrees, is undecided, disagrees, or strongly disagrees with 15 statements concerning attitudes toward inflation. As a first step, the investigator will do a factor analysis on the resulting data to determine which statements belong together in sets that are uncorrelated with other sets. The particular statements that form a single set will be examined to obtain a better understanding of attitudes toward inflation. Scores derived from each set or factor will be used in subsequent analysis to predict consumer spending. Example 2 There are Fifty test questions Each is a function of 3 factors: Verbal, quantitative, analytical skills You are interested in measuring the gain in analytical skills over the course of a week after doing some task. "],
["introduction.html", "11.1 Introduction", " 11.1 Introduction This set of notes uses functions from several new packages. See the links in the Additional Resources section for more information library(corrplot) library(psych) library(ggfortify) # plots scores from `factanal()` library(GPArotation) # to do oblimin rotation 11.1.1 Latent Constructs Latent variables are ones that cannot be measured directly; e.g. Depression, Anxiety, Mathematical ability. They drive how we would respond to various tasks and questions that can be measured; vocabulary, arithmetic, statistical reasoning. How can the correlation in responses to questions help us measure these latent constructs? Factor Analysis aims to Generalize of principal components analysis Explain interrelationships among a set of variables Where we select a small number of factors to convey essential information Can perform additional analyses to improve interpretation 11.1.2 EFA vs CFA Exploratory Factor Analysis Explore the possible underlying factor structure of a set of observed variables Does not impose a preconceived structure on the outcome. Confirmatory Factor Analysis Verifies the theoretical factor structure of a set of observed variables Test the relationship between observed variables and theoretical underlying latent constructs Variable groupings are determined ahead of time. "],
["factor-model.html", "11.2 Factor Model", " 11.2 Factor Model Start with P standardized variables. That is \\(\\frac{(x_{i}-\\bar{x})}{s_{i}}\\). So for the rest of these FA notes, understand that each \\(X\\) written has already been standardized. Express each variable as (its own) linear combination of \\(m\\) common factors plus a unique factor \\(e\\). \\(m &lt;&lt; P\\). Ideally \\(m\\) is known in advance \\[ X_{1} = l_{11}F_{1} + l_{12}F_{2} + \\ldots + l_{1m}F_{m} + e_{1} \\\\ X_{2} = l_{21}F_{1} + l_{22}F_{2} + \\ldots + l_{2m}F_{m} + e_{1} \\\\ \\vdots \\\\ X_{P} = l_{P1}F_{1} + l_{P2}F_{2} + \\ldots + l_{Pm}F_{m} + e_{P} \\] \\(X_{i} = \\sum l_{ij} F_{j}+ \\epsilon_{i}\\) \\(F_{j}\\) = common or latent factors \\(e_{i}\\) = unique factors \\(l_{ij}\\) = coefficients of common factors = factor loadings Each \\(F_{j}\\) has mean 0 and variance 1 \\(F_{j}\\)’s are uncorrelated $e_{i}’s and \\(F_{j}\\)’s are uncorrelated How does this compare to the equations for Principal Components? 11.2.1 Comparison with PCA Similar in that no dependent variable PCA: Select a number of components that explain as much of the total variance as possible. FA: Factors selected mainly to explain the interrelationships among the original variables. Ideally, the number of factors expected is known in advance. Major emphasis is placed on obtaining easily understandable factors that convey the essential information contained in the original set of variables. https://www.researchgate.net/figure/Conceptual-distinction-between-factor-analysis-and-principal-component-analysis-ote-An_fig1_47386956 Mirror image of PCA Each PC is expressed as a linear combination of X’s Each \\(X\\) is expressed as a linear combination of Factors 11.2.2 Implications Variance of any original \\(X\\) is composed of communality: part due to common factors specificity: part due to a unique factor = 1 when \\(X\\)’s are standardized. \\[ V(X_{i}) = communality + specificity \\\\ \\qquad = h^{2}_{i} + u^{2}_{i} \\] 11.2.3 Two big steps Initial factor extraction: estimate loadings and communalities Factor “rotations” to improve interpretation "],
["fa-example.html", "11.3 Example setup", " 11.3 Example setup Generate 100 data points from the following multivariate normal distribution: \\[\\mathbf{\\mu} = \\left(\\begin{array} {r} 0.163 \\\\ 0.142 \\\\ 0.098 \\\\ -0.039 \\\\ -0.013 \\end{array}\\right), \\mathbf{\\Sigma} = \\left(\\begin{array} {cc} 1 &amp; &amp; &amp; &amp; &amp; \\\\ 0.757 &amp; 1 &amp; &amp; &amp; &amp; \\\\ 0.047 &amp; 0.054 &amp; 1 &amp; &amp; &amp; \\\\ 0.155 &amp; 0.176 &amp; 0.531 &amp; 1 &amp; \\\\ 0.279 &amp; 0.322 &amp; 0.521 &amp; 0.942 &amp; 1 \\end{array}\\right) \\]. set.seed(456) m &lt;- c(0.163, 0.142, 0.098, -0.039, -0.013) s &lt;- matrix(c(1.000, 0.757, 0.047, 0.155, 0.279, 0.757, 1.000, 0.054, 0.176, 0.322, 0.047, 0.054, 1.000, 0.531, 0.521, 0.155, 0.176, 0.531, 1.000, 0.942, 0.279, 0.322, 0.521, 0.942, 1.000), nrow=5) data &lt;- data.frame(MASS::mvrnorm(n=100, mu=m, Sigma=s)) colnames(data) &lt;- paste0(&quot;X&quot;, 1:5) Standardize the \\(X\\)’s. stan.dta &lt;- as.data.frame(scale(data)) The hypothetical data model is that these 5 variables are generated from 2 underlying factors. \\[ \\begin{equation} \\begin{aligned} X_{1} &amp;= (1)*F_{1} + (0)*F_{2} + e_{1} \\\\ X_{2} &amp;= (1)*F_{1} + (0)*F_{2} + e_{2} \\\\ X_{3} &amp;= (0)*F_{1} + (.5)*F_{2} + e_{3} \\\\ X_{4} &amp;= (0)*F_{1} + (1.5)*F_{2} + e_{4} \\\\ X_{5} &amp;= (0)*F_{1} + (2)*F_{2} + e_{5} \\\\ \\end{aligned} \\end{equation} \\] Implications \\(F_{1}, F_{2}\\) and all \\(e_{i}\\)’s are independent normal variables The first two \\(X\\)’s are inter-correlated, and the last 3 \\(X\\)’s are inter-correlated The first 2 \\(X\\)’s are NOT correlated with the last 3 \\(X\\)’s #library(corrplot) corrplot(cor(stan.dta), tl.col=&quot;black&quot;) "],
["fa-extract.html", "11.4 Factor Extraction", " 11.4 Factor Extraction Methods PC Factor Model Iterated PC Factor Model Maximum Likelihood Choose the first \\(m\\) principal components and modify them to fit the factor model defined in the previous section. They explain the greatest proportion of the variance and are therefore the most important extract_pca &lt;- princomp(stan.dta) var_pc &lt;- (extract_pca$sdev)^2 qplot(x=1:length(var_pc), y=var_pc, geom=c(&quot;point&quot;, &quot;line&quot;)) + xlab(&quot;PC number&quot;) + ylab(&quot;Eigenvalue&quot;) 11.4.1 PC Factor Model Recall that \\(\\mathbf{C} = \\mathbf{A}\\mathbf{X}\\), C’s are a function of X \\[ X_{1} = a_{11}C_{1} + a_{12}C_{2} + \\ldots + a_{1P}C_{p} \\] We want the reverse: X’s are a function of F’s. Use the inverse! –&gt; If \\(c = 5x\\) then \\(x = 5^{-1}C\\) The inverse PC model is \\(\\mathbf{X} = \\mathbf{A}^{-1}\\mathbf{C}\\). Since \\(\\mathbf{A}\\) is orthogonal, \\(\\mathbf{A}^{-1} = \\mathbf{A}^{T} = \\mathbf{A}^{&#39;}\\), so \\[ X_{1} = a_{11}C_{1} + a_{21}C_{2} + \\ldots + a_{P1}C_{p} \\] But there are more PC’s than Factors… \\[ \\begin{equation} \\begin{aligned} X_{i} &amp;= \\sum_{j=1}^{P}a_{ji}C_{j} \\\\ &amp;= \\sum_{j=1}^{m}a_{ji}C_{j} + \\sum_{j=m+1}^{m}a_{ji}C_{j} \\\\ &amp;= \\sum_{j=1}^{m}l_{ji}F_{j} + e_{i} \\\\ \\end{aligned} \\end{equation} \\] Adjustment \\(V(C_{j}) = \\lambda_{j}\\) not 1 We transform: \\(F_{j} = C_{j}\\lambda_{j}^{-1/2}\\) Now \\(V(F_{j}) = 1\\) Loadings: \\(l_{ij} = \\lambda_{j}^{1/2}a_{ji}\\) \\(l_{ij}\\) is the correlation coefficient between variable \\(i\\) and factor \\(j\\) 11.4.2 Iterated PC Factor Model Select common factors to maximize the total communality Get initial communality estimates Use these (instead of original variances) to get the PC’s and factor loadings Get new communality estimates Rinse and repeat Stop when no appreciable changes occur. 11.4.3 Maximum Likelihood Assume that all the variables are normally distributed Use Maximum Likelihood to estimate the parameters 11.4.4 Example PC Factor Model using the principal function in the psych package. #library(psych) pc.extract.norotate &lt;- principal(stan.dta, nfactors=2, rotate=&quot;none&quot;) print(pc.extract.norotate) ## Principal Components Analysis ## Call: principal(r = stan.dta, nfactors = 2, rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## X1 0.53 0.78 0.90 0.104 1.8 ## X2 0.59 0.74 0.89 0.106 1.9 ## X3 0.70 -0.39 0.64 0.360 1.6 ## X4 0.87 -0.38 0.90 0.099 1.4 ## X5 0.92 -0.27 0.91 0.087 1.2 ## ## PC1 PC2 ## SS loadings 2.71 1.53 ## Proportion Var 0.54 0.31 ## Cumulative Var 0.54 0.85 ## Proportion Explained 0.64 0.36 ## Cumulative Proportion 0.64 1.00 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.08 ## with the empirical chi square 12.61 with prob &lt; 0.00038 ## ## Fit based upon off diagonal values = 0.97 \\[ \\begin{equation} \\begin{aligned} X_{1} &amp;= 0.53F_{1} + 0.78F_{2} + e_{1} \\\\ X_{2} &amp;= 0.59F_{1} + 0.74F_{2} + e_{2} \\\\ X_{3} &amp;= 0.70F_{1} - 0.39F_{2} + e_{3} \\\\ X_{4} &amp;= 0.87F_{1} - 0.38F_{2} + e_{4} \\\\ X_{5} &amp;= 0.92F_{1} - 0.27F_{2} + e_{5} \\\\ \\end{aligned} \\end{equation} \\] Using ML extraction. The cutoff argument hides loadings under that value for ease of interpretation. Here I am setting that cutoff at 0 so that all loadings are being displayed. ml.extract.norotate &lt;- factanal(stan.dta, factors=2, rotation=&quot;none&quot;) print(ml.extract.norotate, digits=2, cutoff=0) ## ## Call: ## factanal(x = stan.dta, factors = 2, rotation = &quot;none&quot;) ## ## Uniquenesses: ## X1 X2 X3 X4 X5 ## 0.37 0.00 0.63 0.06 0.04 ## ## Loadings: ## Factor1 Factor2 ## X1 -0.06 0.79 ## X2 -0.07 1.00 ## X3 0.58 0.19 ## X4 0.93 0.28 ## X5 0.90 0.39 ## ## Factor1 Factor2 ## SS loadings 2.02 1.88 ## Proportion Var 0.40 0.38 ## Cumulative Var 0.40 0.78 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 0.2 on 1 degree of freedom. ## The p-value is 0.652 The uniqueness’s (\\(u^{2}\\)) for X2, X4, X5 are pretty low. The factor equations now are: \\[ \\begin{equation} \\begin{aligned} X_{1} &amp;= -0.06F_{1} + 0.79F_{2} + e_{1} \\\\ X_{2} &amp;= -0.07F_{1} + 1F_{2} + e_{2} \\\\ X_{3} &amp;= 0.58F_{1} + 0.19F_{2} + e_{3} \\\\ \\vdots \\end{aligned} \\end{equation} \\] load &lt;- ml.extract.norotate$loadings[,1:2] plot(load, type=&quot;n&quot;) # set up the plot but don&#39;t put points down text(load, labels=rownames(load)) # put names instead of points Notice that neither extraction method reproduced our true hypothetical factor model. Rotating the factors will achieve our desired results. "],
["rotating-factors.html", "11.5 Rotating Factors", " 11.5 Rotating Factors Find new factors that are easier to interpret For each \\(X\\), we want some high/large (near 1) loadings and some low/small (near zero) Two common rotation methods 11.5.1 Varimax Rotation Restricts the new axes to be orthogonal to each other. (Factors are independent) Maximizes the sum of the variances of the squared factor loadings within each factor \\(\\sum Var(l_{ij}^{2}|F_{j})\\) Interpretations slightly less clear pc.extract.varimax &lt;- principal(stan.dta, nfactors=2, rotate=&quot;varimax&quot;) print(pc.extract.varimax) ## Principal Components Analysis ## Call: principal(r = stan.dta, nfactors = 2, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## X1 0.07 0.94 0.90 0.104 1.0 ## X2 0.13 0.94 0.89 0.106 1.0 ## X3 0.80 0.02 0.64 0.360 1.0 ## X4 0.94 0.11 0.90 0.099 1.0 ## X5 0.93 0.23 0.91 0.087 1.1 ## ## RC1 RC2 ## SS loadings 2.41 1.83 ## Proportion Var 0.48 0.37 ## Cumulative Var 0.48 0.85 ## Proportion Explained 0.57 0.43 ## Cumulative Proportion 0.57 1.00 ## ## Mean item complexity = 1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.08 ## with the empirical chi square 12.61 with prob &lt; 0.00038 ## ## Fit based upon off diagonal values = 0.97 ml.extract.varimax &lt;- factanal(stan.dta, factors=2, rotation=&quot;varimax&quot;) print(ml.extract.varimax, digits=2, cutoff=.3) ## ## Call: ## factanal(x = stan.dta, factors = 2, rotation = &quot;varimax&quot;) ## ## Uniquenesses: ## X1 X2 X3 X4 X5 ## 0.37 0.00 0.63 0.06 0.04 ## ## Loadings: ## Factor1 Factor2 ## X1 0.79 ## X2 0.99 ## X3 0.61 ## X4 0.97 ## X5 0.96 ## ## Factor1 Factor2 ## SS loadings 2.26 1.64 ## Proportion Var 0.45 0.33 ## Cumulative Var 0.45 0.78 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 0.2 on 1 degree of freedom. ## The p-value is 0.652 11.5.2 Oblique rotation Same idea as varimax, but drop the orthogonality requirement Factors are still correlated Better interpretation Methods: quartimax or quartimin minimizes the number of factors needed to explain each variable direct oblimin standard method, but results in diminished interpretability of factors promax is computationally faster than direct oblimin, so good for very large datasets pc.extract.oblimin &lt;- principal(stan.dta, nfactors=2, rotate=&quot;oblimin&quot;) ml.extract.promax&lt;- factanal(stan.dta, factors=2, rotation=&quot;promax&quot;) par(mfrow=c(2,3)) plot(pc.extract.norotate) plot(pc.extract.varimax) plot(pc.extract.oblimin) load &lt;- ml.extract.norotate$loadings[,1:2] plot(load, type=&quot;n&quot;, main=&quot;ML + norotate&quot;) # set up the plot but don&#39;t put points down text(load, labels=rownames(load)) # put names instead of points load &lt;- ml.extract.varimax$loadings[,1:2] plot(load, type=&quot;n&quot;, main=&quot;ML + Varimax&quot;) text(load, labels=rownames(load)) load &lt;- ml.extract.promax$loadings[,1:2] plot(load, type=&quot;n&quot;, main= &quot;ML + Promax&quot;) text(load, labels=rownames(load)) Varimax vs oblique here doesn’t make much of a difference, and typically this is the case. You almost always use some sort of rotation. Recall, this is a hypothetical example and we set up the variables in a distinct two-factor model. So this example will look nice. "],
["factor-scores.html", "11.6 Factor Scores", " 11.6 Factor Scores Can be used as dependent or independent variables in other analyses Each \\(X\\) is a function of \\(F\\)’s Factor Scores are the reverse: Each \\(F\\) is a function of the \\(X\\)’s Can be generated by adding the scores=&quot;regression&quot; option to factanal(), or scores=TRUE in principal() Each record in the data set with no missing data will have a corresponding factor score. principal() also has a missing argument that if set to TRUE it will impute missing values. fa.ml.varimax &lt;- factanal(stan.dta, factors=2, rotation=&quot;varimax&quot;, scores=&quot;regression&quot;) summary(fa.ml.varimax$scores) ## Factor1 Factor2 ## Min. :-2.32732 Min. :-2.79312 ## 1st Qu.:-0.72413 1st Qu.:-0.54362 ## Median : 0.09196 Median :-0.02376 ## Mean : 0.00000 Mean : 0.00000 ## 3rd Qu.: 0.70603 3rd Qu.: 0.65339 ## Max. : 2.43371 Max. : 2.18992 head(fa.ml.varimax$scores) ## Factor1 Factor2 ## [1,] -1.49174313 0.003612941 ## [2,] -0.26254721 1.090864733 ## [3,] 0.55516045 0.631119886 ## [4,] -1.21028676 -1.281240452 ## [5,] -0.04852211 -1.575691468 ## [6,] -0.53768770 0.661138335 #library(ggforitfy) autoplot(fa.ml.varimax) # see vignette for more info. Link at bottom To merge these scores back onto the original data set providing there is no missing data you can use the bind_cols() function in dplyr. data.withscores &lt;- bind_cols(data, data.frame(fa.ml.varimax$scores)) kable(head(data.withscores)) X1 X2 X3 X4 X5 Factor1 Factor2 -0.8236763 -0.1210726 -0.5970760 -1.4752693 -1.2355056 -1.4917431 0.0036129 1.4013214 1.0733569 0.7681035 -0.0509857 0.0180061 -0.2625472 1.0908647 0.2781468 0.7574632 0.6445954 0.6765583 0.7532815 0.5551604 0.6311199 0.1819544 -1.3228227 -1.0847105 -0.9574722 -1.3719843 -1.2102868 -1.2812405 -1.6147171 -1.4254411 0.3519605 -0.0124497 -0.2523487 -0.0485221 -1.5756915 0.8251470 0.6245702 -1.2923348 -0.6345633 -0.0885945 -0.5376877 0.6611383 "],
["what-to-watch-out-for-2.html", "11.7 What to watch out for", " 11.7 What to watch out for Number of factors should be chosen with care. Check default options. There should be at least two variables with non-zero weights per factor If the factors are to be correlated, try oblique factor analysis Results usually are evaluated by reasonableness to investigator rather than by formal tests Motivate theory, not replace it. Missing data - factors will only be created using available data. "],
["help.html", "11.8 Additional Resources", " 11.8 Additional Resources A gentle non-technical introduction to factor analysis Tutorial by a Psych 253 student at Stanford https://web.stanford.edu/class/psych253/tutorials/FactorAnalysis.html ggfortify vignette for the autoplot() function https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html The FactomineR looks promising, it has some helpful graphics for determining/confirming variable groupings and aiding interpretations. However it looks more like for CFA - when you know the theoretical groupings. FactominR: http://factominer.free.fr/ STHDA tutorial using FactominR http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/116-mfa-multiple-factor-analysis-in-r-essentials/ "],
["introduction-1.html", "Chapter 12 Introduction", " Chapter 12 Introduction When outcome data are expected to be more similar within subgroups than across subgroups the data are typically called Clustered outcomes. There are many ways data can be correlated: Sampling students within schools. (Within school correlation) Repeatedly measuring the same tree in different places. (Within-subject correlation) Repeatedly measuring the same tree over time. (Temporal correlation) Poverty measurements from different, but neighboring, counties. (Spatial correlation) In these cases, the assumption of independence between observations is often violated \\(Cor(\\epsilon_i, \\epsilon_j)\\neq 0, \\forall i\\neq j\\). Analyses should take into account such correlation or else conclusions might not be valid. Correlation is not limited to one level of subgroup; i.e. repeated measurements on students within schools. Models that account for multiple levels of clustering are often called multi-level models. Multi-level modeling can also be referred to as Hierarchical modeling Terminology: Fixed Effects vs Random Effects Fixed Effects: The variable is thought to have it’s own specific effect on the outcome relative to some reference group. The factors are fixed by nature, there are only the levels observed in the data. The 23 schools in the data make up the entire universe of schools, and we are interested in how one particular school fares. Random Effects: The variable was chosen at random, and the levels observed in the data are thought of as representative of all possible levels that could be sampled. We are interested in the distribution of the effects rather than the effect of any one specific level. The 23 schools in the data set is a sample of a larger population of schools and we are interested in the overall distribution of how school can affect the math score, not any one specific school. "],
["example-school-data.html", "12.1 Example School Data", " 12.1 Example School Data The data used in this first example comes from a publicly available data set called the National Education Longitudinal Study of 1988 (yes it’s a bit old data, but sufficient for our purposes here). In this data set math scores are recorded for 519 students from 23 schools. List a few characteristics that you think are associated with math performance, and at what level they are measured. The School23 data set contains the following variables: School (macro) level variables School type class structure school size urbanity geographic region percent minority student-teacher ratio Student (micro) level variables Gender Race Time spent on math homework SES parental education math score Imagine a model of math score based on school type (\\(X_{1}\\) 1 for public, 0 for private) and SES (\\(X_{2}\\)). \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{1i} + \\beta_{2}X_{2i} + \\epsilon_{i}, \\qquad i = 1, \\ldots, n=519 \\] This model does not take into the account the hierarchical nature of the data in that students are nested within schools. School type is a macro level variable, SES is a micro level variable. We could consider adding indicator variables for each of the 23 schools to create a Fixed Effects model, \\[ Y_{i} = \\beta_{0} + \\beta_{1}(SchoolType)_{1i} + \\beta_{2}(SES)_{2i} + \\beta_{3}(School2)_{i} + \\ldots + \\beta_{24}(School23)_{i} + \\epsilon_{i} \\] but we already are well aware of fitting models with that many parameters, and when some school only have a few students in them. So we need a different model. "],
["multi-level-models.html", "12.2 Multi-level models", " 12.2 Multi-level models Here we specify separate models for the the fixed micro level effect of SES and the random macro level school effect. \\[\\begin{equation*} \\begin{aligned} MathScore_{ij} &amp; = \\beta_{0j} + \\beta_{1}SES_{ij} + \\epsilon_{ij}, &amp; \\qquad i = 1, \\ldots, n_{j}\\\\ \\beta_{0j} &amp; = \\gamma_{00} + U_{0j}, &amp; \\qquad j=1, \\ldots, 23 \\end{aligned} \\end{equation*}\\] where \\(\\gamma_{00}\\) is the average intercept across all schools \\(U_{0j}\\) is the difference between the intercept for school \\(j\\) and the average intercept across all schools (random effect of school) If we substitute the intercept \\(\\beta_{0j}\\) in the first model with the information from the second model, \\[ MathScore_{ij} = \\overbrace{\\gamma_{00} + \\beta_{1}SES_{ij}}^{^{\\mbox{fixed part}}} + \\overbrace{U_{0j} + \\epsilon_{ij}}^{\\mbox{random part}} \\] Instead of assuming that there is an overall average of math scores (\\(\\beta_0\\)) from which each school deviates by a fixed amount, the above model assumes that the adjusted average school math scores are normally distributed and are centered at \\(\\gamma_{00}\\). Because we assume that there is a distribution of intercepts, this model is also called a Random intercept model. We’ll look at this type of model more closely in the next section using a different example, and using the concept of pooling information. Random Slopes More complex models are possible. That’s a constant theme in all of statistical modeling. Just remember that the underlying complex effects being modeled must be strong enough that a simpler model that doesn’t account for these effects will be invalid you have to have enough data to estimate any additional parameters you have to be able to explain the model and results to an audience We are not going to cover models that have random slopes, or interactions between fixed and random effects in this notebook. "],
["RI.html", "Chapter 13 Random Intercept Models", " Chapter 13 Random Intercept Models Example Data Radon is a radioactive gas that naturally occurs in soils around the U.S. As radon decays it releases other radioactive elements, which stick to, among other things, dust particles commonly found in homes. The EPA believes radon exposure is one of the leading causes of cancer in the United States. This example uses a dataset named radon from the rstanarm package. The dataset contains \\(N=919\\) observations, each measurement taken within a home that is located within one of the \\(J=85\\) sampled counties in Minnesota. The first six rows of the dataframe show us that the county Aitkin has variable levels of \\(log(radon)\\). Our goal is to build a model to predict \\(log(radon)\\). data(radon, package=&quot;rstanarm&quot;) head(radon) ## floor county log_radon log_uranium ## 1 1 AITKIN 0.83290912 -0.6890476 ## 2 0 AITKIN 0.83290912 -0.6890476 ## 3 0 AITKIN 1.09861229 -0.6890476 ## 4 0 AITKIN 0.09531018 -0.6890476 ## 5 0 ANOKA 1.16315081 -0.8473129 ## 6 0 ANOKA 0.95551145 -0.8473129 "],
["pooling.html", "13.1 Pooling", " 13.1 Pooling To highlight the benefits of random intercepts models we will compare three linear regression models: complete pooling no pooling partial pooling (the random intercept model) Complete Pooling The complete pooling model pools all counties together to give one single estimate of the \\(log(radon)\\) level. No Pooling No pooling refers to the fact that no information is shared among the counties. Each county is independent of the next. Partial Pooling The partial pooling model, partially shares information among the counties. Each county should get a unique intercept such that the collection of county intercepts are randomly sampled from a normal distribution with mean \\(0\\) and variance \\(\\sigma^2_{\\alpha}\\). Because all county intercepts are randomly sampled from the same theoretical population, \\(N(0, \\sigma^2_{\\alpha})\\), information is shared among the counties. This sharing of information is generally referred to as shrinkage, and should be thought of as a means to reduce variation in estimates among the counties. When a county has little information to offer, it’s estimated intercept will be shrunk towards to overall mean of all counties. The plot below displays the overall mean as the complete pooling estimate (solid, horizontal line), the no pooling and partial pooling estimates for 8 randomly selected counties contained in the radon data. The amount of shrinkage from the partial pooling fit is determined by a data dependent compromise between the county level sample size, the variation among the counties, and the variation within the counties. Generally, we can see that counties with smaller sample sizes are shrunk more towards the overall mean, while counties with larger sample sizes are shrunk less. The fitted values corresponding to different observations within each county of the no-pooling model are jittered to help the eye determine approximate sample size within each county. Estimates of variation within each county should not be determined from this arbitrary jittering of points. "],
["mathematical-models.html", "13.2 Mathematical Models", " 13.2 Mathematical Models The three models considered set \\(y_n=log(radon)\\), and \\(x_n\\) records floor (0=basement, 1=first floor) for homes \\(n=1, \\ldots, N\\). 13.2.1 Complete Pooling The complete pooling model pools all counties together to give them one single estimate of the \\(log(radon)\\) level, \\(\\hat{\\alpha}\\). The error term \\(\\epsilon_n\\) may represent variation due to measurement error, within-house variation, and/or within-county variation. Fans of the random intercept model think that \\(\\epsilon_n\\), here, captures too many sources of error into one term, and think that this is a fault of the completely pooled model. \\[\\begin{equation*} \\begin{split} y_n = \\alpha &amp; + \\epsilon_n \\\\ &amp; \\epsilon_n \\sim N(0, \\sigma_{\\epsilon}^{2}) \\end{split} \\end{equation*}\\] 13.2.2 No Pooling The no pooling model gives each county an independent estimate of \\(log(radon\\)), \\(\\hat{\\alpha}_{j[n]}\\). Read the subscript \\(j[n]\\) as home \\(n\\) is nested within county \\(j\\). Hence, all homes in each county get their own independent estimate of \\(log(radon)\\). This is equivalent to the fixed effects model Here again, one might argue that the error term captures too much noise. \\[\\begin{equation*} \\begin{split} y_n = \\alpha_{j[n]} &amp; + \\epsilon_n \\\\ \\epsilon_n &amp; \\sim N(0, \\sigma_{\\epsilon}^{2}) \\end{split} \\end{equation*}\\] 13.2.3 Partial Pooling (RI) The random intercept model, better known as the partial pooling model, gives each county an intercept term \\(\\alpha_j[n]\\) that varies according to its own error term, \\(\\sigma_{\\alpha}^2\\). This error term measures within-county variation Separating measurement error (\\(\\sigma_{\\epsilon}^{2}\\)) from county level error (\\(\\sigma_{\\alpha}^{2}\\)) . This multi-level modeling shares information among the counties to the effect that the estimates \\(\\alpha_{j[n]}\\) are a compromise between the completely pooled and not pooled estimates. When a county has a relatively smaller sample size and/or the variance \\(\\sigma^{2}_{\\epsilon}\\) is larger than the variance \\(\\sigma^2_{\\alpha}\\), estimates are shrunk more from the not pooled estimates towards to completely pooled estimate. \\[\\begin{equation*} \\begin{split} y_n = \\alpha_{j[n]} &amp; + \\epsilon_n \\\\ \\epsilon_n &amp; \\sim N(0, \\sigma_{\\epsilon}^{2}) \\\\ \\alpha_j[n] &amp; \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2) \\end{split} \\end{equation*}\\] "],
["components-of-variance.html", "13.3 Components of Variance", " 13.3 Components of Variance Statistics can be thought of as the study of uncertainty, and variance is a measure of uncertainty (and information). So yet again we see that we’re partitioning the variance. Recall that Measurement error: \\(\\sigma^{2}_{\\epsilon}\\) County level error: \\(\\sigma^{2}_{\\alpha}\\) The intraclass correlation (ICC, \\(\\rho\\)) is interpreted as the proportion of total variance that is explained by the clusters. the expected correlation between two individuals who are drawn from the same cluster. \\[ \\rho = \\frac{\\sigma^{2}_{\\alpha}}{\\sigma^{2}_{\\alpha} + \\sigma^{2}_{\\epsilon}} \\] When \\(\\rho\\) is large, a lot of the variance is at the macro level units within each group are very similar If \\(\\rho\\) is small enough, one may ask if fitting a multi-level model is worth the complexity. no hard and fast rule to say “is it large enough?”, rules of thumb include if it’s under 10% (0.1) then a single level analysis may still be appropriate, if it’s over 10% (0.1) then a multilevel model can be justified. "],
["fitri.html", "13.4 Fitting models in R", " 13.4 Fitting models in R Complete Pooling The complete pooling model is fit with the function lm, and is only modeled by 1 and no covariates. This is the simple mean model, and is equivelant to estimating the mean. fit_completepool &lt;- lm(log_radon ~ 1, data=radon) fit_completepool ## ## Call: ## lm(formula = log_radon ~ 1, data = radon) ## ## Coefficients: ## (Intercept) ## 1.265 mean(radon$log_radon) ## [1] 1.264779 No Pooling The no pooling model is also fit with the function lm, but gives each county a unique intercept in the model. fit_nopool &lt;- lm(log_radon ~ -1 + county, data=radon) fit_nopool.withint &lt;- lm(log_radon ~ county, data=radon) Dependent variable: log_radon (1) (2) Constant 0.715* (0.383) countyAITKIN 0.715* (0.383) countyANOKA 0.891*** (0.106) 0.176 (0.398) countyBECKER 1.090** (0.443) 0.375 (0.585) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 The first model (fit_nopool) is coded as lm(log_radon ~ -1 + county, data=radon), and so does not have the global intercept (that’s what the -1 does). Each \\(\\beta\\) coefficient is the estimate of the mean log_radon for that county. The second model (fit_nopool.withint) is coded as lm(log_radon ~ county, data=radon) and is what we are typically used to fitting. Each estimate is the difference in log(radon) for that county compared to a reference county. Because county is alphabetical, the reference group is AITKIN. Aitkin’s mean level of log(radon) shows up as the intercept or Constant term. For display purposes only, only the first 3 county estimates are being shown. Partial Pooling The partial pooling model is fit with the function lmer(), which is part of the lme4 package. The extra notation around the input variable (1|county) dictates that each county should get its own unique intercept \\(\\alpha_{j[n]}\\). library(lme4) fit_partpool &lt;- lmer(log_radon ~ (1 |county), data=radon) The fixed effects portion of the model output of lmer is similar to output from lm, except no p-values are displayed. The fact that no p-values are displayed is a much discussed topic. The author of the library lme4, Douglas Bates, believes that there is no “obviously correct” solution to calculating p-values for models with randomly varying intercepts (or slopes); see here for a general discussion. summary(fit_partpool) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: log_radon ~ (1 | county) ## Data: radon ## ## REML criterion at convergence: 2184.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.6880 -0.5884 0.0323 0.6444 3.4186 ## ## Random effects: ## Groups Name Variance Std.Dev. ## county (Intercept) 0.08861 0.2977 ## Residual 0.58686 0.7661 ## Number of obs: 919, groups: county, 85 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.350 0.047 28.72 The random effects portion of the lmer output provides a point estimate of the variance of component \\(\\sigma^2_{\\alpha} = 0.09\\) and the model’s residual variance, \\(\\sigma_\\epsilon = 0.57\\). The fixed effect here is interpreted in the same way that we would in a normal fixed effects mean model, as the global predicted value of the outcome of log_radon. The random intercepts aren’t automatically shown in this output. We can visualize these using what some call a forest plot. A very easy way to accomplish this is to use the sjPlot package. We use the plot_model() function, on the fit_partpool model, we want to see the random effects (type=&quot;re&quot;), and we want to sort on the name of the random variable, here it’s &quot;(Intercept)&quot;. library(sjPlot) plot_model(fit_partpool, type=&quot;re&quot;, sort.est = &quot;(Intercept)&quot;, y.offset = .4) Notice that these effects are centered around 0. Refering back to Section 10.2 in this notebook, the intercept \\(\\beta_{0j}\\) was modeled equal to some average intercept across all groups \\(\\gamma_{00}\\), plus some difference. What is plotted above is listed in a table below, showing that if you add that random effect to the fixed effect of the intercept, you get the value of the random intercept for each county. showri &lt;- data.frame(Random_Effect = unlist(ranef(fit_partpool)), Fixed_Intercept = fixef(fit_partpool), RandomIntercept = unlist(ranef(fit_partpool))+fixef(fit_partpool)) rownames(showri) &lt;- rownames(coef(fit_partpool)$county) kable(head(showri)) Random_Effect Fixed_Intercept RandomIntercept AITKIN -0.2390574 1.34983 1.1107728 ANOKA -0.4071256 1.34983 0.9427047 BECKER -0.0809977 1.34983 1.2688325 BELTRAMI -0.0804277 1.34983 1.2694025 BENTON -0.0254506 1.34983 1.3243796 BIGSTONE 0.0582831 1.34983 1.4081133 13.4.1 Comparison of estimates By allowing individuals within counties to be correlated, and at the same time let counties be correlated, we allow for some information to be shared across counties. Thus we come back to that idea of shrinkage. Below is a numeric table version of the plot in Section 1.11. cmpr.est &lt;- data.frame(Mean_Model = coef(fit_completepool), Random_Intercept = unlist(ranef(fit_partpool))+fixef(fit_partpool), Fixed_Effects = coef(fit_nopool)) rownames(cmpr.est) &lt;- rownames(coef(fit_partpool)$county) kable(head(cmpr.est)) Mean_Model Random_Intercept Fixed_Effects AITKIN 1.264779 1.1107728 0.7149352 ANOKA 1.264779 0.9427047 0.8908486 BECKER 1.264779 1.2688325 1.0900084 BELTRAMI 1.264779 1.2694025 1.1933029 BENTON 1.264779 1.3243796 1.2822379 BIGSTONE 1.264779 1.4081133 1.5367889 "],
["estimation-methods.html", "13.5 Estimation Methods", " 13.5 Estimation Methods Similar to logistic regression, estimates from multi-level models typically aren’t estimated directly using maximum likelihood (ML) methods. Iterative methods like Restricted (residual) Maximum Likelihood (REML) are used to get approximations. REML is typically the default estimation method for most packages. Details of REML are beyond the scope of this class, but knowing the estimation method is important for two reasons Some type of testing procedures that use the likelihood ratio may not be valid. Comparing models with different fixed effects using a likelihood ratio test is not valid. (Must use Wald Test instead) Can still use AIC/BIC as guidance (not as formal tests) Iterative procedures are procedures that perform estimation steps over and over until the change in estimates from one step to the next is smaller than some tolerance. Sometimes this convergence to an answer never happens. You will get some error message about the algorithm not converging. The more complex the model, the higher chance this can happen scaling, centering, and avoiding collinearity can alleviate these problems with convergence. You can change the fitting algorithm to use the Log Likelihood anyhow, it may be slightly slower but for simple models the estimates are going to be very close to the REML estimate. Below is a table showing the estimates for the random intercepts, REML MLE AITKIN 1.1107728 1.1143654 ANOKA 0.9427047 0.9438526 BECKER 1.2688325 1.2700351 BELTRAMI 1.2694025 1.2702493 BENTON 1.3243796 1.3245917 BIGSTONE 1.4081133 1.4068866 and the same estimates for the variance terms. VarCorr(fit_partpool) ## Groups Name Std.Dev. ## county (Intercept) 0.29767 ## Residual 0.76607 VarCorr(fit_partpool_MLE) ## Groups Name Std.Dev. ## county (Intercept) 0.29390 ## Residual 0.76607 So does it matter? Yes and no. In general you want to fit the models using REML, but if you really want to use a Likelihood Ratio test to compare models then you need to fit the models using ML. "],
["including-covariates.html", "13.6 Including Covariates", " 13.6 Including Covariates A similar sort of shrinkage effect is seen with covariates included in the model. Consider the covariate \\(floor\\), which takes on the value \\(1\\) when the radon measurement was read within the first floor of the house and \\(0\\) when the measurement was taken in the basement. In this case, county means are shrunk towards the mean of the response, \\(log(radon)\\), within each level of the covariate. Covariates are fit using standard + notation outside the random effects specification, i.e. (1|county). ri.with.x &lt;- lmer(log_radon ~ floor + (1 |county), data=radon) sjt.lmer(ri.with.x, show.r2=FALSE) log_radon B CI p Fixed Parts (Intercept) 1.49 1.40 – 1.59 &lt;.001 floor (first floor) -0.66 -0.80 – -0.53 &lt;.001 Random Parts σ2 0.527 τ00, county 0.099 Ncounty 85 ICCcounty 0.159 Observations 919 Note that in this table format, \\(\\tau_{00} = \\sigma^{2}_{\\alpha}\\) and \\(\\sigma^{2} = \\sigma^{2}_{\\epsilon}\\). The estimated random effects can also be easily visualized using functions from the sjPlot package. plot_model(ri.with.x, type=&quot;re&quot;, sort.est = &quot;(Intercept)&quot;, y.offset = .4) Function enhancements – ( Display the fixed effects by changing type=&quot;est&quot;. Plot the slope of the fixed effect for each level of the random effect sjp.lmer(ri.with.x, type=&quot;ri.slope&quot;) – this is being depreciated in the future but works for now. Eventually I’ll figure out how to get this plot out of plot_model(). "],
["centering-terms.html", "13.7 Centering terms", " 13.7 Centering terms Sometimes it might be better to measure the effect of a specific level relative to the average within cluster, rather than overall average. The “frog pond” effect A student with an average IQ may be more confident and excel in a group of students with less than average IQ But they may be discouraged and not perform to their potential in a group of students with higher than average IQ. If the effect of a specific level of a factor is dependent on where the level is in reference to other cluster members, more so than where the level is in reference to all other participants, the model should be adjusted for as follows: Instead of using the actual value in the regression model you would… calculate the cluster specific average calculate the difference between individual and specific cluster average both cluster average (macro) and difference (micro) are included in the model. 13.7.1 A generic dplyr approach to centering. group.means &lt;- data %&gt;% group_by(cluster) %&gt;% summarise(c.ave=mean(variable)) newdata &lt;- data %&gt;% left_join(group.means) %&gt;% mutate(diff = variable - c.ave) Create a new data set that I call group.means that takes the original data set and then (%&gt;%)… groups it by the clustering variable so that all subsequent actions are done on each group makes a new variable that I call c.ave that is the average of the variable of interest I then take the original data set, and then merge onto data, this group.means data set that only contains the clustering variable, and the cluster average variable c.ave. I also toss in a mutate to create a new variable that is the difference between the variable of interest and the group averages. and assign all of this to a newdata set "],
["specifying-correlation-structures.html", "13.8 Specifying Correlation Structures", " 13.8 Specifying Correlation Structures Independence: In standard linear models, the assumption on the residuals \\(\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})\\) means that The variance of each observation is \\(\\sigma_{\\epsilon}^{2}\\) The covariance between two different observations \\(0\\) Consider \\(n=4\\) observations, \\(y_{1}, \\ldots , y_{4}\\). Visually the covariance matrix between these four observations would look like this: \\[ \\begin{array}{c|cccc} &amp; y_{1} &amp; y_{2} &amp; y_{3} &amp; y_{4}\\\\ \\hline y_{1} &amp; \\sigma_{\\epsilon}^{2} &amp; 0 &amp; 0 &amp; 0\\\\ y_{2} &amp; 0 &amp; \\sigma_{\\epsilon}^{2} &amp; 0 &amp; 0\\\\ y_{3} &amp; 0 &amp; 0 &amp; \\sigma_{\\epsilon}^{2} &amp; 0\\\\ y_{4} &amp; 0&amp; 0 &amp; 0 &amp; \\sigma_{\\epsilon}^{2} \\end{array} \\] We can also write the covariance matrix as \\(\\sigma_{\\epsilon}^{2}\\) times the correlation matrix. \\[ \\begin{bmatrix} \\sigma_{\\epsilon}^{2} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\sigma_{\\epsilon}^{2} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\sigma_{\\epsilon}^{2} &amp; 0\\\\ 0&amp; 0 &amp; 0 &amp; \\sigma_{\\epsilon}^{2} \\end{bmatrix} = \\sigma_{\\epsilon}^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ &amp; 1 &amp; 0 &amp; 0 \\\\ &amp; &amp; 1 &amp; 0 \\\\ &amp; &amp; &amp; 1 \\end{bmatrix} \\] Compound Symmetry or Exchangeable: The simplest covariance structure that includes correlated errors is compound symmetry (CS). Here we see correlated errors between individuals, and note that these correlations are presumed to be the same for each pair of responses, namely \\(\\rho\\). \\[ \\sigma_{\\epsilon}^{2} \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ &amp; 1 &amp; \\rho &amp; \\rho \\\\ &amp; &amp; 1 &amp; \\rho \\\\ &amp; &amp; &amp; 1 \\end{bmatrix} \\] Autoregressive: Imagine that \\(y_{1}, \\ldots , y_{4}\\) were 4 different time points on the same person. The autoregressive (Lag 1) structure considers correlations to be highest for time adjacent times, and a systematically decreasing correlation with increasing distance between time points. This structure is only applicable for evenly spaced time intervals for the repeated measure. \\[ \\sigma_{\\epsilon}^{2} \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho^{2} &amp; \\rho^{3} \\\\ &amp; 1 &amp; \\rho &amp; \\rho^{2} \\\\ &amp; &amp; 1 &amp; \\rho \\\\ &amp; &amp; &amp; 1 \\end{bmatrix} \\] Unstructured: The Unstructured covariance structure (UN) is the most complex because it is estimating unique correlations for each pair of observations. It is not uncommon to find out that you are not able to use this structure simply because there are too many parameters to estimate. \\[ \\begin{bmatrix} \\sigma_{1}^{2} &amp; \\rho_{12} &amp; \\rho_{13} &amp; \\rho_{14} \\\\ &amp; \\sigma_{2}^{2} &amp; \\rho_{23} &amp; \\rho_{24} \\\\ &amp; &amp; \\sigma_{3}^{2} &amp; \\rho_{34} \\\\ &amp; &amp; &amp; \\sigma_{4}^{2} \\end{bmatrix} \\] Random Intercept Model Let \\(y_{1}\\) and \\(y_{2}\\) be from group 1, and \\(y_{3}\\) and \\(y_{4}\\) be from group 2. error terms between groups are uncorrelated (groups are independent) two different observations from the same group have covariance \\(\\sigma_{\\alpha}^{2}\\) individuals now have the error associated with their own observation but also due to the group \\(\\sigma_{\\epsilon}^{2} + \\sigma_{\\alpha}^{2}\\) \\[ \\left[ \\begin{array}{cc|cc} \\sigma_{\\epsilon}^{2} + \\sigma_{\\alpha}^{2} &amp; \\sigma_{\\alpha}^{2} &amp; 0 &amp; 0\\\\ \\sigma_{\\alpha}^{2} &amp; \\sigma_{\\epsilon}^{2} + \\sigma_{\\alpha}^{2} &amp; 0 &amp; 0\\\\ \\hline 0 &amp; 0 &amp; \\sigma_{\\epsilon}^{2} + \\sigma_{\\alpha}^{2} &amp; \\sigma_{\\alpha}^{2}\\\\ 0 &amp; 0 &amp; \\sigma_{\\alpha}^{2} &amp; \\sigma_{\\epsilon}^{2} + \\sigma_{\\alpha}^{2} \\end{array} \\right] \\] 13.8.1 Specifying different covariance structures in R Not easily able to do this using lmer() from package lme4 Can do this using lme() from package nlme. Syntax is similar. The standard classes of correlation structures available in the nlme package can be found in [this help file] library(nlme) model_lme_cs&lt;-lme(log_radon ~ floor, random = ~ 1 | county, cor=corCompSymm(value=0.159,form=~1|county),data = radon) Using a different covariance structure can have a large effect on the results. lmer using Identity: \\(\\sigma^{2}_{\\alpha} = 0.10, \\sigma^{2}_{\\epsilon} = 0.53\\) nlme using Identity: \\(\\sigma^{2}_{\\alpha} = 0.32^2 = 0.10, \\sigma^{2}_{\\epsilon} = 0.73^2 = 0.53\\) nlme using CS: \\(\\sigma^{2}_{\\alpha} = 0.14^2 = 0.02, \\sigma^{2}_{\\epsilon} = 0.78^2 = 0.61\\) Mis-specifying the covariance structure can also have a large effect on the results. "],
["additional-references-1.html", "13.9 Additional References", " 13.9 Additional References Random effects ANOVA in SAS and R http://stla.github.io/stlapblog/posts/AV1R_SASandR.html ICCs in mixed models https://www.theanalysisfactor.com/the-intraclass-correlation-coefficient-in-mixed-models/ Very nice introduction to mixed models in R https://m-clark.github.io/mixed-models-with-R/introduction.html Interesting blog by Tristan Mahr about pooling and shrinkage. Derivation of the covariance structures http://www.bristol.ac.uk/cmm/learning/videos/correlation.html#matrix2 13.9.1 Package Vignettes lme4 Functions to fit HLMs sjPlot Really nice way of printing output as tables (and plots). Changing covariance structures in lme4qtl: [paper] [github] "],
["longitudinal.html", "Chapter 14 Longitudinal Data", " Chapter 14 Longitudinal Data What is Longitudinal Data? Repeated measures over time on subjects Different than other ways of repeated measures on clustered data (like family members, or kids in a classroom) Other similar data structures include time series data. Typically only have one subject (one stock market, sales from one product), and usually measured for a longer length of time. Key feature is that longitudinal data is ordered over time, linear distance, or some other “meta-meter” such as trial number, dose or length. What can we learn from longitudinal data? Population (average) response over time. Population average response over time between different groups. How individual responses behave over time. What is the estimate for the next observation over time? How do covariates affect the population mean time trend and individual variation? The rest of this chapter is organized as follows: Introduce the Pediatric Pain experiment and explore the resulting data. See how we can visualize longitudinal data Explore some analyses methods that are often conducted on longitudinal data. There are many ways to parameterize the data and to make comparisons. Fitting a random intercept model on this data. Compare estimates between and across groups by writing contrast statements. Data and examples in this section come from Modeling Longitudinal Data, Robert E. Weiss (2005) https://www.springer.com/us/book/9780387402710 "],
["pediatric-pain-data.html", "14.1 Pediatric pain data", " 14.1 Pediatric pain data The Pediatric Pain data set used in this chapter is a result of a designed experiment. Most studies on human pain are observational. The data consist of up to four observations on 64 children aged 8-10. The response is the length of time in seconds that the child can tolerate keeping his or her arm in very cold water, a proxy measure of pain tolerance. After the cold becomes intolerable, the child removes his or her arm. The arm is toweled off, no harm is done. Two measurements were done on the first visit, then another 2 measurements were taken two weeks later during a second visit. After the arm was dried off study staff asked the child what they were thinking about during that trial. Children were classified into two groups depending on the child’s coping style during the trial. Attenders thought about the experiment, objects related to the experiment or their arm. Distracters distracted themselves during the test by thinking about something unrelated to the experiment: the wall, homework, their pet, etc. A treatment was administered before the fourth trial. This treatment was a 10 minute conversation where the child was either counseled to attend to the arm, distract themselves from the arm, or no advice was given. The study authors were interested in the main effects of treatment, coping style, and the interaction between the two. 14.1.1 Data sample kable(pain[1:10,]) id ses cs treatment sex age trial paintol l2paintol 1 73.5 attender attend female 10.083333 1 20.55 4.361066 1 73.5 attender attend female 10.083333 2 35.31 5.142005 1 73.5 attender attend female 10.083333 3 14.29 3.836934 1 73.5 attender attend female 10.083333 4 11.71 3.549669 2 74.7 distracter distract female 9.250000 1 28.13 4.814038 2 74.7 distracter distract female 9.250000 2 24.22 4.598127 2 74.7 distracter distract female 9.250000 3 15.86 3.987321 2 74.7 distracter distract female 9.250000 4 20.30 4.343408 3 81.5 attender no directions female 8.666667 1 12.00 3.584963 3 81.5 attender no directions female 8.666667 2 10.00 3.321928 id: participant ID ses: child’s socio-economic status cs: coping style. Assessed and assigned at the first visit. Attender : thought about the experiment, objects related to their their arm, Distracter : distracted themselves during the test treatment: Treatment group. Attend advised to pay attention to the arm Distract advised to pay attention to something other than the arm no direction no advice was given. trial: 1, 2, 3, 4 (time) paintol = The length of time in seconds arm was underwater. Proxy measure of pain tolerance. l2paintol = \\(\\log_{2}(paintol)\\) - Normality transformation. 14.1.2 Univariate Visualizations Pain tolerance The raw score for paintol is very skewed right. A \\(log_{2}\\) transformation was applied and found sufficient to achieve approximate normality. pain.dist &lt;- ggplot(pain, aes(x=paintol)) + geom_density() l2pain.dist &lt;- ggplot(pain, aes(l2paintol)) + geom_density() grid.arrange(pain.dist, l2pain.dist) Coping style and treatment There are nearly equal proportions of coping styles and treatment types. kable(table(pain$cs, pain$treatment)) attend distract no directions attender 40 48 40 distracter 44 44 40 "],
["visualizing-longitudinal-data.html", "14.2 Visualizing longitudinal data", " 14.2 Visualizing longitudinal data 14.2.1 Profile or spaghetti plots How does individual’s (log 2) pain tolerance change over time/trials? What does the average trend look like? all &lt;- ggplot(pain, aes(trial, l2paintol)) + geom_point(alpha=.5) + geom_path(aes(group=id), alpha=.5) + geom_smooth(col=&quot;blue&quot;, lwd=2) few &lt;- ggplot(pain[1:16,], aes(trial, l2paintol, group=id)) + geom_line() + geom_point() grid.arrange(all, few, ncol=2) What’s the difference in how the lines are connected using geom_path() vs geom_line() between the two plots? The average is around 5, with a slight decrease at time 2 and then relatively constant. The shading around the trendline shows that the variance across time points is pretty constant. The right hand plot shows the profile plot for the first 4 kids in the data set, notice how one path stops after trial 2? This tells us there is missing data in this data set that we will have to look out for. Quite a bit of variation across individuals, but what about the variation within person? I.e. how does the individual pain tolerance at each trial differ from the individual’s average pain tolerance. We’ll center the data around each person’s own average. With longitudinal data - the person is the cluster. person.ave &lt;- pain %&gt;% group_by(id) %&gt;% summarise(l2.ave = mean(l2paintol, na.rm=TRUE)) pain &lt;- pain %&gt;% left_join(person.ave) %&gt;% mutate(diff = l2.ave-l2paintol) # Always be very cautious when overwriting your data object! # Novice users recommended to create a new data set with a different name. ggplot(pain, aes(trial, diff)) + geom_path(aes(group=id), alpha=.5) + geom_hline(yintercept=0, col=&quot;blue&quot;, lwd=2) Few individuals have large variations in their pain tolerance across time, and it appears as if those that had higher than average at trial 2, were lower than average at trial 3, and vice versa. 14.2.2 Correlation of pain tolerance within person across time Correlations are calculated between variables. To calculate the correlation of l2paintol between trials, we need to reshape the data to wide format. In wide data, the measure of pain tolerance for each trial is it’s own variable (column). # First select only the variable we want to transform wide pain.2.wide &lt;- pain %&gt;% select(id, trial, l2paintol) # Use the reshape() function to transform - this comes with baseR pw &lt;- reshape(pain.2.wide, idvar=&quot;id&quot;, timevar=&quot;trial&quot;, direction=&quot;wide&quot;) # then calculate and visualize the correlation of pain tolerance across time points. # use=pairwise complete observations has to be used here because there is missing values corrplot::corrplot.mixed(cor(pw[,-1], use = &#39;pairwise.complete.obs&#39;)) There is a fairly high correlation of pain tolerance between trials. "],
["analyze-pain-tolerance-across-coping-style.html", "14.3 Analyze pain tolerance across coping style", " 14.3 Analyze pain tolerance across coping style Visualize the distribution of pain tolerance across the different coping styles. The first plots show the distribution of the raw number of seconds the arms are held under ice water, and the log2 transformation. For the rest of this analysis we’ll be using this log2 transformation. pt.dist &lt;- ggplot(pain, aes(paintol, fill=cs)) + geom_density(alpha=.5) l2pt.dist &lt;- ggplot(pain, aes(l2paintol, fill=cs)) + geom_density(alpha=.5) grid.arrange(pt.dist, l2pt.dist, ncol=2) There appears to be a slight difference, but is it significantly different? What about the treatment? How does it appear to affect the pain tolerance of the coping styles? ggplot(pain, aes(l2paintol, x=cs, fill=treatment)) + geom_boxplot(width=1, alpha=.5) + geom_violin(alpha=.3) + stat_summary(fun.y=mean, colour=&quot;black&quot;, geom=&quot;point&quot;, shape=18, size=4, position=position_dodge(width=.9)) Attenders seem to lower pain tolerance, and lower variation in that pain tolerance. The exception is attenders who were under the “no directions” treatment group. Distracters who were advised to distract themselves were able to keep their arm under water for much longer than the other groups. We’ll look at many different ways to approach this analysis. 14.3.1 Two Sample T-test Compare pain tolerance across coping styles at each trial. Individuals only have one measurement per trial, and individuals are independent of each other, so the data within trials satisfy the independence assumption. ggplot(pain, aes(l2paintol, fill=cs)) + geom_density(alpha=.5) + facet_wrap(~trial) There is a lot of overlap. Which differences are significant? We have no preconceived notion whether or not we can assume equal variances across groups, so we’ll run t-tests under both conditions. # Make an empty data frame to hold the p-values from the t-tests below. ttst &lt;- data.frame(eq.var=rep(NA,4), uneq.var=rep(NA,4), row.names=paste0(&quot;Trial&quot;, 1:4)) kable(ttst) # what does this look like empty? eq.var uneq.var Trial1 Trial2 Trial3 Trial4 # loop over 1 to 4 trials, conduct a t.test, extract the p-value and store it into this empty data frame for(t in 1:4){ ttst$eq.var[t] &lt;- t.test(l2paintol~cs, data=subset(pain, trial==t), var.equal=TRUE)$p.value ttst$uneq.var[t] &lt;- t.test(l2paintol~cs, data=subset(pain, trial==t), var.equal=FALSE)$p.value } kable(ttst, type=&#39;html&#39;, digits=4, caption=&quot;T-test p-values for a difference between averge pain tolerance across coping style groups&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;center&quot;) # now it&#39;s full Table 14.1: T-test p-values for a difference between averge pain tolerance across coping style groups eq.var uneq.var Trial1 0.0524 0.0535 Trial2 0.0308 0.0305 Trial3 0.1041 0.1049 Trial4 0.1121 0.1104 Conclusion: regardless if we assume equal or unequal variances, there is a difference in the log2 pain tolerance between attending groups during trial 1 and 2 only, not during 3 or 4. 14.3.2 Averaged over the observed baseline values. Perhaps the difference we’re seeing is due to a difference at baseline between the two groups? Lets compute the baseline average over the first three responses and see how that differs between coping styles. For this analyses the data needs to be in wide format, so we can calculate the row-wise average across times 1-3. We did this earlier using reshape(), but that example only turned the l2paintol variable wide. Here we need to keep the coping style (cs) in addition to the time. wide.pain.cs &lt;- pain %&gt;% select(id, trial, l2paintol, cs) pwc &lt;- reshape(wide.pain.cs, idvar=&quot;id&quot;, timevar=&quot;trial&quot;, direction=&quot;wide&quot;) kable(head(pwc)) id l2paintol.1 cs.1 l2paintol.2 cs.2 l2paintol.3 cs.3 l2paintol.4 cs.4 1 1 4.361066 attender 5.142005 attender 3.836934 attender 3.549669 attender 5 2 4.814038 distracter 4.598127 distracter 3.987321 distracter 4.343408 distracter 9 3 3.584963 attender 3.321928 attender 3.614710 attender 3.047887 attender 13 4 3.956986 distracter 4.503349 distracter distracter distracter 17 5 4.638653 attender 4.549053 attender 4.961160 attender 4.971773 attender 21 6 6.773073 distracter 6.424754 distracter 6.099716 distracter 5.914086 distracter Each variable that we kept now has 4 entries, one for each time point. Some variables such as cs are time fixed, meaning they don’t change over time; cs.1 = cs.2 = cs.3 = cs=4. We can drop the other copies of this variable. Other variables like paintol are time varying, they change within person across time. pwc &lt;- pwc %&gt;% select(-cs.2, -cs.3, -cs.4) %&gt;% rowwise() %&gt;% mutate(ave.baseline.tol = mean(c(l2paintol.1, l2paintol.2, l2paintol.3), na.rm=TRUE)) kable(head(pwc)) id l2paintol.1 cs.1 l2paintol.2 l2paintol.3 l2paintol.4 ave.baseline.tol 1 4.361066 attender 5.142005 3.836934 3.549669 4.446669 2 4.814038 distracter 4.598127 3.987321 4.343408 4.466495 3 3.584963 attender 3.321928 3.614710 3.047887 3.507200 4 3.956986 distracter 4.503349 4.230167 5 4.638653 attender 4.549053 4.961160 4.971773 4.716289 6 6.773073 distracter 6.424754 6.099716 5.914086 6.432514 The boxplot below has the individual points plotted on top of the boxplots, this allows us to see that the high end outlying point on the boxplot is due to two individuals (one in each coping style group). Distractors seem to have a higher average average baseline tolerance compared to attenders. ggplot(pwc, aes(y=ave.baseline.tol, x=cs.1, fill=cs.1)) + geom_boxplot() + geom_jitter(width=.1, col=&quot;purple&quot;) + stat_summary(fun.y=mean, colour=&quot;blue&quot;, geom=&quot;point&quot;, shape=18, size=4) Is this difference significant? pander(t.test(ave.baseline.tol ~ cs.1, data=pwc)) Welch Two Sample t-test: ave.baseline.tol by cs.1 (continued below) Test statistic df P value Alternative hypothesis -1.899 59.84 0.06236 two.sided mean in group attender mean in group distracter 4.54 5.004 Children who distracted themselves from the experiment could keep their arm in the ice water for an average of 5.0 seconds. This was slightly longer than children who attended to the experiment who averaged 4.5 seconds (p=.06). How does this result compare to the previous results? Was this expected? 14.3.3 Missing Data We noticed earlier that there were some missing values in the data. Turns out that no one dropped out for reasons related to the experiment (i.e. they were uncomfortable), but there were some absences from school that day and a few broken arms. So far we’ve been ignoring that there is missing data. The table below shows that there are 6 individuals who are missing a score at time 3, but only 3 at time 4. This means that the time-stratified t-tests conducted earlier are on different sets of data. kable(table(is.na(pain$l2paintol), pain$trial)) 1 2 3 4 FALSE 63 63 58 61 TRUE 1 1 6 3 Is it the same individuals who are missing data? First let’s identify the rows with any missing values and save that as a vector of row numbers into miss.idx. miss.idx &lt;- which(is.na(pain$l2paintol)) miss.idx ## [1] 15 16 71 79 131 132 166 167 168 173 175 Then let’s see the id’s that correspond to those rows, and save those id numbers in a vector. ids.with.missing &lt;- pain$id[miss.idx] ids.with.missing ## [1] 4 4 18 20 33 33 42 42 42 44 44 There are 6 unique individuals who are missing measurements for pain tolerance. What does their data look like? Half male, half distracters, 3 in the no directions treatment group, 2 in the distraction treatment group, and 1 in the attend group. Understanding the difference in characteristics between those with missing data and those without is important - and a topic we’ll come back to in a later chapter. kable(pain[pain$id %in% ids.with.missing,c(&#39;id&#39;, &#39;cs&#39;, &#39;treatment&#39;, &#39;sex&#39;, &#39;trial&#39;, &#39;paintol&#39; )], type=&#39;html&#39;, row.names=FALSE) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;center&quot;) id cs treatment sex trial paintol 4 distracter attend male 1 15.53 4 distracter attend male 2 22.68 4 distracter attend male 3 4 distracter attend male 4 18 attender no directions male 1 22.48 18 attender no directions male 2 20.03 18 attender no directions male 3 18 attender no directions male 4 26.82 20 distracter no directions male 1 30.57 20 distracter no directions male 2 20.29 20 distracter no directions male 3 20 distracter no directions male 4 11.89 33 attender distract female 1 35.88 33 attender distract female 2 53.04 33 attender distract female 3 33 attender distract female 4 42 attender distract female 1 52.58 42 attender distract female 2 42 attender distract female 3 42 attender distract female 4 44 distracter no directions female 1 44 distracter no directions female 2 16.75 44 distracter no directions female 3 44 distracter no directions female 4 14.66 What if we just drop all records for all children with missing values at any time point. How does the results here compare to the previous test? The syntax a %in% b results in a vector of TRUE and FALSE for every entry in a, whether or not that value is found in b. So (pwc$id %in% ids.with.missing) is a list of TRUE/FALSE for every pwc$id, where TRUE is if that id was found in the vector ids.with.missing. Then we negate that boolean/logic vector by using !, (turn TRUE to FALSE and FALSE to TRUE) - so now TRUE means that record belongs to an id with no missing data. Those are the ones we want to keep. pwc.nomiss &lt;- pwc[!(pwc$id %in% ids.with.missing),] pander(t.test(ave.baseline.tol ~ cs.1, data=pwc.nomiss)) Welch Two Sample t-test: ave.baseline.tol by cs.1 (continued below) Test statistic df P value Alternative hypothesis -2.314 53.53 0.02452 * two.sided mean in group attender mean in group distracter 4.473 5.076 Dropping all data on children with any missing visit data changed our conclusion! Of the students providing data at all time points, distractors can keep their arm under ice water significantly longer on average compared to attenders. From here on all analyses will be on the 58 students with data on all four time points. "],
["paired-t-tests.html", "14.4 Paired t-tests", " 14.4 Paired t-tests So far we’ve assumed that pain tolerance within person is independent across time points. Let’s modify the analysis now to use a paired t-test, where we model the difference between time points within individuals and see if the average difference is not zero. But there are 4 time points, so how many comparisons/tests will this create? trial4 - trial3 trial4 - trial2 trial4 - trial1 trial4 - average baseline For display and comparison purposes, I’ll build another empty vector to store the p-value for the test into. Since there was no difference between the tests assuming equal variance and those without that assumption, I will go with the default here of false. p&lt;-rep(NA, 4) p[1] &lt;- t.test(pwc.nomiss$l2paintol.4, pwc.nomiss$l2paintol.3, paired=TRUE)$p.value p[2] &lt;- t.test(pwc.nomiss$l2paintol.4, pwc.nomiss$l2paintol.2, paired=TRUE)$p.value p[3] &lt;- t.test(pwc.nomiss$l2paintol.4, pwc.nomiss$l2paintol.1, paired=TRUE)$p.value p[4] &lt;- t.test(pwc.nomiss$l2paintol.4, pwc.nomiss$ave.baseline.tol, paired=TRUE)$p.value names(p) &lt;- c(&quot;4-3&quot;, &quot;4-2&quot;, &quot;4-1&quot;, &quot;4-ave&quot;) kable(t(p), type=&#39;html&#39;, digits=3) 4-3 4-2 4-1 4-ave 0.773 0.587 0.371 0.954 Using this approach, we see there is no significant difference between any pair of time points when combining the data across all coping style and treatment groups. What is the average trajectory of pain tolerance within coping styles? Notice here that i’m taking the data in long format and then using dplyr to calculate the average l2paintol by time and coping style. mean.plot &lt;- pain %&gt;% filter(!(id %in% ids.with.missing)) %&gt;% #take id&#39;s that are NOT in those identified as having missing data group_by(trial, cs) %&gt;% summarise(apt = mean(l2paintol)) ggplot(mean.plot, aes(x=trial, y=apt, col=cs)) + geom_path(data=mean.plot, aes(group=cs), lwd=2)+ ylab(&quot;log2 pain tolerance&quot;) + #geom_line(data=pain, aes(x=trial, y=l2paintol, group=id), alpha=.3) + geom_jitter(data=pain, aes(x=trial, y=l2paintol), alpha=.5, width=.1) Red lines are attenders, and for the most part their paths are lower than distractors in blue. The dark thick lines are the average for each coping style. Did the log2 pain tolerance change at the same rate between time 2 and 4 for both attenders and distractors? In essence, we are testing a difference of differences. One way to accomplish this without explicitly putting trial in the model is to calculate the trial4-trial2 difference on each person, then use that as the outcome. pwc.nomiss$diff24 &lt;- pwc.nomiss$l2paintol.4 - pwc.nomiss$l2paintol.2 t.test(diff24 ~ cs.1, data=pwc.nomiss) ## ## Welch Two Sample t-test ## ## data: diff24 by cs.1 ## t = 0.53337, df = 46.824, p-value = 0.5963 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3700586 0.6370431 ## sample estimates: ## mean in group attender mean in group distracter ## 0.134765185 0.001272957 The average change in log2paintol between time 4 and time 2 for distractors is 0.001, and 0.135 for attenders. These changes are not significantly different from each other (p=0.59). 14.4.1 But what about treatment groups? Let’s look at the full interaction model by plotting the average log2 pain tolerance for each of the 6 coping style * treatment combinations. full.mean.plot &lt;- pain %&gt;% filter(!(id %in% ids.with.missing)) %&gt;% #take id&#39;s that are NOT in those identified as having missing data group_by(trial, cs, treatment) %&gt;% summarise(apt = mean(l2paintol)) %&gt;% ungroup() %&gt;% mutate(cop = factor(cs, labels=c(&quot;A&quot;, &quot;D&quot;)), tx = factor(treatment, labels=c(&quot;A&quot;, &quot;D&quot;, &quot;N&quot;)), intx = paste0(cop, tx)) ggplot(full.mean.plot, aes(x=trial, y=apt, col=cs, shape=treatment)) + geom_jitter(data=pain, aes(x=trial, y=l2paintol), alpha=.5, width=.1)+ geom_path(aes(group=intx), lwd=2)+ ylab(&quot;log2 pain tolerance&quot;) + geom_point(data=full.mean.plot, size=2, col=&quot;black&quot;) The slopes for the distractor group between trial 2 and trial 4 is positive for those in the distractor treatment, but negative for those in the other treatments. These are averaging out to that zero slope that we saw in the previous graphic. The trajectories are different enough that a full interaction model between treatment and CS is warrented. We will move on to using a random intercept model to account for the repeated measures on the individual child now as well. "],
["random-intercept-model.html", "14.5 Random Intercept Model", " 14.5 Random Intercept Model The random intercept model combines aspects of the various simple analyses in one large model that uses all the data available and in an appropriate fashion. This interaction model could be fit using a standard linear model (two way ANOVA anyone?), but we already know that pain tolerance is correlated within individual, so we should use a model that accounts for that design. Which correlation structure is more appropriate? Why? Autoregressive with lag 1 (AR1) Compound Symmetry Why did we use 0.7 in the argument for corCompSymm in the function below? What does this mean? library(nlme) ri.model &lt;- lme(l2paintol ~ cs + treatment + cs*treatment, random = ~ 1|id, cor = corCompSymm(value = 0.7, form=~1|id), data=pain, na.action=na.omit) summary(ri.model) ## Linear mixed-effects model fit by REML ## Data: pain ## AIC BIC logLik ## 602.7188 634.007 -292.3594 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 0.5708238 0.9116882 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | id ## Parameter estimate(s): ## Rho ## 0.5697763 ## Fixed effects: l2paintol ~ cs + treatment + cs * treatment ## Value Std.Error DF t-value ## (Intercept) 4.655032 0.2981311 181 15.614041 ## csdistracter 0.200314 0.4127482 58 0.485319 ## treatmentdistract -0.127153 0.4062028 58 -0.313028 ## treatmentno directions -0.231649 0.4219640 58 -0.548979 ## csdistracter:treatmentdistract 0.801256 0.5720853 58 1.400588 ## csdistracter:treatmentno directions -0.083080 0.5912098 58 -0.140525 ## p-value ## (Intercept) 0.0000 ## csdistracter 0.6293 ## treatmentdistract 0.7554 ## treatmentno directions 0.5851 ## csdistracter:treatmentdistract 0.1667 ## csdistracter:treatmentno directions 0.8887 ## Correlation: ## (Intr) csdstr trtmnt trtmnd csdst: ## csdistracter -0.722 ## treatmentdistract -0.734 0.530 ## treatmentno directions -0.707 0.510 0.519 ## csdistracter:treatmentdistract 0.521 -0.721 -0.710 -0.368 ## csdistracter:treatmentno directions 0.504 -0.698 -0.370 -0.714 0.504 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.7351583 -0.5228960 -0.1537673 0.5140625 2.5298887 ## ## Number of Observations: 245 ## Number of Groups: 64 The model converged - don’t forget to check this. There was no error message in the output saying it didn’t converge, so we’re good. Initially we would think that none of the main effects or interaction terms are significant. However.. Recall this is an interaction model, so we cannot interpret the main effects directly. We have to write contrasts to examine specific effects. "],
["writing-contrasts.html", "14.6 Writing Contrasts", " 14.6 Writing Contrasts The goal is to write down a vector $\\mathbf{L}$ of linear coefficients for each covariate profile we want to examine and compare, such that when you multiply \\(\\mathbf{L}\\beta\\) you are only left with the beta coefficients that you want to test. Since all covariates in this model are categorical, what is modeled is a series of binary indicator variables (and in this case the interaction between the two). We need to know the ordering of the coefficients to write our \\(L\\). names(fixef(ri.model)) ## [1] &quot;(Intercept)&quot; ## [2] &quot;csdistracter&quot; ## [3] &quot;treatmentdistract&quot; ## [4] &quot;treatmentno directions&quot; ## [5] &quot;csdistracter:treatmentdistract&quot; ## [6] &quot;csdistracter:treatmentno directions&quot; The linear combination vector \\(L\\) for attenders in the attending treatment (AA), have the following \\(L\\). \\[ L = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] The matrix of \\(\\beta\\) regression coefficients is: \\[ \\beta = \\begin{bmatrix} 4.66 \\\\ 0.20 \\\\ -0.13 \\\\ -0.23 \\\\ 0.81 \\\\ -0.09 \\end{bmatrix} \\] The quantity \\(\\mathbf{L}\\beta\\) is then calculated as \\[ \\begin{aligned} L\\beta &amp; = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 4.66 \\\\ 0.20 \\\\ -0.13 \\\\ -0.23 \\\\ 0.81 \\\\ -0.09 \\end{bmatrix} \\\\ &amp; = 1*4.66 + 0*0.20 - 0 * 0.13 - 0*0.23 + 0*0.81 - 0*0.09 \\\\ &amp; = 4.66 \\end{aligned} \\] This is the estimate for the log base 2 seconds for an attender in the attending (AA) treatment group. Attenders in the attending treatment group are expected to be able to keep their arm in ice water for on average \\(2^{4.66}=25.3\\) seconds. The \\(L\\) matrix for an attender in the distracting treatment group (AD) would look like: \\[ \\begin{aligned} L\\beta &amp; = \\begin{bmatrix} 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\begin{bmatrix} 4.66 \\\\ 0.20 \\\\ -0.13 \\\\ -0.23 \\\\ 0.81 \\\\ -0.09 \\end{bmatrix} \\\\ &amp; = 1*4.66 + 0*0.20 - 1*0.13 - 0*0.23 + 0*0.81 - 0*0.09 \\\\ &amp; = 4.53 \\end{aligned} \\] We can have R do this matrix multiplication %*% for us: L = c(1,0,1,0,0,0) B = fixed.effects(ri.model) L%*%B ## [,1] ## [1,] 4.527879 Write down the \\(L\\) vector for the following groups, calculate \\(L\\beta\\) and interpret the results. An attender in the no direction treatment group (AN). A distractor in the attending treatment group (DA). A distractor in the distraction treatment group (DD). A distractor in the no direction treatment group (DN). 14.6.1 Comparing groups (contrasts) Subtracting \\(L_{1}\\) from \\(L_{2}\\) What we’re really interested in, is the pairwise contrasts. The effect of the distracting treatment on attenders, relative to the attending treatment, AA-AD. \\[ \\ \\ \\ \\ \\ L_{1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\\\ \\ \\ \\ \\ \\ L_{2} = \\begin{bmatrix} 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\\\ L_{1} - L_{2} = \\begin{bmatrix} 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] L = c(0,0,-1,0,0,0) L%*%B ## [,1] ## [1,] 0.127153 Attenders in the attending treatment group can keep their arm in ice water on average \\(2^{0.12} = 1.09\\) seconds longer compared to attenders in the distracting treatment group. Explicitly write down \\(L_{1}-L_{2}\\) vector for the specified comparisons, calculate the estimate and interpret the results. DD - DN DN - AN DD - AA AA - AN AD - AN 14.6.2 Confidence intervals of those estimates What we did above provides point estimates, but no confidence intervals that will allow you to determine if that difference is significant. Once we know what our \\(L\\) matrix looks like for the comparison of interest, we need to use a model-based approach to this estimation. The glht() function in the multcomp packages allows us to test any number of generalized linear hypothesis tests from many different types of models. Does the treatment that matches the childs existing coping style improve tolerance compared to the null treatment? library(multcomp) contrast.formulas &lt;- rbind(`DD-DN` = c(0,0, 1, -1, 1, -1), `AA-AN` = c(0,0, 0, -1, 0,0)) plot(glht(ri.model, linfct=contrast.formulas)) There was a positive effect of the treatment on the distractor group under the distracting treatment, but no effect on the attending group under the attending treatment compared to no treatment. "],
["what-to-watch-out-for-3.html", "14.7 What to watch out for", " 14.7 What to watch out for These are also topics for a deeper treatment of this topic. How time is measured. Is there consistent units, and is there a baseline value or zero point where all time is referenced back to? Calendar date vs days since start If time0 is the first treatment, may end up having negative time values. Measurements collected prior to the treatment date. Pattern of missing responses Time spacing that form part of the design Periodic sampling 0-1-2-3-4 3 month: 0-3-6-9-12 logarithmic: 0-1-3-6-12-24 Those are examples of balanced time Random spacing: take a measurement anytime you encounter the patient. I.e. parent brings child in to doctor for a cold or immunizations, vital measurements are taken. Time is days since birth, but there is no special design enforced. Nominal vs actual times Just because you are told to come back in 3 months, doesn’t mean everyone makes it exactly in 90 days. Specifying the correlation structure correctly based on study design "],
["additional-resources.html", "14.8 Additional Resources", " 14.8 Additional Resources Applied Longitudinal Analysis 2nd edition. Fitzmaurice, Laird, Ware (2011) https://content.sph.harvard.edu/fitzmaur/ala2e/ Modeling time series data https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials "],
["spatial.html", "Chapter 15 Spatial Data", " Chapter 15 Spatial Data Outline: Terminology How to make maps using ggplot, and shade areas of interest by the value of other variables How to identify neighbors How to create a neighborhood matrix How to test for spatial autocorrelation How to calculate a regression model using geographically correlated data. As before with longitudinal data, this topic can (and should) fill an entire class. There are also MANY MANY ways to play with spatial data in R. Much googling has occured to complete these notes. "],
["terminology.html", "15.1 Terminology", " 15.1 Terminology polygons : A shape that usually outlines a region such as a county or state. shape file: a specific data format that can spatially describe features such as points lines and polygons. neighbors : two shapes/regions/points that are next to each other longitude : distance east or west of the meridian at Greenwich English latitude : distance north or south of the equator FIPS: Federal Information Processing standards - a.k.a. a code to uniquely identify counties in the United States centroid: The lat/long point that marks the “center” of a polygon Cartesian distance: distance between two points on a plane Great circle distance: shortest distance between two points on the surface of a sphere "],
["mapping-in-r.html", "15.2 Mapping in R", " 15.2 Mapping in R Use existing packages (easier) Import your own shape files (harder). A series of lat/long points and an order in which to to draw the lines between points can also be used but will need to be converted to a spatial polygon object at a later point. 15.2.1 Plotting Region polygons Use the maps library get county border data. Take note of the variable names, we’ll want to merge other data onto this data frame later. counties &lt;- map_data(&quot;county&quot;) head(counties) ## long lat group order region subregion ## 1 -86.50517 32.34920 1 1 alabama autauga ## 2 -86.53382 32.35493 1 2 alabama autauga ## 3 -86.54527 32.36639 1 3 alabama autauga ## 4 -86.55673 32.37785 1 4 alabama autauga ## 5 -86.57966 32.38357 1 5 alabama autauga ## 6 -86.59111 32.37785 1 6 alabama autauga We can use ggplot to draw polygons for each region/subregion. ggplot() + geom_polygon(data = counties, aes(x = long, y = lat, group = group), color = &quot;black&quot;, fill=&quot;NA&quot;) + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(x=NULL, y=NULL) + theme(panel.background = element_blank(), panel.border = element_blank(), axis.ticks=element_blank(), axis.text = element_blank()) Play around with the plotting code to understand what each layer controls. Let’s get county level data that we can plot and analyze. The countyComplete data can be obtained by either the OpenIntro package or from Dr. Donatello’s website. cc &lt;- read.csv(&quot;C:/GitHub/website/static/data/countyComplete.csv&quot;, header=TRUE) cc[1:5,c(1,3,4,6,(ncol(cc)-5):ncol(cc))] ## name FIPS pop2010 age_under_5 fed_spending fed_spend00 ## 1 Autauga County 1001 54571 6.6 331142 7.582652 ## 2 Baldwin County 1003 182265 6.1 1119082 7.969818 ## 3 Barbour County 1005 27457 6.2 240308 8.275639 ## 4 Bibb County 1007 22915 6.0 163201 7.836406 ## 5 Blount County 1009 57322 6.3 294114 5.764229 ## fed_spend10 area density smoking_ban ## 1 6.068095 594.44 91.8 none ## 2 6.139862 1589.78 114.6 none ## 3 8.752158 884.88 31.0 partial ## 4 7.122016 622.58 36.8 none ## 5 5.130910 644.78 88.9 none Looking at a few of the columns we see that this data contains county names, FIPS, county level data such as population, amount of federal spending, population density and what level of smoking ban that is in place. The only information that helps identify each county (other than the name) is FIPS. To add any points to a map, we need lat/long for points and region/subregion name. The housingData package has a geoCounty data set that connects FIPS to the county centroid. A little data cleaning needs to be done first to match the region names with the names found in the counties data set. library(housingData) head(geoCounty) ## fips county state lon lat rMapState rMapCounty ## 1 01001 Autauga County AL -86.64565 32.54009 alabama autauga ## 2 01003 Baldwin County AL -87.72627 30.73831 alabama baldwin ## 3 01005 Barbour County AL -85.39733 31.87403 alabama barbour ## 4 01007 Bibb County AL -87.12526 32.99902 alabama bibb ## 5 01009 Blount County AL -86.56271 33.99044 alabama blount ## 6 01011 Bullock County AL -85.71680 32.10634 alabama bullock # align names county.info &lt;- geoCounty %&gt;% mutate(FIPS = as.numeric(as.character(fips)), long.c = lon, lat.c=lat, region=rMapState, subregion=rMapCounty) %&gt;% select(FIPS, long.c, lat.c, region, subregion) head(county.info) ## FIPS long.c lat.c region subregion ## 1 1001 -86.64565 32.54009 alabama autauga ## 2 1003 -87.72627 30.73831 alabama baldwin ## 3 1005 -85.39733 31.87403 alabama barbour ## 4 1007 -87.12526 32.99902 alabama bibb ## 5 1009 -86.56271 33.99044 alabama blount ## 6 1011 -85.71680 32.10634 alabama bullock Then we merge lat/long centroids and region names to county complete data frame so that data and all geographic information are all on one data frame. cc &lt;- cc %&gt;% left_join(county.info) cc[1:5,c(1,3,4,6,(ncol(cc)-5):ncol(cc))] ## name FIPS pop2010 age_under_5 density smoking_ban long.c ## 1 Autauga County 1001 54571 6.6 91.8 none -86.64565 ## 2 Baldwin County 1003 182265 6.1 114.6 none -87.72627 ## 3 Barbour County 1005 27457 6.2 31.0 partial -85.39733 ## 4 Bibb County 1007 22915 6.0 36.8 none -87.12526 ## 5 Blount County 1009 57322 6.3 88.9 none -86.56271 ## lat.c region subregion ## 1 32.54009 alabama autauga ## 2 30.73831 alabama baldwin ## 3 31.87403 alabama barbour ## 4 32.99902 alabama bibb ## 5 33.99044 alabama blount Now we see that lat/long data as well as region/subregion names have been added. Last we’ll merge all this county level data to the map data. cc.map &lt;- counties %&gt;% left_join(cc) cc.map[1:5,c(1:4,6, (ncol(cc.map)-5):ncol(cc.map))] ## long lat group order subregion fed_spend10 area density ## 1 -86.50517 32.34920 1 1 autauga 6.068095 594.44 91.8 ## 2 -86.53382 32.35493 1 2 autauga 6.068095 594.44 91.8 ## 3 -86.54527 32.36639 1 3 autauga 6.068095 594.44 91.8 ## 4 -86.55673 32.37785 1 4 autauga 6.068095 594.44 91.8 ## 5 -86.57966 32.38357 1 5 autauga 6.068095 594.44 91.8 ## smoking_ban long.c lat.c ## 1 none -86.64565 32.54009 ## 2 none -86.64565 32.54009 ## 3 none -86.64565 32.54009 ## 4 none -86.64565 32.54009 ## 5 none -86.64565 32.54009 Notice now that values such as area and density are repeated across rows per subregion, which directly corresponds to the group variable that allowed us to identify counties on the map. Now we can re-plot the county level map, and add a shading layer to this plot that is % poverty. ggplot() + geom_polygon(data = cc.map, aes(x = long, y = lat, group = group, fill = poverty), color = &quot;black&quot;) + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(x=NULL, y=NULL) + theme(panel.background = element_blank(), panel.border = element_blank(), axis.ticks=element_blank(), axis.text = element_blank()) + scale_fill_continuous(low=&#39;white&#39;, high=&#39;darkorchid4&#39;) We couldn’t set group=subregion because county names are not unique. Change this value and replot the map above to see the impact. This demonstrates why having uniquely numbered shapes are critical for plotting. 15.2.2 Plotting points Here is a different approach using a different package: ggmap. We use county centers for selected counties in Northern California instead of polygon regions to demonstrate how to plot points onto a map instead of regions. library(ggmap) norcal &lt;- cc %&gt;% filter(region==&quot;california&quot;, subregion %in% c(&#39;butte&#39;, &#39;shasta&#39;, &#39;tehama&#39;, &#39;lassen&#39;, &#39;modoc&#39;, &#39;siskiyou&#39;, &#39;humboldt&#39;, &#39;trinity&#39;, &#39;glenn&#39;, &#39;mendocino&#39;, &#39;sutter&#39;, &#39;sierra&#39;, &#39;nevada&#39;, &#39;del norte&#39;, &#39;lake&#39;, &#39;colusa&#39;)) norcal.center &lt;- c(mean(norcal$long.c), mean(norcal$lat.c)) norcal.center ## [1] -122.15392 40.18119 The get_map function downloads google map data centered around location. See the ggmap quick start to see all the options you can use. norcal.map &lt;- get_map(location= norcal.center, zoom=7, color=&quot;bw&quot;) This approach uses ggmap() instead of ggplot(), but layers (such as points) are still added on in a similar fashion. ggmap(norcal.map) + geom_point(data=norcal, aes(x=long.c, y=lat.c, color=per_capita_income, size=pop2010)) + scale_color_gradient(low=&quot;darkgreen&quot;, high=&quot;chartreuse&quot;) "],
["spatial-influence.html", "15.3 Spatial Influence", " 15.3 Spatial Influence The closer the points, the more influence they should have on each other. Want to identify “neighbors” Neighbors can be identified based on (inverse) distance between centroids or points existence of a shared border length of shared borders 15.3.1 Distance between points Calculate the great circle distance between points. county.cent.mat &lt;- as.matrix(norcal[,c(&#39;long.c&#39;, &#39;lat.c&#39;)]) # convert lat/long centroids to a matrix D &lt;- raster::pointDistance(county.cent.mat, lonlat=TRUE) rownames(D) &lt;- colnames(D) &lt;- norcal$subregion D[1:5, 1:5] ## butte colusa del norte glenn humboldt ## butte 0.00 NA NA NA NA ## colusa 77055.36 0.00 NA NA NA ## del norte 303103.83 319031.41 0.0 NA NA ## glenn 68210.62 48641.26 271321.8 0.0 NA ## humboldt 225521.47 219448.27 116948.0 175980.8 0 What are the units of the values in D? Why couldn’t we have just used dist(norcal) here to create a distance matrix? 15.3.2 Spatial Weights Matrix The further away the points, the lower the correlation. The simplest approach is to use the inverse distance, and then normalize weights across the rows. Calculate 1/ each distance (cell). Some values are Inf. Why did this happen? Set these to NA Calculate the row totals Divide the weight matrix W by the row totals to normalize rows W &lt;- 1/D W[!is.finite(W)] &lt;- NA rtot &lt;- rowSums(W, na.rm=TRUE) W &lt;- W/rtot W[1:5, 1:5] # confirm the weights add up to 1 across the rows ## butte colusa del norte glenn humboldt ## butte NA NA NA NA NA ## colusa 1.0000000 NA NA NA NA ## del norte 0.5128007 0.4871993 NA NA NA ## glenn 0.3768293 0.5284354 0.09473533 NA NA ## humboldt 0.1909279 0.1962118 0.36818384 0.2446764 NA 15.3.3 Identifying Neighbors Recap terminology. polygons, shapes, data frames, coordinates. This section converts a lot of R objects from one object type to another. Points are within a certain distance away from each other library(spdep) norcal_nb &lt;- dnearneigh(x=county.cent.mat, d1=0, d2=80, longlat=T) summary(norcal_nb) ## Neighbour list object: ## Number of regions: 16 ## Number of nonzero links: 26 ## Percentage nonzero weights: 10.15625 ## Average number of links: 1.625 ## 4 regions with no links: ## 3 7 9 13 ## Link number distribution: ## ## 0 1 2 3 4 ## 4 6 1 2 3 ## 6 least connected regions: ## 5 8 10 11 12 16 with 1 link ## 3 most connected regions: ## 1 2 4 with 4 links The dnearneigh function identifies neighbors of region points by Great Circle distance (specified using longlat=TRUE) between lower (d1) and upper (80) bounds in kilometers. This creates a neighborhood object. We’ll look at the structure of this shortly. To visualize these neighbors, we need to combine this neighborhood information back onto the data frame with existing coordinate data. To do that we need to convert the neighborhood object to data frame using nb_to_df, a user defined function that was googled. Then we can map county centers and connect neighbors with lines. norcal.nb.toplot &lt;- nb_to_df(norcal_nb, county.cent.mat) ggmap(norcal.map) + geom_point(data=norcal, aes(x=long.c, y=lat.c), col=&quot;red&quot;, size=2) + geom_segment(data=norcal.nb.toplot, aes(x=x, xend=xend, y=y, yend=yend), lwd=1.5) Adjacent polygons The maps data provides polygon data in that it gives us a series of lat/long points and an order on how to connect them. This is not quite the same as a shape file. Using a function found on this StackExchange post we convert the data frame containing polygon information to a special SpatialPolygon object. library(sp) local &lt;- counties %&gt;% filter(region==&quot;california&quot;, subregion %in% c(&#39;butte&#39;, &#39;glenn&#39;, &#39;colusa&#39;, &#39;plumas&#39;, &#39;sutter&#39;, &#39;yuba&#39;, &#39;tehama&#39;)) %&gt;% select(long, lat, subregion) local.spp &lt;- local %&gt;% group_by(subregion) %&gt;% do(poly=select(., long, lat) %&gt;%Polygon()) %&gt;% rowwise() %&gt;% do(polys=Polygons(list(.$poly),.$subregion)) %&gt;% {SpatialPolygons(.$polys)} plot(local.spp) Now we can create the neighborhood matrix on this polygon shape file using poly2nb. local_nb &lt;- poly2nb(local.spp) summary(local_nb) ## Neighbour list object: ## Number of regions: 7 ## Number of nonzero links: 24 ## Percentage nonzero weights: 48.97959 ## Average number of links: 3.428571 ## Link number distribution: ## ## 3 6 ## 6 1 ## 6 least connected regions: ## colusa glenn plumas sutter tehama yuba with 3 links ## 1 most connected region: ## butte with 6 links The maptools package also has a map2SpatialPoylgons function that may be helpful in some situations to convert maps to polygons. In this example we used the function get_map() which returned a data.frame. The map() function is another way to get map data out of the maps library, but it returns a map object. 15.3.4 Neighborhood matrix A neighborhood matrix is not stored in R the way we like to imagine matrices (squares). Instead it is a list, where each entry in the list is a vector of neighbor identifiers. str(norcal_nb) ## List of 16 ## $ : int [1:4] 2 4 14 15 ## $ : int [1:4] 1 4 6 14 ## $ : int 0 ## $ : int [1:4] 1 2 6 15 ## $ : int 16 ## $ : int [1:3] 2 4 8 ## $ : int 0 ## $ : int 6 ## $ : int 0 ## $ : int 12 ## $ : int 15 ## $ : int 10 ## $ : int 0 ## $ : int [1:2] 1 2 ## $ : int [1:3] 1 4 11 ## $ : int 5 ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;nbtype&quot;)= chr &quot;distance&quot; ## - attr(*, &quot;distances&quot;)= num [1:2] 0 80 ## - attr(*, &quot;region.id&quot;)= chr [1:16] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## - attr(*, &quot;call&quot;)= language dnearneigh(x = county.cent.mat, d1 = 0, d2 = 80, longlat = T) ## - attr(*, &quot;dnn&quot;)= num [1:2] 0 80 ## - attr(*, &quot;bounds&quot;)= chr [1:2] &quot;GT&quot; &quot;LE&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE Recall the first few counties in norcal are Butte, Colusa and Del Norte. The first three lines in norcal_nb identify the neighbors for that county. Butte has 3 neighbors that are within 80km (50 miles) away. There are 3 lines extending out of Butte. Colusa also has 3. Del Norte has 0 neighbors. Another example is the neighborhood matrix created using the local county shape files. str(local_nb) ## List of 7 ## $ : int [1:6] 2 3 4 5 6 7 ## $ : int [1:3] 1 3 5 ## $ : int [1:3] 1 2 6 ## $ : int [1:3] 1 6 7 ## $ : int [1:3] 1 2 7 ## $ : int [1:3] 1 3 4 ## $ : int [1:3] 1 4 5 ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:7] &quot;butte&quot; &quot;colusa&quot; &quot;glenn&quot; &quot;plumas&quot; ... ## - attr(*, &quot;call&quot;)= language poly2nb(pl = local.spp) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE We can transform a neighborhood object to a matrix using nb2mat. local_nb_mat &lt;- nb2mat(local_nb, style=&#39;B&#39;, zero.policy = TRUE) colnames(local_nb_mat) &lt;- unique(local$subregion) kable(local_nb_mat) butte colusa glenn plumas sutter tehama yuba butte 0 1 1 1 1 1 1 colusa 1 0 1 0 1 0 0 glenn 1 1 0 0 0 1 0 plumas 1 0 0 0 0 1 1 sutter 1 1 0 0 0 0 1 tehama 1 0 1 1 0 0 0 yuba 1 0 0 1 1 0 0 We can use this matrix structure as a correlation structure in a random intercept model. Details forthcoming "],
["modeling-geographically-correlated-data.html", "15.4 Modeling geographically correlated data", " 15.4 Modeling geographically correlated data 15.4.1 Geographically weighted regression Fit a county-stratified regression model Plot the regression coefficients on a map. Look for spatial trends. You can formally test this using Moran’s I. Here is a good walk through if you want to take this approach: http://rspatial.org/analysis/rst/6-local_regression.html#geographicaly-weighted-regression Recall the stratified model… is the no-pooled model - each region has it’s own model estimates many more parameters than the pooled model regions with little to no data will have unstable estimates or simply won’t be able to be fit. 15.4.2 Bayesian Models Really this is where we all should be moving to. But we’re not there yet. 15.4.3 Random Intercept Model Let’s look to build a model to estimate the population. It’s skewed right so we’ll take a (natural) log. raw &lt;- ggplot(cc, aes(x=pop2010)) + geom_histogram(aes(y=..density..), col=&quot;black&quot;,fill=&quot;NA&quot;) + geom_density() log &lt;- ggplot(cc, aes(x=log(pop2010))) + geom_histogram(aes(y=..density..), col=&quot;black&quot;,fill=&quot;NA&quot;) + geom_density() gridExtra::grid.arrange(raw, log, nrow=1) Does there appear to be a spatial trend? Yup. Note i took off the color of the polygons here to make it more clear to read. ggplot() + geom_polygon(data = cc.map, aes(x = long, y = lat, group = group, fill = log(pop2010))) + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(x=NULL, y=NULL) + theme(panel.background = element_blank(), panel.border = element_blank(), axis.ticks=element_blank(), axis.text = element_blank()) + scale_fill_continuous(low=&#39;white&#39;, high=&#39;darkorchid4&#39;) We will use % of the county who own a home (home_ownership), % of people who are foreign_born, the % of the county living in poverty and the % of the county who have a bachelors degree. Data preparation Drop data from countyComplete on states such as Alaska and Hawaii, that aren’t included in the map data obtained from map_data. These rows are identifable because they are missing the region variable from when we merged the mapping data onto the county data. Convert the state name to all lower case so it will match with the state name (as region) when mapping. Restrict the analysis data set to only the variables that will be used in modeling, and drop all records with missing data. cont.us &lt;- cc %&gt;% filter(!is.na(cc$region)) %&gt;% mutate(state=tolower(state), y=log(pop2010)) %&gt;% select(y, home_ownership, foreign_born, poverty, bachelors, state, region) %&gt;% na.omit() The countyComplete data set has one value per county, so we can’t fit a random intercept to the county level (and that would be too many clusters) - so we’ll cluster at the state level. Get the map data at the state level states &lt;- map_data(&quot;state&quot;) Create a neighbor matrix states.spp &lt;- states %&gt;% group_by(region) %&gt;% do(poly=select(., long, lat) %&gt;%Polygon()) %&gt;% rowwise() %&gt;% do(polys=Polygons(list(.$poly),.$region)) %&gt;% {SpatialPolygons(.$polys)} plot(states.spp) states_nb &lt;- poly2nb(states.spp) states_nb_mat &lt;- nb2mat(states_nb, style=&#39;B&#39;, zero.policy = TRUE) rownames &lt;- colnames(states_nb_mat) &lt;- unique(states$region) View a sample to make sure it works - theres some wierd lines between WI and OH. states_nb_mat[c(&#39;wisconsin&#39;, &#39;ohio&#39;, &#39;michigan&#39;, &#39;illinois&#39;, &#39;indiana&#39;), c(&#39;wisconsin&#39;, &#39;ohio&#39;, &#39;michigan&#39;, &#39;illinois&#39;, &#39;indiana&#39;)] ## wisconsin ohio michigan illinois indiana ## wisconsin 0 0 1 1 0 ## ohio 0 0 1 0 1 ## michigan 1 1 0 0 1 ## illinois 1 0 0 0 1 ## indiana 0 1 1 1 0 All neighbors are accounted for, and none extra. Good. Define a function that extracts the fitted values from a model, and merges it onto the shape data for mapping. I’m going to be doing this process for several models. When you repeat the same steps more than 3 times, it’s time to write a function. model.to.map &lt;- function(mod){ fit.data&lt;- data.frame(y.hat=predict(mod), region=cont.us$state) state.pred &lt;- fit.data %&gt;% group_by(region) %&gt;% summarise(state.ave = mean(y.hat)) plot.fitted &lt;- states %&gt;% left_join(state.pred) return(plot.fitted) } 15.4.3.1 Fully pooled model: Ignore state level information fully_pooled &lt;- lm(y ~ home_ownership + foreign_born + poverty + bachelors, data=cont.us) fpm &lt;- model.to.map(mod=fully_pooled) 15.4.3.2 No pooled model - Fixed effects for state. no_pooled &lt;- lm(y ~ home_ownership + foreign_born + poverty + bachelors + state, data=cont.us) npm &lt;- model.to.map(mod=no_pooled) 15.4.3.3 Partially pooled: States are independent Disclaimer: This section does not work as intended. Do not use. For discussion purposes only. pp_ind &lt;- lme4::lmer(y ~ home_ownership + foreign_born + poverty + bachelors + (1|state), data=cont.us) pp_ind_map &lt;- model.to.map(mod=pp_ind) 15.4.3.4 Partially pooled: Neighboring states are correlated. We need to manually create the correlation matrix. Start with a matrix with 1’s down the diagonal. Notice there’s only 49 entries here. states.corr &lt;- diag(49); colnames(states.corr) &lt;- rownames(states.corr) &lt;- rownames(states_nb_mat) corrplot::corrplot(states.corr) Then we add on the neighbor matrix - but add correlation between states. states.spcorr &lt;- states.corr + 0.9*states_nb_mat corrplot::corrplot(states.spcorr) Model using lme4qtl so we can specifiy the relationship matrix. pp_spcorr &lt;- lme4qtl::relmatLmer(y ~ home_ownership + foreign_born + poverty + bachelors + (1|state), data=cont.us, relmat = list(state = states.corr)) pp_spcorr_map &lt;- model.to.map(mod=pp_spcorr) Let’s see these all together. Estimates are similar, so let’s look at the differences (residuals) between the predicted values using the fully pooled and all others. Red = fully pooled model under-estimated Blue = fully pooled over-estimated Story possibly to be continued. The RI model is creating identical state-level estimates as the spatially correlated model. This is due to one of two reasons: We’re predicting at the county level but aggregating up to the state level The package lme4qtl doesn’t work the way it claims. Again… easier if we fit a Bayesian model here. "],
["additional-info.html", "15.5 Additional Info", " 15.5 Additional Info ggmap quick start https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/ggmap/ggmapCheatsheet.pdf http://www.rspatial.org/ NOAA’s Southwest Fisheries Science Center. Making maps with R walk through/tutorial (2014) http://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html Stat 4580 at University of Iowa: https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/maps.html How map_id works (county names are not unique): https://stackoverflow.com/questions/37912418/how-does-geom-map-map-id-function-work map2spatialpolygon https://stackoverflow.com/questions/26062280/converting-a-map-object-to-a-spatialpolygon-object More on Moran’s I from ESRI http://resources.esri.com/help/9.3/arcgisengine/java/gp_toolref/spatial_statistics_tools/how_spatial_autocorrelation_colon_moran_s_i_spatial_statistics_works.htm lme4qtl documentation https://github.com/variani/lme4qtl https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5830078/ "],
["mda.html", "Chapter 16 Missing Data", " Chapter 16 Missing Data Missing Data happens. Not always General: Item non-response. Individual pieces of data are missing. Unit non-response: Records have some background data on all units, but some units don’t respond to any question. Monotonone missing data: Variables can be ordered such that one block of variables more observed than the next. "],
["identifying-missing-data.html", "16.1 Identifying missing data", " 16.1 Identifying missing data Missing data in R is denoted as NA Arithmetic functions on missing data will return missing survey &lt;- MASS::survey # to avoid loading the MASS library which will conflict with dplyr head(survey$Pulse) ## [1] 92 104 87 NA 35 64 mean(survey$Pulse) ## [1] NA The summary() function will always show missing. summary(survey$Pulse) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 35.00 66.00 72.50 74.15 80.00 104.00 45 The is.na() function is helpful to identify rows with missing data table(is.na(survey$Pulse)) ## ## FALSE TRUE ## 192 45 The function table() will not show NA by default. table(survey$M.I) ## ## Imperial Metric ## 68 141 table(survey$M.I, useNA=&quot;always&quot;) ## ## Imperial Metric &lt;NA&gt; ## 68 141 28 What percent of the data set is missing? round(prop.table(table(is.na(survey)))*100,1) ## ## FALSE TRUE ## 96.2 3.8 4% of the data points are missing. How much missing is there per variable? prop.miss &lt;- apply(survey, 2, function(x) round(sum(is.na(x))/NROW(x),4)) prop.miss ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height ## 0.0042 0.0042 0.0042 0.0042 0.0000 0.1899 0.0042 0.0000 0.0042 0.1181 ## M.I Age ## 0.1181 0.0000 The amount of missing data per variable varies from 0 to 19%. 16.1.1 Visualize missing patterns Using ggplot2 pmpv &lt;- data.frame(variable = names(survey), pct.miss =prop.miss) ggplot(pmpv, aes(x=variable, y=pct.miss)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;Percent&quot;) + scale_y_continuous(labels=scales::percent, limits=c(0,1)) + geom_text(data=pmpv, aes(label=paste0(round(pct.miss*100,1),&quot;%&quot;), y=pct.miss+.025), size=4) Using mice library(mice) md.pattern(survey) ## Fold Exer Age Sex Wr.Hnd NW.Hnd W.Hnd Clap Smoke Height M.I Pulse ## 168 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 1 1 1 1 0 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 0 1 1 1 1 1 1 ## 38 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 20 1 1 1 1 1 1 1 1 1 0 0 1 2 ## 1 1 1 1 1 0 0 1 0 1 1 1 1 3 ## 7 1 1 1 1 1 1 1 1 1 0 0 0 3 ## 1 1 1 1 1 1 1 1 1 0 0 0 1 3 ## 0 0 0 1 1 1 1 1 1 28 28 45 107 This somewhat ugly output tells us that 168 records have no missing data, 38 records are missing only Pulse and 20 are missing both Height and M.I. Using VIM library(VIM) aggr(survey, col=c(&#39;chartreuse3&#39;,&#39;mediumvioletred&#39;), numbers=TRUE, sortVars=TRUE, labels=names(survey), cex.axis=.7, gap=3, ylab=c(&quot;Missing data&quot;,&quot;Pattern&quot;)) The plot on the left is a simplified, and ordered version of the ggplot from above, except the bars appear to be inflated because the y-axis goes up to 15% instead of 100%. The plot on the right shows the missing data patterns, and indicate that 71% of the records has complete cases, and that everyone who is missing M.I. is also missing Height. Another plot that can be helpful to identify patterns of missing data is a marginplot (also from VIM). Two continuous variables are plotted against each other. Blue bivariate scatterplot and univariate boxplots are for the observations where values on both variables are observed. Red univariate dotplots and boxplots are drawn for the data that is only observed on one of the two variables in question. The darkred text indicates how many records are missing on both. marginplot(survey[,c(6,10)]) This shows us that the observations missing pulse have the same median height, but those missing height have a higher median pulse rate. "],
["effects-of-nonresponse.html", "16.2 Effects of Nonresponse", " 16.2 Effects of Nonresponse Textbook example: Example reported in W.G. Cochran, Sampling Techniques, 3rd edition, 1977, ch. 13 Consider data that come form an experimental sampling of fruit orcharts in North Carolina in 1946. Three successive mailings of the same questionnaire were sent to growers. For one of the questions the number of fruit trees, complete data were available for the population… Ave. # trees # of growers % of pop’n Ave # trees/grower 1st mailing responders 300 10 456 2nd mailing responders 543 17 382 3rd mailing responders 434 14 340 Nonresponders 1839 59 290 ——– ——– ——– Total population 3116 100 329 The overall response rate was very low. The rate of non response is clearly related to the average number of trees per grower. The estimate of the average trees per grower can be calculated as a weighted average from responders \\(\\bar{Y_{1}}\\) and non responders \\(\\bar{Y_{2}}\\). Bias: The difference between the observed estimate \\(\\bar{y}_{1}\\) and the true parameter \\(\\mu\\). \\[ \\begin{aligned} E(\\bar{y}_{1}) - \\mu &amp; = \\bar{Y_{1}} - \\bar{Y} \\\\ &amp; = \\bar{Y}_{1} - \\left[(1-w)\\bar{Y}_{1} - w\\bar{Y}_{2}\\right] \\\\ &amp; = w(\\bar{Y}_{1} - \\bar{Y}_{2}) \\end{aligned} \\] Where \\(w\\) is the proportion of non-response. The amount of bias is the product of the proportion of non-response and the difference in the means between the responders and the non-responders. The sample provides no information about \\(\\bar{Y_{2}}\\), the size of the bias is generally unknown without information gained from external data. "],
["missing-data-mechanisms.html", "16.3 Missing Data Mechanisms", " 16.3 Missing Data Mechanisms Process by which some units observed, some units not observed Missing Completely at Random (MCAR): The probability that a data point is missing is completely unrelated (independent) of any observed and unobserved data or parameters. P(Y missing| X, Y) = P(Y missing) Ex: Miscoding or forgetting to log in answer Missing at Random (MAR): The probability that a data point is missing is independent can be explained or modeled by other observed variables. P(Y missing|x, Y) = P(Y missing | X) Ex: Y = age, X = sex - Pr (Y miss| X = male) = 0.2 - Pr (Y miss| X = female) = 0.3 - Males people are less likely to fill out an income survey - The missing data on income is related to gender - After accounting for gender the missing data is unrelated to income. Not missing at Random (NMAR): The probability that a data point is missing depends on the value of the variable in question. P(Y missing | X, Y) = P (Y missing|X, Y) Ex: Y = income, X = immigration status Richer person may be less willing to disclose income Undocumented immigrant may be less willing to disclose income Write down an example of each. Does it matter to inferences? Yes! 16.3.1 Demonstration via Simulation What follows is just one method of approaching this problem via code. Simulation is a frequently used technique to understand the behavior of a process over time or over repeated samples. 16.3.1.1 MCAR Draw a random sample of size 100 from a standard Normal distribution (Z) and calculate the mean. set.seed(456) # setting a seed ensures the same numbers will be drawn each time z &lt;- rnorm(100) mean.z &lt;- mean(z) mean.z ## [1] 0.1205748 Delete data at a rate of \\(p\\) and calculate the complete case (available) mean. Sample 100 random Bernoulli (0/1) variables with probability \\(p\\). x &lt;- rbinom(100, 1, p=.5) Find out which elements are are 1’s delete.these &lt;- which(x==1) Set those elements in z to NA. z[delete.these] &lt;- NA Calculate the complete case mean mean(z, na.rm=TRUE) ## [1] 0.1377305 Calculate the bias as the sample mean minus the true mean (\\(E(\\hat\\theta) - \\theta\\)). mean(z, na.rm=TRUE) - mean.z ## [1] 0.01715565 How does the bias change as a function of the proportion of missing? Let \\(p\\) range from 0% to 99% and plot the bias as a function of \\(p\\). calc.bias &lt;- function(p){ # create a function to handle the repeated calculations mean(ifelse(rbinom(100, 1, p)==1, NA, z), na.rm=TRUE) - mean.z } p &lt;- seq(0,.99,by=.01) plot(c(0,1), c(-1, 1), type=&quot;n&quot;, ylab=&quot;Bias&quot;, xlab=&quot;Proportion of missing&quot;) points(p, sapply(p, calc.bias), pch=16) abline(h=0, lty=2, col=&quot;blue&quot;) What is the behavior of the bias as \\(p\\) increases? Look specifically at the position/location of the bias, and the variance/variability of the bias. 16.3.1.2 NMAR: Missing related to data What if the rate of missing is related to the value of the outcome? Again, let’s setup a simulation to see how this works. Randomly draw 100 random data points from a Standard Normal distribution to serve as our population, and 100 uniform random values between 0 and 1 to serve as probabilities of the data being missing (\\(p=P(miss)\\)) Z &lt;- rnorm(100) p &lt;- runif(100, 0, 1) Sort both in ascending order, shove into a data frame and confirm that \\(p(miss)\\) increases along with \\(z\\). dta &lt;- data.frame(Z=sort(Z), p=sort(p)) head(dta) ## Z p ## 1 -2.898122 0.003673455 ## 2 -2.185058 0.013886146 ## 3 -2.076032 0.035447986 ## 4 -1.938288 0.039780643 ## 5 -1.930809 0.051362816 ## 6 -1.905331 0.054639596 ggplot(dta, aes(x=p, y=Z)) + geom_point() + xlab(&quot;P(missing)&quot;) + ylab(&quot;Z~Normal(0,1)&quot;) Set \\(Z\\) missing with probability equal to the \\(p\\) for that row. Create a new vector dta$z.miss that is either 0, or the value of dta$Z with probability 1-dta$p. Then change all the 0’s to NA. dta$Z.miss &lt;- dta$Z * (1-rbinom(NROW(dta), 1, dta$p)) head(dta) # see structure of data to understand what is going on ## Z p Z.miss ## 1 -2.898122 0.003673455 -2.898122 ## 2 -2.185058 0.013886146 -2.185058 ## 3 -2.076032 0.035447986 -2.076032 ## 4 -1.938288 0.039780643 -1.938288 ## 5 -1.930809 0.051362816 -1.930809 ## 6 -1.905331 0.054639596 -1.905331 dta$Z.miss[dta$Z.miss==0] &lt;- NA Calculate the complete case mean and the bias mean(dta$Z.miss, na.rm=TRUE) ## [1] -0.7777319 mean(dta$Z) - mean(dta$Z.miss, na.rm=TRUE) ## [1] 0.6830372 Did the complete case estimate over- or under-estimate the true mean? Is the bias positive or negative? 16.3.1.3 NMAR: Pure Censoring Consider a hypothetical blood test to measure a hormone that is normally distributed in the blood with mean 10\\(\\mu g\\) and variance 1. However the test to detect the compound only can detect levels above 10. z &lt;- rnorm(100, 10, 1) y &lt;- z y[y&lt;10] &lt;- NA mean(z) - mean(y, na.rm=TRUE) ## [1] -0.6850601 Did the complete case estimate over- or under-estimate the true mean? When the data is not missing at random, the bias can be much greater. Usually you don’t know the missing data mechanism. Degrees of difficulty MCAR: is easiest to deal with. MAR: we can live with it. NMAR: most difficult to handle. Evidence? What can we learn from evidence in the data set at hand? May be evidence in the data rule out MCAR - test responders vs. nonresponders. Example: Responders tend to have higher/lower average education than nonresponders by t-test Example: Response more likely in one geographic area than another by chi-square test No evidence in data set to rule out MAR (although there may be evidence from an external data source) What is plausible? Cochran example: when human behavior is involved, MCAR must be viewed as an extremely special case that would often be violated in practice Missing data may be introduced by design (e.g., measure some variables, don’t measure others for reasons of cost, response burden), in which case MCAR would apply MAR is much more common than MCAR We cannot be too cavalier about assuming MAR, but anecdotal evidence shows that it often is plausible when conditioning on enough information Ignorable Missing If missing-data mechanism is MCAR or MAR then nonresponse is said to be “ignorable”. Origin of name: in likelihood-based inference, both the data model and missing-data mechanism are important but with MCAR or MAR, inference can be based solely on the data model, thus making inference much simpler “Ignorability” is a relative assumption: missingness on income may be NMAR given only gender, but may be MAR given gender, age, occupation, region of the country "],
["general-strategies.html", "16.4 General strategies", " 16.4 General strategies Strategies for handling missing data include: Complete-case/available-case analysis: drop cases that make analysis inconvenient. If variables are known to contribute to the missing values, then appropriate modeling can often account for the missingness. Imputation procedures: fill in missing values, then analyze completed data sets using complete-date methods Weighting procedures: modify “design weights” (i.e., inverse probabilities of selection from sampling plan) to account for probability of response Model-based approaches: develop model for partially missing data, base inferences on likelihood under that model 16.4.1 Complete cases analysis If not all variables observed, delete case from analysis Advantages: Simplicity Common sample for all estimates Disadvantages: Loss of valid information Bias due to violation of MCAR 16.4.2 Available-case analysis Use all cases where the variable of interest is present Potentially different sets of cases for means of X and Y and complete pairs for \\(r_{XY}\\) Tempting to think that available-case analysis will be superior to complete-case analysis But it can distort relationships between variables by not using a common base of observations for all quantities being estimated. 16.4.3 Imputation Fill in missing values, analyze completed data set Advantage: Rectangular data set easier to analyze Disadvantage: “Both seductive and dangerous” (Little and Rubin) Can understate uncertainty due to missing values. Can induce bias if imputing under the wrong model. "],
["imputation-methods.html", "16.5 Imputation Methods", " 16.5 Imputation Methods Unconditional mean substitution. Never use Impute all missing data using the mean of observed cases Artificially decreases the variance. Hot deck imputation Impute values by randomly sampling values from observed data. Good for categorical data Reasonable for MCAR and MAR Model based imputation Conditional Mean imputation: Use regression on observed variables to estimate missing values Predictive Mean Matching: Fills in a value randomly by sampling observed values whose regression-predicted values are closest to the regression-predicted value for the missing point. Cross between hot-deck and conditional mean Categorical data can be imputed using classification models Less biased than mean substitution but SE’s could be inflated Adding a residual Impute regression value \\(\\pm\\) a randomly selected residual based on estimated residual variance Over the long-term, we can reduce bias, on the average …but we can do better. "],
["multiple-imputation-mi.html", "16.6 Multiple Imputation (MI)", " 16.6 Multiple Imputation (MI) 16.6.1 Goals Accurately reflect available information Avoid bias in estimates of quantities of interest Estimation could involve explicit or implicit model Accurately reflect uncertainty due to missingness 16.6.2 Technique For each missing value, impute \\(m\\) estimates (usually \\(m\\) = 5) Imputation method must include a random component Create \\(m\\) complete data sets Perform desired analysis on each of the \\(m\\) complete data sets Combine final estimates in a manner that accounts for the between, and within imputation variance. _Credit: http://www.stefvanbuuren.nl_ 16.6.3 MI as a paradigm Logic: “Average over” uncertainty, don’t assume most likely scenario (single imputation) covers all plausible scenarios Principle: Want nominal 95% intervals to cover targets of estimation 95% of the time Simulation studies show that, when MAR assumption holds: Proper imputations will yield close to nominal coverage (Rubin 87) Improvement over single imputation is meaningful Number of imputations can be modest - even 2 adequate for many purposes, so 5 is plenty Rubin 87: Multiple Imputation for Nonresponse in Surveys, Wiley, 1987). 16.6.4 Inference on MI Consider \\(m\\) imputed data sets. For some quantity of interest \\(Q\\) with squared \\(SE = U\\), calculate \\(Q_{1}, Q_{2}, \\ldots, Q_{m}\\) and \\(U_{1}, U_{2}, \\ldots, U_{m}\\) (e.g., carry out \\(m\\) regression analyses, obtain point estimates and SE from each). Then calculate the average estimate \\(\\bar{Q}\\), the average variance \\(\\bar{U}\\), and the variance of the averages \\(B\\). \\[ \\begin{aligned} \\bar{Q} &amp; = \\sum^{m}_{i=1}Q_{i}/m \\\\ \\bar{U} &amp; = \\sum^{m}_{i=1}U_{i}/m \\\\ B &amp; = \\frac{1}{m-1}\\sum^{m}_{i=1}(Q_{i}-\\bar{Q})^2 \\end{aligned} \\] Then \\(T = \\bar{U} + \\frac{m+1}{m}B\\) is the estimated total variance of \\(\\bar{Q}\\). Significance tests and interval estimates can be based on \\[\\frac{\\bar{Q}-Q}{\\sqrt{T}} \\sim t_{df}, \\mbox{ where } df = (m-1)(1+\\frac{1}{m+1}\\frac{\\bar{U}}{B})^2\\] df are similar to those for comparison of normal means with unequal variances, i.e., using Satterthwaite approximation. Ratio of (B = between-imputation variance) to (T = between + within-imputation variance) is known as the fraction of missing information (FMI). The FMI has been proposed as a way to monitor ongoing data collection and estimate the potential bias resulting from survey non-responders Wagner, 2018 "],
["multiple-imputation-using-chained-equations-mice.html", "16.7 Multiple Imputation using Chained Equations (MICE)", " 16.7 Multiple Imputation using Chained Equations (MICE) 16.7.1 Overview Generates multiple imputations for incomplete multivariate data by Gibbs sampling. Missing data can occur anywhere in the data. Impute an incomplete column by generating ‘plausible’ synthetic values given other columns in the data. For predictors that are incomplete themselves, the most recently generated imputations are used to complete the predictors prior to imputation of the target column. A separate univariate imputation model can be specified for each column. The default imputation method depends on the measurement level of the target column. Your best reference guide to this section of the notes is the mice: Multivariate Imputation by Chained Equations in R article in the Journal of Statistical Software by Stef van Buuren. https://www.jstatsoft.org/article/view/v045i03 16.7.2 Process / Algorithm Consider a data matrix with 3 variables \\(y_{1}\\), \\(y_{2}\\), \\(y_{3}\\), each with missing values. At iteration \\((\\ell)\\): Fit a model on \\(y_{1}^{(\\ell-1)}\\) using current values of \\(y_{2}^{(\\ell-1)}, y_{3}^{(\\ell-1)}\\) Impute missing \\(y_{1}\\), generating \\(y_{1}^{(\\ell)}\\) Fit a model on \\(y_{2}^{(\\ell-1)}\\) using current versions of \\(y_{1}^{(\\ell)}, y_{3}^{(\\ell-1)}\\) Impute missing \\(y_{2}\\), generating \\(y_{2}^{(\\ell)}\\) Fit a model on \\(y_{3}\\) using current versions of \\(y_{1}^{(\\ell)}, y_{2}^{(\\ell)}\\) Impute missing \\(y_{3}\\), generating \\(y_{3}^{(\\ell)}\\) Start next cycle using updated values \\(y_{1}^{(\\ell)}, y_{2}^{(\\ell)}, y_{3}^{(\\ell)}\\) where \\((\\ell)\\) cycles from 1 to \\(L\\), before an imputed value is drawn. 16.7.3 Convergence How many imputations (\\(m\\)) should we create and how many iterations (\\(L\\)) should I run between imputations? Original research from Rubin states that small amount of imputations (\\(m=5\\)) would be sufficient. Advances in computation have resulted in very efficient programs such as mice - so generating a larger number of imputations (say \\(m=40\\)) are more common Pan, 2016 You want the number of iterations between draws to be long enough that the Gibbs sampler has converged. There is no test or direct method for determing convergence. Plot parameter against iteration number, one line per chain. These lines should be intertwined together, without showing trends. Convergence can be identified when the variance between lines is smaller (or at least no larger) than the variance within the lines. Mandatory Reading Read 4.3: Assessing Convergence in the JSS article on mice 16.7.4 Imputation Methods Some built-in imputation methods in the mice package are: pmm: Predictive mean matching (any) DEFAULT FOR NUMERIC norm.predict: Linear regression, predicted values (numeric) mean: Unconditional mean imputation (numeric) logreg: Logistic regression (factor, 2 levels) DEFAULT logreg.boot: Logistic regression with bootstrap polyreg: Polytomous logistic regression (factor, &gt;= 2 levels) DEFAULT lda: Linear discriminant analysis (factor, &gt;= 2 categories) cart: Classification and regression trees (any) rf: Random forest imputations (any) "],
["example-prescribed-amount-of-missing-.html", "16.8 Example: Prescribed amount of missing.", " 16.8 Example: Prescribed amount of missing. We will demonstrate using Fisher’s Iris data (pre-built in with R) where we can artificially create a prespecified percent of the data missing. This allows us to be able to estimate the bias incurred by using these imputation methods. For the iris data we set a seed and use the prodNA() function from the missForest package to create 10% missing values in this data set. library(missForest) set.seed(12345) # Note to self: Change the combo on my luggage iris.mis &lt;- prodNA(iris, noNA=0.1) prop.table(table(is.na(iris.mis))) ## ## FALSE TRUE ## 0.9 0.1 Visualize missing data pattern. library(VIM) aggr(iris.mis, col=c(&#39;darkolivegreen3&#39;,&#39;salmon&#39;), numbers=TRUE, sortVars=TRUE, labels=names(iris.mis), cex.axis=.7, gap=3, ylab=c(&quot;Missing data&quot;,&quot;Pattern&quot;)) ## ## Variables sorted by number of missings: ## Variable Count ## Sepal.Length 0.10666667 ## Petal.Width 0.10666667 ## Sepal.Width 0.10000000 ## Species 0.10000000 ## Petal.Length 0.08666667 Here’s another example of where only 10% of the data overall is missing, but it results in only 58% complete cases. 16.8.1 Multiply impute the missing data using mice() imp_iris &lt;- mice(iris.mis, m=10, maxit=25, meth=&quot;pmm&quot;, seed=500, printFlag=FALSE) summary(imp_iris) ## Multiply imputed data set ## Call: ## mice(data = iris.mis, m = 10, method = &quot;pmm&quot;, maxit = 25, printFlag = FALSE, ## seed = 500) ## Number of multiple imputations: 10 ## Missing cells per column: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 16 15 13 16 15 ## Imputation methods: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; ## VisitSequence: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 2 3 4 5 ## PredictorMatrix: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Sepal.Length 0 1 1 1 1 ## Sepal.Width 1 0 1 1 1 ## Petal.Length 1 1 0 1 1 ## Petal.Width 1 1 1 0 1 ## Species 1 1 1 1 0 ## Random generator seed value: 500 The Stack Exchange post listed below has a great explanation/description of what each of these arguments control. It is a very good idea to understand these controls. https://stats.stackexchange.com/questions/219013/how-do-the-number-of-imputations-the-maximum-iterations-affect-accuracy-in-mul/219049#219049 16.8.2 Check the imputation method used on each variable. imp_iris$meth ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; Predictive mean matching was used for all variables, even Species. This is reasonable because PMM is a hot deck method of imputation. 16.8.3 Check Convergence plot(imp_iris, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;)) The variance across chains is no larger than the variance within chains. 16.8.4 Look at the values generated for imputation imp_iris$imp$Sepal.Length ## 1 2 3 4 5 6 7 8 9 10 ## 1 5.0 5.0 5.0 5.8 5.0 5.0 5.5 5.0 5.0 4.9 ## 5 5.1 5.0 4.9 4.9 5.8 5.1 4.8 4.7 5.1 5.3 ## 26 4.9 5.1 5.0 5.0 5.0 4.6 5.1 4.6 4.9 5.1 ## 31 5.1 4.7 5.0 4.9 5.2 4.6 4.6 5.0 4.6 5.0 ## 33 5.2 5.5 6.0 4.9 5.5 5.5 5.5 5.6 5.7 5.7 ## 39 4.4 5.0 4.3 5.0 4.4 5.0 4.4 4.4 4.4 4.9 ## 43 4.7 5.4 4.9 4.9 4.6 4.6 4.9 4.7 4.8 4.6 ## 56 5.8 5.6 6.1 5.4 5.4 5.5 6.2 6.0 5.8 5.8 ## 96 5.8 6.3 5.6 6.5 6.2 6.2 5.8 6.5 6.4 6.6 ## 103 6.5 6.9 6.7 6.8 6.7 7.2 7.4 7.2 6.7 6.8 ## 113 6.3 6.7 6.7 6.7 6.7 6.7 6.7 6.7 6.4 5.9 ## 124 5.7 5.8 6.2 6.1 5.5 6.6 6.2 6.2 6.2 5.5 ## 132 7.7 7.7 7.7 7.7 7.7 7.7 7.6 7.6 7.7 7.3 ## 135 6.0 6.4 6.3 6.7 6.7 6.5 6.3 6.4 7.0 5.6 ## 149 6.4 6.5 6.8 7.0 7.0 6.4 6.3 6.3 6.5 6.3 ## 150 6.4 6.1 6.4 6.4 6.3 6.5 6.4 6.1 6.4 6.4 This is just for us to see what this imputed data look like. Each column is an imputed value, each row is a row where an imputation for Sepal.Length was needed. Notice only imputations are shown, no observed data is showing here. 16.8.5 Create a complete data set by filling in the missing data using the imputations iris_1 &lt;- complete(imp_iris, action=1) Action=1 returns the first completed data set, action=2 returns the second completed data set, and so on. 16.8.5.1 Alternative - Stack the imputed data sets in long format. iris_long &lt;- complete(imp_iris, &#39;long&#39;) By looking at the names of this new object we can confirm that there are indeed 10 complete data sets with \\(n=150\\) in each. names(iris_long) ## [1] &quot;.imp&quot; &quot;.id&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; ## [5] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; table(iris_long$.imp) ## ## 1 2 3 4 5 6 7 8 9 10 ## 150 150 150 150 150 150 150 150 150 150 16.8.6 Visualize Imputations Let’s compare the imputed values to the observed values to see if they are indeed “plausible” We want to see that the distribution of of the magenta points (imputed) matches the distribution of the blue ones (observed). Univariately densityplot(imp_iris) Multivariately xyplot(imp_iris, Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width | Species, cex=.8, pch=16) Analyze and pool All of this imputation was done so we could actually perform an analysis! Let’s run a simple linear regression on Sepal.Length as a function of Sepal.Width, Petal.Length and Species. model &lt;- with(imp_iris, lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species)) summary(pool(model)) ## est se t df Pr(&gt;|t|) ## (Intercept) 2.4003660 0.27904211 8.602164 117.11666 4.107825e-14 ## Sepal.Width 0.4635384 0.08556538 5.417359 122.66722 3.066295e-07 ## Petal.Length 0.7126897 0.07046809 10.113652 87.37099 2.220446e-16 ## Species2 -0.7875803 0.23149303 -3.402177 100.83240 9.597852e-04 ## Species3 -1.1614620 0.32513199 -3.572279 69.21508 6.493711e-04 ## lo 95 hi 95 nmis fmi lambda ## (Intercept) 1.8477435 2.9529885 NA 0.10362235 0.08844452 ## Sepal.Width 0.2941624 0.6329144 15 0.08849659 0.07375537 ## Petal.Length 0.5726351 0.8527443 13 0.18448110 0.16602439 ## Species2 -1.2468094 -0.3283511 NA 0.14686085 0.13010510 ## Species3 -1.8100466 -0.5128774 NA 0.24332027 0.22176707 Pooled parameter estimates \\(\\bar{Q}\\) and their standard errors \\(\\sqrt{T}\\) are provided, along with a significance test (against \\(\\beta_p=0\\)), and a 95% interval. Additional information included in this table is the number of missing values, the fraction of missing information (fmi) as defined by Rubin (1987), and lambda, the proportion of total variance that is attributable to the missing data (\\(\\lambda = (B + B/m)/T)\\). kable(summary(pool(model))[,c(1:3, 5:7, 9)], digits=3) est se t Pr(&gt;|t|) lo 95 hi 95 fmi (Intercept) 2.400 0.279 8.602 0.000 1.848 2.953 0.104 Sepal.Width 0.464 0.086 5.417 0.000 0.294 0.633 0.088 Petal.Length 0.713 0.070 10.114 0.000 0.573 0.853 0.184 Species2 -0.788 0.231 -3.402 0.001 -1.247 -0.328 0.147 Species3 -1.161 0.325 -3.572 0.001 -1.810 -0.513 0.243 16.8.7 Calculating bias The iris data set had no missing data to begin with. So we can calculate the “true” parameter estimates… true.model &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species, data=iris) and find the difference in coefficients. The variance of the multiply imputed estimates is larger because of the between-imputation variance. library(forestplot) te.mean &lt;- summary(true.model)$coefficients[,1] mi.mean &lt;- summary(pool(model))[,1] te.ll &lt;- te.mean - 1.96*summary(true.model)$coefficients[,2] mi.ll &lt;- summary(pool(model))[,6] te.ul &lt;- te.mean + 1.96*summary(true.model)$coefficients[,2] mi.ul &lt;- summary(pool(model))[,7] names &lt;- names(coef(true.model)) forestplot(names, legend = c(&quot;True Model&quot;, &quot;MICE&quot;), fn.ci_norm = c(fpDrawNormalCI, fpDrawCircleCI), mean = cbind(te.mean, mi.mean), lower = cbind(te.ll, mi.ll), upper = cbind(te.ul, mi.ul), col=fpColors(box=c(&quot;blue&quot;, &quot;darkred&quot;)), xlab=&quot;Regression coefficients&quot;, boxsize = .1 ) "],
["final-thoughts.html", "16.9 Final thoughts", " 16.9 Final thoughts “In our experience with real and artificial data…, the practical conclusion appears to be that multiple imputation, when carefully done, can be safely used with real problems even when the ultimate user may be applying models or analyses not contemplated by the imputer.” - Little &amp; Rubin (Book, p. 218) Don’t ignore missing data. Impute sensibly and multiple times. It’s typically desirable to include many predictors in an imputation model, both to improve precision of imputed values make MAR assumption more plausible But the number of covariance parameters goes up as the square of the number of variables in the model, implying practical limits on the number of variables for which parameters can be estimated well MI applies to subjects who have a general missingness pattern, i.e., they have measurements on some variables, but not on others. But, subjects can be lost to follow up due to death or other reasons (i.e., attrition). Here we have only baseline data, but not the outcome or other follow up data. If attrition subjects are eliminated from the sample, they can produce non-response or attrition bias. Use attrition weights. Given a baseline profile, predict the probability that subject will stay and use the inverse probability as weight. e.g., if for a given profile all subjects stay, then the predicted probability is 1 and the attrition weight is 1. Such a subject “counts once”. For another profile, the probability may be 0.5, attrition weight is 1/.5 = 2 and that person “counts twice”. For differential drop-out, or self-selected treatment, you can consider using Propensity Scores. "],
["additional-references-2.html", "16.10 Additional References", " 16.10 Additional References Little, R. and Rubin, D. Statistical Analysis with Missing Data, 2nd Ed., Wiley, 2002 Standard reference Requires some math Allison, P. Missing Data, Sage, 2001 Small and cheap Requires very little math Multiple Imputation.com http://www.stefvanbuuren.nl/mi/ http://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/ http://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/ Imputation methods for complex survey data and data not missing at random is an open research topic. Read more about this here: https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mi_sect032.htm https://shirinsplayground.netlify.com/2017/11/mice_sketchnotes/ "]
]
