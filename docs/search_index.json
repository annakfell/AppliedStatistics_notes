[["slr.html", "Chapter 7 Simple Linear Regression", " Chapter 7 Simple Linear Regression The goal of linear regression is to describe the relationship between an independent variable X and a continuous dependent variable \\(Y\\) as a straight line. Data for this type of model can arise in two ways; Fixed-\\(X\\): values of \\(X\\) are preselected by investigator Variable-\\(X\\): have random sample of \\((X,Y)\\) values Both Regression and Correlation can be used for two main purposes: Descriptive: Draw inferences regarding the relationship Predictive: Predict value of \\(Y\\) for a given value of \\(X\\) Simple Linear Regression is an example of a Bivariate analysis since there is only one covariate (explanatory variable) under consideration. "],["example.html", "7.1 Example", " 7.1 Example Lung function data were obtained from an epidemiological study of households living in four areas with different amounts and types of air pollution. The data set used in this book is a subset of the total data. In this chapter we use only the data taken on the fathers, all of whom are nonsmokers (see PMA6 Appendix A for more details). # Read in the data from a version stored online. fev &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/Lung_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) One of the major early indicators of reduced respiratory function is FEV1 or forced expiratory volume in the first second (amount of air exhaled in 1 second). Since it is known that taller males tend to have higher FEV1, we wish to determine the relationship between height and FEV1. We can use regression analysis for both a descriptive and predictive purpose. Descriptive: Describing the relationship between FEV1 and height Predictive: Use the equation to determine expected or normal FEV1 for a given height ggplot(fev, aes(y=FFEV1, x=FHEIGHT)) + geom_point() + xlab(&quot;Height&quot;) + ylab(&quot;FEV1&quot;) + ggtitle(&quot;Scatterplot and Regression line of FEV1 Versus Height for Males.&quot;) + geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;blue&quot;) In this graph, height is given on the horizontal axis since it is the independent or predictor variable and FEV1 is given on the vertical axis since it is the dependent or outcome variable. Interpretation: There does appear to be a tendency for taller men to have higher FEV1. The regression line is also added to the graph. The line is tilted upwards, indicating that we expect larger values of FEV1 with larger values of height. Specifically the equation of the regression line is \\[ Y = -4.087 + 0.118 X \\] The quantity 0.118 in front of \\(X\\) is greater than zero, indicating that as we increase \\(X, Y\\) will increase. For example, we would expect a father who is 70 inches tall to have an FEV1 value of \\[\\mbox{FEV1} = -4.087 + (0.118) (70) = 4.173\\] If the height was 66 inches then we would expect an FEV1 value of only 3.70. 7.1.1 Caution on out of range predictions To take an extreme example, suppose a father was 2 feet tall. Then the equation would predict a negative value of FEV1 (\\(-1.255\\)). A safe policy is to restrict the use of the equation to the range of the \\(X\\) observed in the sample. "],["mathematical-model.html", "7.2 Mathematical Model", " 7.2 Mathematical Model The mathematical model that we use for regression has three features. \\(Y\\) values are normally distributed at any given \\(X\\) The mean of \\(Y\\) values at any given \\(X\\) follows a straight line \\(Y = \\beta_{0} + \\beta_{1} X\\). The variance of \\(Y\\) values at any \\(X\\) is \\(\\sigma^2\\) (same for all X). This is known as homoscedasticity, or homogeneity of variance. Mathematically this is written as: \\[ Y|X \\sim N(\\mu_{Y|X}, \\sigma^{2}) \\\\ \\mu_{Y|X} = \\beta_{0} + \\beta_{1} X \\\\ Var(Y|X) = \\sigma^{2} \\] and can be visualized as: Figure 6.2 7.2.1 Unifying model framework The mathematical model above describes the theoretical relationship between \\(Y\\) and \\(X\\). So in our unifying model framework to describe observed data, DATA = MODEL + RESIDUAL Our observed data values \\(y_{i}\\) can be modeled as being centered on \\(\\mu_{Y|X}\\), with normally distributed residuals. \\[ y_{i} = \\beta_{0} + \\beta_{1} X + \\epsilon_{i} \\\\ \\epsilon_{i} \\sim N(0, \\sigma^{2}) \\] 7.2.2 Parameter Estimates Estimate the slope \\(\\beta_{1}\\) and intercept \\(\\beta_{0}\\) using a method called Least Squares. The residual mean squared error (RMSE) is an estimate of the variance \\(s^{2}\\) RMSE can also refer to the root mean squared error. "],["least-squares-regression.html", "7.3 Least Squares Regression", " 7.3 Least Squares Regression The Least Squares method finds the estimates for the intercept \\(b_{0}\\) and slope \\(b_{1}\\) that minimize the SSE (Sum of squared errors). Let’s see how that works: See https://paternogbc.shinyapps.io/SS_regression/ Initial Setup Set the sample size to 50 Set the regression slope to 1 Set the standard deviation to 5 Partitioning the Variance using the Sum of Squares SS Total- how far are the points away from \\(\\bar{y}\\)? (one sample mean) SS Regression - how far away is the regression line from \\(\\bar{y}\\)?. SS Error - how far are the points away from the estimated regression line? Looking at it this way, we are asking “If I know the value of \\(x\\), how much better will I be at predicting \\(y\\) than if I were just to use \\(\\bar{y}\\)? This is the same partitioning of variance that is happens with ANOVA! Increase the standard deviation to 30. What happens to SSReg? What about SSE? Here is a link to another interactive app where you can try to fit your own line to minimize the SSE. RMSE is the Root Mean Squared Error. In the PMA textbook this is denoted as \\(S\\), which is an estimate for \\(\\sigma\\). \\[ S = \\sqrt{\\frac{SSE}{N-2}}\\] "],["assumptions-2.html", "7.4 Assumptions", " 7.4 Assumptions Many of the assumptions for regression are on the form of the residuals, which can’t be assessed until after the model has been fit. Assumptions to check before modeling Randomness / Independence Very serious Can use hierarchical models for clustered samples No real good way to “test” for independence. Need to know how the sample was obtained. Linear relationship Slight departures OK Can use transformations to achieve it Graphical assessment: Simple scatterplot of \\(y\\) vs \\(x\\). Looking for linearity in the relationship. Should be done prior to any analysis. Assumptions to check after modeling Homogeneity of variance (same \\(\\sigma^{2}\\)) Not extremely serious Can use transformations to achieve it Graphical assessment: Plot the residuals against the x variable, add a lowess line. This assumption is upheld if there is no relationship/trend between the residuals and the predictor. Normal residuals Slight departures OK Can use transformations to achieve it Graphical assessment: normal qqplot of the model residuals. "],["slr-fev.html", "7.5 Example", " 7.5 Example This section uses functions from the broom, and performance packages to help tidy and visualize results from regression models. Returning to the Lung function data set from PMA6, lets analyze the relationship between height and FEV for fathers in this data set. ggplot(fev, aes(y=FFEV1, x=FHEIGHT)) + geom_point() + xlab(&quot;Height&quot;) + ylab(&quot;FEV1&quot;) + ggtitle(&quot;Scatter Diagram with Regression (blue) and Lowess (red) Lines of FEV1 Versus Height for Fathers.&quot;) + geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;blue&quot;) + geom_smooth(se=FALSE, col=&quot;red&quot;) There does appear to be a tendency for taller men to have higher FEV1. The trend is linear, the red lowess trend line follows the blue linear fit line quite well. Let’s fit a linear model and report the regression parameter estimates. fev.dad.model &lt;- lm(FFEV1 ~ FHEIGHT, data=fev) broom::tidy(fev.dad.model) |&gt; kable(digits=3) term estimate std.error statistic p.value (Intercept) -4.087 1.152 -3.548 0.001 FHEIGHT 0.118 0.017 7.106 0.000 The least squares equation is \\(Y = -4.09 + 0.118X\\). We can calculate the confidence interval for that estimate using the confint function. confint(fev.dad.model) |&gt; kable(digits=3) 2.5 % 97.5 % (Intercept) -6.363 -1.810 FHEIGHT 0.085 0.151 For ever inch taller a father is, his FEV1 measurement significantly increases by .12 (95%CI: .09, .15, p&lt;.0001). Lastly, we need to check assumptions on the residuals to see if the model results are valid. performance::check_model(fev.dad.model, check = c(&quot;qq&quot;, &quot;linearity&quot;, &quot;homogeneity&quot;, &quot;pp_check&quot;)) No major deviations away from what is expected. "],["interval-estimation.html", "7.6 Interval estimation", " 7.6 Interval estimation Everything is estimated with some degree of error Confidence intervals for the mean of \\(Y\\) at some given value of \\(X\\) (say, \\(X^*\\)) \\[ \\hat{Y} \\pm tS \\bigg[ \\frac{1}{N} + \\sqrt{\\frac{(X^* - \\bar{X})^{2}}{\\sum(X - \\bar{X})^{2}}} \\quad \\bigg] \\] Prediction intervals for an individual \\(Y\\) \\[ \\hat{Y} \\pm tS \\bigg[ 1+ \\frac{1}{N} + \\sqrt{\\frac{(X^* - \\bar{X})^{2}}{\\sum(X - \\bar{X})^{2}}} \\quad \\bigg] \\] Which one is wider? Why? How does this relate to the standard deviation of individual \\(x\\)’s, and the standard deviation of \\(\\bar{x}\\)’s? If we set the se argument in geom_smooth to TRUE, the shaded region is the confidence band for the mean. To get the prediction interval, we have use the predict function to calculate the prediction interval, and then we can add that onto the plot as additional geom_lines. pred.int &lt;- predict(fev.dad.model, interval=&quot;predict&quot;) |&gt; data.frame() ggplot(fev, aes(y=FFEV1, x=FHEIGHT)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=TRUE, col=&quot;blue&quot;) + geom_line(aes(y=pred.int$lwr), linetype=&quot;dashed&quot;, col=&quot;red&quot;, lwd=1.5) + geom_line(aes(y=pred.int$upr), linetype=&quot;dashed&quot;, col=&quot;red&quot;, lwd=1.5) "],["anova-for-regression.html", "7.7 ANOVA for regression", " 7.7 ANOVA for regression Since an ANOVA is an analysis of the variance due to a model, compared to the unexplained variance, it can be used to test the overall model fit. This will give us the same general answer to the question of “is there an association between X and Y” that testing for a non-zero slope (\\(\\beta \\neq 0\\)). If the mean squared value for the regression is much larger than the mean squared value for the residual error, then the line fits the data better than the simple mean, and thus, the slope of the line is not zero. aov(fev.dad.model) |&gt; summary() |&gt; pander() Analysis of Variance Model   Df Sum Sq Mean Sq F value Pr(&gt;F) FHEIGHT 1 16.05 16.05 50.5 4.677e-11 Residuals 148 47.05 0.3179 NA NA "],["mlr.html", "Chapter 8 Multiple Linear Regression", " Chapter 8 Multiple Linear Regression Extends simple linear regression. Describes a linear relationship between a single continuous \\(Y\\) variable, and several \\(X\\) variables. Predicts \\(Y\\) from \\(X_{1}, X_{2}, \\ldots , X_{P}\\). Now it’s no longer a 2D regression line, but a \\(p\\) dimensional regression plane. "],["types-of-x-variables.html", "8.1 Types of X variables", " 8.1 Types of X variables Fixed: The levels of \\(X\\) are selected in advance with the intent to measure the affect on an outcome \\(Y\\). Variable: Random sample of individuals from the population is taken and \\(X\\) and \\(Y\\) are measured on each individual. X’s can be continuous or discrete (categorical) X’s can be transformations of other X’s, e.g., \\(log(x), x^{2}\\). "],["mathematical-model-1.html", "8.2 Mathematical Model", " 8.2 Mathematical Model \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\ldots + \\beta_{p}x_{pi} + \\epsilon_{i}\\] The assumptions on the residuals \\(\\epsilon_{i}\\) still hold: They have mean zero They are homoscedastic, that is all have the same finite variance: \\(Var(\\epsilon_{i})=\\sigma^{2}&lt;\\infty\\) Distinct error terms are uncorrelated: (Independent) \\(\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0,\\forall i\\neq j.\\) The regression model relates \\(y\\) to a function of \\(\\textbf{X}\\) and \\(\\mathbf{\\beta}\\), where \\(\\textbf{X}\\) is a \\(nxp\\) matrix of \\(p\\) covariates on \\(n\\) observations and \\(\\mathbf{\\beta}\\) is a length \\(p\\) vector of regression coefficients. In matrix notation this looks like: \\[ \\textbf{y} = \\textbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\] "],["parameter-estimation.html", "8.3 Parameter Estimation", " 8.3 Parameter Estimation The goal of regression analysis is to minimize the residual error. That is, to minimize the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable. \\[ \\epsilon_{i} = \\hat{y_{i}} - y_{i}\\] The method of Least Squares accomplishes this by finding parameter estimates \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimized the sum of the squared residuals: \\[ \\sum_{i=1}^{n} \\epsilon_{i} \\] For simple linear regression the regression coefficient estimates that minimize the sum of squared errors can be calculated as: \\[ \\hat{\\beta_{0}} = \\bar{y} - \\hat{\\beta_{1}}\\bar{x} \\quad \\mbox{ and } \\quad \\hat{\\beta_{1}} = r\\frac{s_{y}}{s_{x}} \\] For multiple linear regression, the fitted values \\(\\hat{y_{i}}\\) are calculated as the linear combination of x’s and \\(\\beta\\)’s, \\(\\sum_{i=1}^{p}X_{ij}\\beta_{j}\\). The sum of the squared residual errors (the distance between the observed point \\(y_{i}\\) and the fitted value) now has the following form: \\[ \\sum_{i=1}^{n} |y_{i} - \\sum_{i=1}^{p}X_{ij}\\beta_{j}|^{2}\\] Or in matrix notation \\[ || \\mathbf{y} - \\mathbf{X}\\mathbf{\\beta} ||^{2} \\] The details of methods to calculate the Least Squares estimate of \\(\\beta\\)’s is left to a course in mathematical statistics. "],["example-1.html", "8.4 Example", " 8.4 Example The analysis in example ?? concluded that FEV1 in fathers significantly increases by 0.12 (95% CI:0.09, 0.15) liters per additional inch in height (p&lt;.0001). Looking at the multiple \\(R^{2}\\) (correlation of determination), this simple model explains 25% of the variance seen in the outcome \\(y\\). However, FEV tends to decrease with age for adults, so we should be able to predict it better if we use both height and age as independent variables in a multiple regression equation. What direction do you expect the slope coefficient for age to be? For height? Fitting a regression model in R with more than 1 predictor is done by adding each variable to the right hand side of the model notation connected with a +. mv_model &lt;- lm(FFEV1 ~ FAGE + FHEIGHT, data=fev) summary(mv_model) ## ## Call: ## lm(formula = FFEV1 ~ FAGE + FHEIGHT, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.34708 -0.34142 0.00917 0.37174 1.41853 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.760747 1.137746 -2.427 0.0165 * ## FAGE -0.026639 0.006369 -4.183 4.93e-05 *** ## FHEIGHT 0.114397 0.015789 7.245 2.25e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5348 on 147 degrees of freedom ## Multiple R-squared: 0.3337, Adjusted R-squared: 0.3247 ## F-statistic: 36.81 on 2 and 147 DF, p-value: 1.094e-13 confint(mv_model) ## 2.5 % 97.5 % ## (Intercept) -5.00919751 -0.51229620 ## FAGE -0.03922545 -0.01405323 ## FHEIGHT 0.08319434 0.14559974 Holding height constant, a father who is one year older is expected to have a FEV value 0.03 (0.01, 0.04) liters less than another man (p&lt;.0001). Holding age constant, a father who is 1cm taller than another man is expected to have a FEV value of 0.11 (.08, 0.15) liter greater than the other man (p&lt;.0001). For the model that includes age, the coefficient for height is now 0.11, which is interpreted as the rate of change of FEV1 as a function of height after adjusting for age. This is also called the partial regression coefficient of FEV1 on height after adjusting for age. Both height and age are significantly associated with FEV in fathers (p&lt;.0001 each). "],["binary-predictors..html", "8.5 Binary predictors.", " 8.5 Binary predictors. Does gender also play a roll in FEV? Let’s look at the separate effects of height and age on FEV1, and visualize how gender plays a roll. ht.plot &lt;- ggplot(fev_long, aes(x=ht, y=fev1)) + geom_point(aes(col=gender)) + geom_smooth(se=FALSE, aes(col=gender), method=&quot;lm&quot;) + geom_smooth(se=FALSE, col=&quot;red&quot;, method=&quot;lm&quot;) + scale_color_viridis_d() + theme(legend.position = c(0.15,0.85)) age.plot &lt;- ggplot(fev_long, aes(x=age, y=fev1)) + geom_point(aes(col=gender)) + geom_smooth(se=FALSE, aes(col=gender), method=&quot;lm&quot;) + geom_smooth(se=FALSE, col=&quot;red&quot;, method=&quot;lm&quot;) + scale_color_viridis_d(guide=FALSE) gridExtra::grid.arrange(ht.plot, age.plot, ncol=2) The points are colored by gender Each gender has it’s own best fit line in the same color as the points The red line is the best fit line overall - ignoring gender Is gender a moderator for either height or age? Let’s compare the models with, and without gender Dependent variable: fev1 W/o gender w/ gender age -0.02*** (-0.03, -0.01) -0.02*** (-0.03, -0.02) ht 0.16*** (0.15, 0.18) 0.11*** (0.08, 0.13) genderF -0.64*** (-0.79, -0.48) Constant -6.74*** (-7.84, -5.63) -2.24*** (-3.71, -0.77) Observations 300 300 Adjusted R2 0.57 0.65 Residual Std. Error 0.53 (df = 297) 0.48 (df = 296) F Statistic 197.57*** (df = 2; 297) 182.77*** (df = 3; 296) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Gender is a binary categorical variable, with reference group “Male”. This is detected because the variable that shows up in the regression model output is genderF. So the estimate shown is for males, compared to females. More details on how categorical variables are included in multivariable models is covered in section 8.6. Interpretation of Coefficients The regression equation for the model without gender is \\[ y = -6.74 - 0.02 age + 0.16 height \\] \\(b_{0}:\\) For someone who is 0 years old and 0 cm tall, their FEV is -6.74L. \\(b_{1}:\\) For every additional year older an individual is, their FEV1 decreases by 0.02L. \\(b_{2}:\\) For every additional cm taller an individual is, their FEV1 increases by 0.16L. The regression equation for the model with gender is \\[ y = -2.24 - 0.02 age + 0.11 height - 0.64genderF \\] \\(b_{0}:\\) For a male who is 0 years old and 0 cm tall, their FEV is -2.24L. \\(b_{1}:\\) For every additional year older an individual is, their FEV1 decreases by 0.02L. \\(b_{2}:\\) For every additional cm taller an individual is, their FEV1 increases by 0.16L. \\(b_{3}:\\) Females have 0.64L lower FEV compared to males. Note: The interpretation of categorical variables still falls under the template language of “for every one unit increase in \\(X_{p}\\), \\(Y\\) changes by \\(b_{p}\\)”. Here, \\(X_{3}=0\\) for males, and 1 for females. So a 1 “unit” change is females compared to males. Which model fits better? What measure are you using to quanitify “fit”? What part of the model (intercept, or one of the slope parameters) did adding gender have the most effect on? "],["cat-predictors.html", "8.6 Categorical Predictors", " 8.6 Categorical Predictors Let’s continue to model the length of the iris petal based on the length of the sepal, controlling for species. But here we’ll keep species as a categorical variable. What happens if we just put the variable in the model? summary(lm(Petal.Length ~ Sepal.Length + Species, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76390 -0.17875 0.00716 0.17461 0.79954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.70234 0.23013 -7.397 1.01e-11 *** ## Sepal.Length 0.63211 0.04527 13.962 &lt; 2e-16 *** ## Speciesversicolor 2.21014 0.07047 31.362 &lt; 2e-16 *** ## Speciesvirginica 3.09000 0.09123 33.870 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2826 on 146 degrees of freedom ## Multiple R-squared: 0.9749, Adjusted R-squared: 0.9744 ## F-statistic: 1890 on 3 and 146 DF, p-value: &lt; 2.2e-16 Examine the coefficient names, Speciesversicolor and Speciesvirginica. R (and most software packages) automatically take a categorical variable and turn it into a series of binary indicator variables. Let’s look at what the software program does in the background. Below is a sample of the iris data. The first column shows the row number, specifically I am only showing 2 sample rows from each species. The second column is the value of the sepal length, the third is the binary indicator for if the iris is from species versicolor, next the binary indicator for if the iris is from species virginica, and lastly the species as a 3 level categorical variable (which is what we’re used to seeing at this point.)   Sepal.Length Speciesversicolor Speciesvirginica Species 1 5.1 0 0 setosa 2 4.9 0 0 setosa 51 7 1 0 versicolor 52 6.4 1 0 versicolor 101 6.3 0 1 virginica 102 5.8 0 1 virginica 8.6.1 Factor variable coding Most commonly known as “Dummy coding”. Not an informative term to use. Better used term: Indicator variable Math notation: I(gender == “Female”). A.k.a reference coding For a nominal X with K categories, define K indicator variables. Choose a reference (referent) category: Leave it out Use remaining K-1 in the regression. Often, the largest category is chosen as the reference category. For the iris example, 2 indicator variables are created for versicolor and virginica. Interpreting the regression coefficients are going to be compared to the reference group. In this case, it is species setosa. The mathematical model is now written as follows, where \\(x_{1}\\) is Sepal Length, \\(x_{2}\\) is the indicator for versicolor, and \\(x_{3}\\) the indicator for virginica \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{3i}+ \\epsilon_{i}\\] Let’s look at the regression coefficients and their 95% confidence intervals from the main effects model again. main.eff.model &lt;- lm(Petal.Length ~ Sepal.Length + Species, data=iris) pander(main.eff.model) Fitting linear model: Petal.Length ~ Sepal.Length + Species   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.702 0.2301 -7.397 1.005e-11 Sepal.Length 0.6321 0.04527 13.96 1.121e-28 Speciesversicolor 2.21 0.07047 31.36 9.646e-67 Speciesvirginica 3.09 0.09123 33.87 4.918e-71 pander(confint(main.eff.model))   2.5 % 97.5 % (Intercept) -2.157 -1.248 Sepal.Length 0.5426 0.7216 Speciesversicolor 2.071 2.349 Speciesvirginica 2.91 3.27 In this main effects model, Species only changes the intercept. The effect of species is not multiplied by Sepal length. The interpretations are the following: \\(b_{1}\\): After controlling for species, Petal length significantly increases with the length of the sepal (0.63, 95% CI 0.54-0.72, p&lt;.0001). \\(b_{2}\\): Versicolor has on average 2.2cm longer petal lengths compared to setosa (95% CI 2.1-2.3, p&lt;.0001). \\(b_{3}\\): Virginica has on average 3.1cm longer petal lengths compared to setosa (95% CI 2.9-3.3, p&lt;.0001). "],["model-diagnostics.html", "8.7 Model Diagnostics", " 8.7 Model Diagnostics The same set of regression diagnostics can be examined to identify any potential influential points, outliers or other problems with the linear model. par(mfrow=c(2,2)) plot(mv_model) "],["multicollinearity.html", "8.8 Multicollinearity", " 8.8 Multicollinearity Occurs when some of the X variables are highly intercorrelated. Affects estimates and their SE’s (p. 143) Look at tolerance, and its inverse, the Variance Inflation Factor (VIF) Need tolerance &lt; 0.01, or VIF &gt; 100. car::vif(mv_model) ## FAGE FHEIGHT ## 1.003163 1.003163 tolerance = 1/car::vif(mv_model) tolerance ## FAGE FHEIGHT ## 0.9968473 0.9968473 Solution: use variable selection to delete some X variables. Alternatively, use dimension reduction techniques such as Principal Components "],["what-to-watch-out-for.html", "8.9 What to watch out for", " 8.9 What to watch out for Representative sample Range of prediction should match observed range of X in sample Use of nominal or ordinal, rather than interval or ratio data Errors-in-variables Correlation does not imply causation Violation of assumptions Influential points Appropriate model Multicollinearity "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
